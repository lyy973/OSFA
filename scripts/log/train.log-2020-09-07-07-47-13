Show configuration
adam:
  beta1: 0.9
  beta2: 0.999
cuhk03:
  classic_split: False
  labeled_images: True
  use_metric_cuhk03: False
data:
  combineall: False
  height: 256
  norm_mean: [0.485, 0.456, 0.406]
  norm_std: [0.229, 0.224, 0.225]
  root: /home/s2019020843/changeeeee/data/
  save_dir: log
  sources: ['market1501']
  split_id: 0
  targets: ['market1501']
  transforms: ['random_flip', 'random_erase']
  type: image
  width: 128
  workers: 4
loss:
  name: triplet
  softmax:
    label_smooth: True
  triplet:
    margin: 0.15
    weight_t: 1.0
    weight_x: 1.0
market1501:
  use_500k_distractors: False
model:
  load_weights: 
  name: plr_osnet
  pretrained: True
  resume: 
rmsprop:
  alpha: 0.99
sampler:
  num_instances: 4
  train_sampler: RandomIdentitySampler
sgd:
  dampening: 0.0
  momentum: 0.9
  nesterov: False
test:
  batch_size: 300
  dist_metric: euclidean
  eval_freq: 10
  evaluate: False
  normalize_feature: False
  ranks: [1, 5, 10, 20]
  rerank: False
  start_eval: 0
  visactmap: False
  visrank: False
  visrank_topk: 10
train:
  base_lr_mult: 0.1
  batch_size: 64
  fixbase_epoch: 0
  gamma: 0.1
  lr: 3.5e-05
  lr_scheduler: warmup
  max_epoch: 150
  multiplier: 10
  new_layers: ['classifier']
  open_layers: ['classifier']
  optim: adam
  print_freq: 20
  seed: 1
  staged_lr: False
  start_epoch: 0
  stepsize: [60, 90]
  total_epoch: 39
  weight_decay: 0.0005
use_gpu: True
video:
  pooling_method: avg
  sample_method: evenly
  seq_len: 15

Collecting env info ...
** System info **
PyTorch version: 1.5.1
Is debug build: No
CUDA used to build PyTorch: 10.2

OS: CentOS Linux 7 (Core)
GCC version: (GCC) 4.8.5 20150623 (Red Hat 4.8.5-39)
CMake version: Could not collect

Python version: 3.7
Is CUDA available: Yes
CUDA runtime version: 10.2.89
GPU models and configuration: 
GPU 0: GeForce RTX 2080 Ti
GPU 1: GeForce RTX 2080 Ti
GPU 2: GeForce RTX 2080 Ti
GPU 3: GeForce RTX 2080 Ti

Nvidia driver version: 440.82
cuDNN version: Could not collect

Versions of relevant libraries:
[pip3] numpy==1.19.0
[pip3] torch==1.5.1
[pip3] torchvision==0.6.1
[conda] torch                     1.5.1                     <pip>
[conda] torchvision               0.6.1                     <pip>
        Pillow (7.1.2)

Building train transforms ...
+ resize to 256x128
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
+ random erase
Building test transforms ...
+ resize to 256x128
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
=> Loading train (source) dataset
=> Loaded Market1501
  ----------------------------------------
  subset   | # ids | # images | # cameras
  ----------------------------------------
  train    |   751 |    12936 |         6
  query    |   750 |     3368 |         6
  gallery  |   751 |    15913 |         6
  ----------------------------------------
=> Loading test (target) dataset
=> Loaded Market1501
  ----------------------------------------
  subset   | # ids | # images | # cameras
  ----------------------------------------
  train    |   751 |    12936 |         6
  query    |   750 |     3368 |         6
  gallery  |   751 |    15913 |         6
  ----------------------------------------


  **************** Summary ****************
  train            : ['market1501']
  # train datasets : 1
  # train ids      : 751
  # train images   : 12936
  # train cameras  : 6
  test             : ['market1501']
  *****************************************


Building model: plr_osnet
Successfully loaded imagenet pretrained weights from "/home/s2019020843/.cache/torch/checkpoints/osnet_x1_0_imagenet.pth"
Model complexity: params=4,034,696 flops=1,490,847,488
Model structure: PLR_OSNet(
  (layer0): Sequential(
    (0): ConvLayer(
      (conv): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (1): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
  )
  (layer1): Sequential(
    (0): OSBlock(
      (conv1): Conv1x1(
        (conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (conv2a): LightConv3x3(
        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)
        (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (conv2b): Sequential(
        (0): LightConv3x3(
          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)
          (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (1): LightConv3x3(
          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)
          (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
      )
      (conv2c): Sequential(
        (0): LightConv3x3(
          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)
          (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (1): LightConv3x3(
          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)
          (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (2): LightConv3x3(
          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)
          (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
      )
      (conv2d): Sequential(
        (0): LightConv3x3(
          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)
          (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (1): LightConv3x3(
          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)
          (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (2): LightConv3x3(
          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)
          (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (3): LightConv3x3(
          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)
          (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
      )
      (gate): ChannelGate(
        (global_avgpool): AdaptiveAvgPool2d(output_size=1)
        (fc1): Conv2d(64, 4, kernel_size=(1, 1), stride=(1, 1))
        (relu): ReLU(inplace=True)
        (fc2): Conv2d(4, 64, kernel_size=(1, 1), stride=(1, 1))
        (gate_activation): Sigmoid()
      )
      (conv3): Conv1x1Linear(
        (conv): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (downsample): Conv1x1Linear(
        (conv): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): OSBlock(
      (conv1): Conv1x1(
        (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (conv2a): LightConv3x3(
        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)
        (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (conv2b): Sequential(
        (0): LightConv3x3(
          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)
          (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (1): LightConv3x3(
          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)
          (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
      )
      (conv2c): Sequential(
        (0): LightConv3x3(
          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)
          (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (1): LightConv3x3(
          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)
          (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (2): LightConv3x3(
          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)
          (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
      )
      (conv2d): Sequential(
        (0): LightConv3x3(
          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)
          (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (1): LightConv3x3(
          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)
          (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (2): LightConv3x3(
          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)
          (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (3): LightConv3x3(
          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)
          (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
      )
      (gate): ChannelGate(
        (global_avgpool): AdaptiveAvgPool2d(output_size=1)
        (fc1): Conv2d(64, 4, kernel_size=(1, 1), stride=(1, 1))
        (relu): ReLU(inplace=True)
        (fc2): Conv2d(4, 64, kernel_size=(1, 1), stride=(1, 1))
        (gate_activation): Sigmoid()
      )
      (conv3): Conv1x1Linear(
        (conv): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (2): Sequential(
      (0): Conv1x1(
        (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (1): AvgPool2d(kernel_size=2, stride=2, padding=0)
    )
  )
  (attention_module1): Attention_Module(
    (pam): PAM_Module(
      (query_conv): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1))
      (key_conv): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1))
      (softmax): Softmax(dim=-1)
      (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (se): SEModule(
      (fc1): Conv2d(256, 16, kernel_size=(1, 1), stride=(1, 1))
      (relu): ReLU(inplace=True)
      (fc2): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1))
      (sigmoid): Sigmoid()
    )
  )
  (layer2): Sequential(
    (0): OSBlock(
      (conv1): Conv1x1(
        (conv): Conv2d(256, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (conv2a): LightConv3x3(
        (conv1): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv2): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
        (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (conv2b): Sequential(
        (0): LightConv3x3(
          (conv1): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (conv2): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
          (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (1): LightConv3x3(
          (conv1): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (conv2): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
          (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
      )
      (conv2c): Sequential(
        (0): LightConv3x3(
          (conv1): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (conv2): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
          (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (1): LightConv3x3(
          (conv1): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (conv2): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
          (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (2): LightConv3x3(
          (conv1): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (conv2): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
          (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
      )
      (conv2d): Sequential(
        (0): LightConv3x3(
          (conv1): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (conv2): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
          (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (1): LightConv3x3(
          (conv1): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (conv2): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
          (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (2): LightConv3x3(
          (conv1): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (conv2): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
          (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (3): LightConv3x3(
          (conv1): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (conv2): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
          (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
      )
      (gate): ChannelGate(
        (global_avgpool): AdaptiveAvgPool2d(output_size=1)
        (fc1): Conv2d(96, 6, kernel_size=(1, 1), stride=(1, 1))
        (relu): ReLU(inplace=True)
        (fc2): Conv2d(6, 96, kernel_size=(1, 1), stride=(1, 1))
        (gate_activation): Sigmoid()
      )
      (conv3): Conv1x1Linear(
        (conv): Conv2d(96, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (downsample): Conv1x1Linear(
        (conv): Conv2d(256, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): OSBlock(
      (conv1): Conv1x1(
        (conv): Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (conv2a): LightConv3x3(
        (conv1): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv2): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
        (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (conv2b): Sequential(
        (0): LightConv3x3(
          (conv1): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (conv2): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
          (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (1): LightConv3x3(
          (conv1): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (conv2): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
          (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
      )
      (conv2c): Sequential(
        (0): LightConv3x3(
          (conv1): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (conv2): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
          (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (1): LightConv3x3(
          (conv1): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (conv2): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
          (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (2): LightConv3x3(
          (conv1): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (conv2): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
          (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
      )
      (conv2d): Sequential(
        (0): LightConv3x3(
          (conv1): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (conv2): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
          (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (1): LightConv3x3(
          (conv1): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (conv2): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
          (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (2): LightConv3x3(
          (conv1): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (conv2): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
          (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (3): LightConv3x3(
          (conv1): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (conv2): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
          (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
      )
      (gate): ChannelGate(
        (global_avgpool): AdaptiveAvgPool2d(output_size=1)
        (fc1): Conv2d(96, 6, kernel_size=(1, 1), stride=(1, 1))
        (relu): ReLU(inplace=True)
        (fc2): Conv2d(6, 96, kernel_size=(1, 1), stride=(1, 1))
        (gate_activation): Sigmoid()
      )
      (conv3): Conv1x1Linear(
        (conv): Conv2d(96, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (2): Sequential(
      (0): Conv1x1(
        (conv): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (1): AvgPool2d(kernel_size=2, stride=2, padding=0)
    )
  )
  (attention_module2): Attention_Module(
    (pam): PAM_Module(
      (query_conv): Conv2d(384, 48, kernel_size=(1, 1), stride=(1, 1))
      (key_conv): Conv2d(384, 48, kernel_size=(1, 1), stride=(1, 1))
      (softmax): Softmax(dim=-1)
      (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (se): SEModule(
      (fc1): Conv2d(384, 24, kernel_size=(1, 1), stride=(1, 1))
      (relu): ReLU(inplace=True)
      (fc2): Conv2d(24, 384, kernel_size=(1, 1), stride=(1, 1))
      (sigmoid): Sigmoid()
    )
  )
  (layer3): Sequential(
    (0): OSBlock(
      (conv1): Conv1x1(
        (conv): Conv2d(384, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (conv2a): LightConv3x3(
        (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)
        (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (conv2b): Sequential(
        (0): LightConv3x3(
          (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)
          (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (1): LightConv3x3(
          (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)
          (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
      )
      (conv2c): Sequential(
        (0): LightConv3x3(
          (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)
          (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (1): LightConv3x3(
          (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)
          (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (2): LightConv3x3(
          (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)
          (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
      )
      (conv2d): Sequential(
        (0): LightConv3x3(
          (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)
          (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (1): LightConv3x3(
          (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)
          (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (2): LightConv3x3(
          (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)
          (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (3): LightConv3x3(
          (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)
          (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
      )
      (gate): ChannelGate(
        (global_avgpool): AdaptiveAvgPool2d(output_size=1)
        (fc1): Conv2d(128, 8, kernel_size=(1, 1), stride=(1, 1))
        (relu): ReLU(inplace=True)
        (fc2): Conv2d(8, 128, kernel_size=(1, 1), stride=(1, 1))
        (gate_activation): Sigmoid()
      )
      (conv3): Conv1x1Linear(
        (conv): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (downsample): Conv1x1Linear(
        (conv): Conv2d(384, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): OSBlock(
      (conv1): Conv1x1(
        (conv): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (conv2a): LightConv3x3(
        (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)
        (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (conv2b): Sequential(
        (0): LightConv3x3(
          (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)
          (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (1): LightConv3x3(
          (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)
          (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
      )
      (conv2c): Sequential(
        (0): LightConv3x3(
          (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)
          (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (1): LightConv3x3(
          (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)
          (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (2): LightConv3x3(
          (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)
          (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
      )
      (conv2d): Sequential(
        (0): LightConv3x3(
          (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)
          (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (1): LightConv3x3(
          (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)
          (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (2): LightConv3x3(
          (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)
          (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (3): LightConv3x3(
          (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)
          (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
      )
      (gate): ChannelGate(
        (global_avgpool): AdaptiveAvgPool2d(output_size=1)
        (fc1): Conv2d(128, 8, kernel_size=(1, 1), stride=(1, 1))
        (relu): ReLU(inplace=True)
        (fc2): Conv2d(8, 128, kernel_size=(1, 1), stride=(1, 1))
        (gate_activation): Sigmoid()
      )
      (conv3): Conv1x1Linear(
        (conv): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (layer4): Conv1x1(
    (conv): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
  )
  (conv10): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (conv20): Conv2d(384, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (global_avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
  (global_maxpool): AdaptiveMaxPool2d(output_size=(1, 1))
  (fc1): Linear(in_features=512, out_features=512, bias=True)
  (bn1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn2): BatchNorm1d(6144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (classifier1): Linear(in_features=512, out_features=751, bias=True)
  (classifier2): Linear(in_features=6144, out_features=751, bias=True)
)
Building triplet-engine for image-reid
=> Start training
Epoch: [1/150][20/185]	Time 0.497 (0.548)	Data 0.000 (0.024)	Loss_t 0.6439 (0.7075)	Loss_x 6.7555 (6.7569)	Acc 1.56 (0.23)	Lr 0.000035	eta 4:13:05
Epoch: [1/150][40/185]	Time 0.494 (0.511)	Data 0.000 (0.012)	Loss_t 0.5367 (0.6472)	Loss_x 6.6755 (6.7312)	Acc 1.56 (0.20)	Lr 0.000035	eta 3:55:46
Epoch: [1/150][60/185]	Time 0.495 (0.503)	Data 0.000 (0.008)	Loss_t 0.5473 (0.6099)	Loss_x 6.6437 (6.7045)	Acc 0.00 (0.13)	Lr 0.000035	eta 3:52:00
Epoch: [1/150][80/185]	Time 0.489 (0.499)	Data 0.000 (0.006)	Loss_t 0.4921 (0.5867)	Loss_x 6.6147 (6.6855)	Acc 0.00 (0.12)	Lr 0.000035	eta 3:50:05
Epoch: [1/150][100/185]	Time 0.459 (0.493)	Data 0.000 (0.005)	Loss_t 0.4483 (0.5656)	Loss_x 6.4914 (6.6579)	Acc 0.00 (0.12)	Lr 0.000035	eta 3:47:03
Epoch: [1/150][120/185]	Time 0.467 (0.489)	Data 0.000 (0.004)	Loss_t 0.4389 (0.5491)	Loss_x 6.4310 (6.6337)	Acc 3.12 (0.14)	Lr 0.000035	eta 3:45:23
Epoch: [1/150][140/185]	Time 0.489 (0.486)	Data 0.000 (0.004)	Loss_t 0.4825 (0.5361)	Loss_x 6.4937 (6.6106)	Acc 0.00 (0.18)	Lr 0.000035	eta 3:43:28
Epoch: [1/150][160/185]	Time 0.485 (0.486)	Data 0.000 (0.003)	Loss_t 0.4281 (0.5245)	Loss_x 6.3815 (6.5805)	Acc 0.00 (0.20)	Lr 0.000035	eta 3:43:16
Epoch: [1/150][180/185]	Time 0.481 (0.484)	Data 0.000 (0.003)	Loss_t 0.4255 (0.5140)	Loss_x 6.1590 (6.5441)	Acc 0.00 (0.25)	Lr 0.000035	eta 3:42:19
Epoch: [2/150][20/185]	Time 0.495 (0.496)	Data 0.000 (0.017)	Loss_t 0.3550 (0.3971)	Loss_x 6.1664 (6.2104)	Acc 0.00 (0.47)	Lr 0.000043	eta 3:47:46
Epoch: [2/150][40/185]	Time 0.523 (0.499)	Data 0.000 (0.009)	Loss_t 0.3937 (0.3900)	Loss_x 6.1734 (6.1639)	Acc 0.00 (0.51)	Lr 0.000043	eta 3:48:47
Epoch: [2/150][60/185]	Time 0.466 (0.491)	Data 0.000 (0.006)	Loss_t 0.3712 (0.3882)	Loss_x 6.0352 (6.1486)	Acc 1.56 (0.65)	Lr 0.000043	eta 3:45:11
Epoch: [2/150][80/185]	Time 0.452 (0.491)	Data 0.000 (0.004)	Loss_t 0.3698 (0.3855)	Loss_x 5.9600 (6.1096)	Acc 4.69 (0.94)	Lr 0.000043	eta 3:44:47
Epoch: [2/150][100/185]	Time 0.474 (0.488)	Data 0.000 (0.004)	Loss_t 0.3139 (0.3828)	Loss_x 5.8874 (6.0757)	Acc 3.12 (1.12)	Lr 0.000043	eta 3:43:23
Epoch: [2/150][120/185]	Time 0.569 (0.489)	Data 0.000 (0.003)	Loss_t 0.3644 (0.3801)	Loss_x 5.9649 (6.0383)	Acc 0.00 (1.26)	Lr 0.000043	eta 3:43:33
Epoch: [2/150][140/185]	Time 0.499 (0.488)	Data 0.000 (0.003)	Loss_t 0.3694 (0.3779)	Loss_x 5.6221 (5.9937)	Acc 1.56 (1.58)	Lr 0.000043	eta 3:43:15
Epoch: [2/150][160/185]	Time 0.485 (0.489)	Data 0.000 (0.002)	Loss_t 0.3799 (0.3755)	Loss_x 5.5460 (5.9423)	Acc 3.12 (1.91)	Lr 0.000043	eta 3:43:16
Epoch: [2/150][180/185]	Time 0.572 (0.489)	Data 0.000 (0.002)	Loss_t 0.3106 (0.3711)	Loss_x 4.8676 (5.8633)	Acc 15.62 (2.79)	Lr 0.000043	eta 3:43:06
Epoch: [3/150][20/185]	Time 0.494 (0.515)	Data 0.000 (0.023)	Loss_t 0.3462 (0.3288)	Loss_x 5.6568 (5.5127)	Acc 0.00 (5.16)	Lr 0.000051	eta 3:54:43
Epoch: [3/150][40/185]	Time 0.509 (0.500)	Data 0.000 (0.012)	Loss_t 0.3342 (0.3286)	Loss_x 5.3701 (5.4902)	Acc 10.94 (4.77)	Lr 0.000051	eta 3:47:54
Epoch: [3/150][60/185]	Time 0.448 (0.498)	Data 0.000 (0.008)	Loss_t 0.3198 (0.3235)	Loss_x 5.6934 (5.4675)	Acc 1.56 (4.71)	Lr 0.000051	eta 3:46:37
Epoch: [3/150][80/185]	Time 0.509 (0.493)	Data 0.000 (0.006)	Loss_t 0.3440 (0.3240)	Loss_x 5.1870 (5.4360)	Acc 7.81 (5.18)	Lr 0.000051	eta 3:44:10
Epoch: [3/150][100/185]	Time 0.447 (0.493)	Data 0.000 (0.005)	Loss_t 0.3275 (0.3228)	Loss_x 4.9491 (5.3754)	Acc 7.81 (5.97)	Lr 0.000051	eta 3:44:13
Epoch: [3/150][120/185]	Time 0.524 (0.491)	Data 0.000 (0.004)	Loss_t 0.2959 (0.3166)	Loss_x 4.8764 (5.3148)	Acc 12.50 (6.30)	Lr 0.000051	eta 3:43:11
Epoch: [3/150][140/185]	Time 0.524 (0.492)	Data 0.000 (0.003)	Loss_t 0.3288 (0.3169)	Loss_x 5.0575 (5.2672)	Acc 10.94 (7.11)	Lr 0.000051	eta 3:43:10
Epoch: [3/150][160/185]	Time 0.497 (0.491)	Data 0.000 (0.003)	Loss_t 0.2660 (0.3136)	Loss_x 4.6036 (5.1964)	Acc 31.25 (8.58)	Lr 0.000051	eta 3:42:39
Epoch: [3/150][180/185]	Time 0.451 (0.489)	Data 0.000 (0.003)	Loss_t 0.2823 (0.3119)	Loss_x 4.3344 (5.1140)	Acc 46.88 (10.73)	Lr 0.000051	eta 3:41:31
Epoch: [4/150][20/185]	Time 0.454 (0.501)	Data 0.000 (0.020)	Loss_t 0.3533 (0.2695)	Loss_x 4.7903 (4.8421)	Acc 9.38 (7.03)	Lr 0.000059	eta 3:46:42
Epoch: [4/150][40/185]	Time 0.562 (0.480)	Data 0.000 (0.010)	Loss_t 0.3485 (0.2778)	Loss_x 5.1114 (4.8523)	Acc 7.81 (7.11)	Lr 0.000059	eta 3:37:08
Epoch: [4/150][60/185]	Time 0.497 (0.483)	Data 0.000 (0.007)	Loss_t 0.2656 (0.2751)	Loss_x 4.8961 (4.8013)	Acc 1.56 (7.97)	Lr 0.000059	eta 3:38:25
Epoch: [4/150][80/185]	Time 0.425 (0.477)	Data 0.000 (0.005)	Loss_t 0.2058 (0.2710)	Loss_x 4.3743 (4.7567)	Acc 15.62 (9.16)	Lr 0.000059	eta 3:35:25
Epoch: [4/150][100/185]	Time 0.430 (0.475)	Data 0.000 (0.004)	Loss_t 0.2744 (0.2686)	Loss_x 4.3189 (4.6923)	Acc 17.19 (10.02)	Lr 0.000059	eta 3:34:25
Epoch: [4/150][120/185]	Time 0.432 (0.469)	Data 0.000 (0.003)	Loss_t 0.2375 (0.2668)	Loss_x 4.2210 (4.6482)	Acc 15.62 (11.13)	Lr 0.000059	eta 3:31:38
Epoch: [4/150][140/185]	Time 0.458 (0.467)	Data 0.000 (0.003)	Loss_t 0.2437 (0.2664)	Loss_x 4.2139 (4.5917)	Acc 25.00 (12.91)	Lr 0.000059	eta 3:30:45
Epoch: [4/150][160/185]	Time 0.483 (0.468)	Data 0.000 (0.003)	Loss_t 0.2697 (0.2661)	Loss_x 3.9376 (4.5306)	Acc 25.00 (14.40)	Lr 0.000059	eta 3:30:47
Epoch: [4/150][180/185]	Time 0.420 (0.467)	Data 0.000 (0.002)	Loss_t 0.3693 (0.2666)	Loss_x 3.8609 (4.4480)	Acc 39.06 (17.13)	Lr 0.000059	eta 3:30:03
Epoch: [5/150][20/185]	Time 0.518 (0.489)	Data 0.000 (0.018)	Loss_t 0.2434 (0.2532)	Loss_x 4.2816 (4.3486)	Acc 4.69 (9.30)	Lr 0.000067	eta 3:39:54
Epoch: [5/150][40/185]	Time 0.457 (0.479)	Data 0.000 (0.009)	Loss_t 0.2674 (0.2403)	Loss_x 4.2220 (4.2803)	Acc 23.44 (10.55)	Lr 0.000067	eta 3:35:11
Epoch: [5/150][60/185]	Time 0.441 (0.475)	Data 0.000 (0.006)	Loss_t 0.2212 (0.2394)	Loss_x 4.1736 (4.2419)	Acc 15.62 (11.61)	Lr 0.000067	eta 3:33:31
Epoch: [5/150][80/185]	Time 0.473 (0.473)	Data 0.000 (0.005)	Loss_t 0.1401 (0.2307)	Loss_x 3.7810 (4.1862)	Acc 18.75 (12.79)	Lr 0.000067	eta 3:32:17
Epoch: [5/150][100/185]	Time 0.425 (0.472)	Data 0.000 (0.004)	Loss_t 0.2919 (0.2299)	Loss_x 4.2043 (4.1402)	Acc 6.25 (14.03)	Lr 0.000067	eta 3:31:37
Epoch: [5/150][120/185]	Time 0.431 (0.469)	Data 0.000 (0.003)	Loss_t 0.2507 (0.2287)	Loss_x 3.8226 (4.0964)	Acc 15.62 (15.46)	Lr 0.000067	eta 3:30:07
Epoch: [5/150][140/185]	Time 0.550 (0.467)	Data 0.000 (0.003)	Loss_t 0.2546 (0.2276)	Loss_x 3.8065 (4.0443)	Acc 25.00 (17.31)	Lr 0.000067	eta 3:29:03
Epoch: [5/150][160/185]	Time 0.492 (0.466)	Data 0.000 (0.002)	Loss_t 0.1931 (0.2284)	Loss_x 3.5143 (3.9903)	Acc 32.81 (19.60)	Lr 0.000067	eta 3:28:38
Epoch: [5/150][180/185]	Time 0.447 (0.468)	Data 0.000 (0.002)	Loss_t 0.3099 (0.2271)	Loss_x 3.2711 (3.9106)	Acc 45.31 (23.26)	Lr 0.000067	eta 3:29:08
Epoch: [6/150][20/185]	Time 0.480 (0.510)	Data 0.000 (0.023)	Loss_t 0.1635 (0.2143)	Loss_x 3.9859 (3.8987)	Acc 7.81 (12.81)	Lr 0.000075	eta 3:47:41
Epoch: [6/150][40/185]	Time 0.483 (0.497)	Data 0.000 (0.011)	Loss_t 0.1470 (0.2041)	Loss_x 3.5070 (3.8239)	Acc 21.88 (14.53)	Lr 0.000075	eta 3:42:00
Epoch: [6/150][60/185]	Time 0.477 (0.484)	Data 0.000 (0.008)	Loss_t 0.1834 (0.2009)	Loss_x 3.3407 (3.7701)	Acc 34.38 (15.68)	Lr 0.000075	eta 3:36:03
Epoch: [6/150][80/185]	Time 0.435 (0.476)	Data 0.000 (0.006)	Loss_t 0.1381 (0.2002)	Loss_x 3.7566 (3.7389)	Acc 0.00 (16.17)	Lr 0.000075	eta 3:32:17
Epoch: [6/150][100/185]	Time 0.463 (0.473)	Data 0.000 (0.005)	Loss_t 0.2074 (0.2022)	Loss_x 3.4946 (3.6892)	Acc 26.56 (18.02)	Lr 0.000075	eta 3:30:50
Epoch: [6/150][120/185]	Time 0.455 (0.474)	Data 0.000 (0.004)	Loss_t 0.1903 (0.1994)	Loss_x 3.5363 (3.6502)	Acc 31.25 (19.47)	Lr 0.000075	eta 3:30:53
Epoch: [6/150][140/185]	Time 0.441 (0.472)	Data 0.000 (0.003)	Loss_t 0.1999 (0.1974)	Loss_x 3.2843 (3.5953)	Acc 23.44 (21.65)	Lr 0.000075	eta 3:29:56
Epoch: [6/150][160/185]	Time 0.489 (0.473)	Data 0.000 (0.003)	Loss_t 0.1487 (0.1952)	Loss_x 3.0400 (3.5357)	Acc 56.25 (24.81)	Lr 0.000075	eta 3:30:12
Epoch: [6/150][180/185]	Time 0.544 (0.473)	Data 0.000 (0.003)	Loss_t 0.1571 (0.1955)	Loss_x 2.6529 (3.4698)	Acc 75.00 (28.58)	Lr 0.000075	eta 3:30:04
Epoch: [7/150][20/185]	Time 0.440 (0.487)	Data 0.000 (0.022)	Loss_t 0.1441 (0.1837)	Loss_x 3.3232 (3.4474)	Acc 20.31 (19.53)	Lr 0.000083	eta 3:36:10
Epoch: [7/150][40/185]	Time 0.552 (0.484)	Data 0.000 (0.011)	Loss_t 0.2228 (0.1771)	Loss_x 3.1952 (3.4056)	Acc 43.75 (20.27)	Lr 0.000083	eta 3:34:40
Epoch: [7/150][60/185]	Time 0.465 (0.489)	Data 0.000 (0.007)	Loss_t 0.1226 (0.1767)	Loss_x 3.4111 (3.3901)	Acc 14.06 (20.73)	Lr 0.000083	eta 3:36:26
Epoch: [7/150][80/185]	Time 0.440 (0.484)	Data 0.000 (0.006)	Loss_t 0.1537 (0.1739)	Loss_x 3.0700 (3.3501)	Acc 39.06 (21.93)	Lr 0.000083	eta 3:34:02
Epoch: [7/150][100/185]	Time 0.485 (0.481)	Data 0.000 (0.005)	Loss_t 0.2024 (0.1755)	Loss_x 3.2443 (3.3130)	Acc 31.25 (24.33)	Lr 0.000083	eta 3:32:36
Epoch: [7/150][120/185]	Time 0.496 (0.480)	Data 0.000 (0.004)	Loss_t 0.0992 (0.1733)	Loss_x 2.8573 (3.2700)	Acc 56.25 (26.68)	Lr 0.000083	eta 3:32:10
Epoch: [7/150][140/185]	Time 0.515 (0.481)	Data 0.000 (0.003)	Loss_t 0.2202 (0.1718)	Loss_x 2.7881 (3.2230)	Acc 45.31 (29.05)	Lr 0.000083	eta 3:32:39
Epoch: [7/150][160/185]	Time 0.522 (0.480)	Data 0.000 (0.003)	Loss_t 0.0958 (0.1691)	Loss_x 2.7399 (3.1642)	Acc 50.00 (32.50)	Lr 0.000083	eta 3:31:57
Epoch: [7/150][180/185]	Time 0.513 (0.481)	Data 0.000 (0.003)	Loss_t 0.1436 (0.1683)	Loss_x 2.3032 (3.0927)	Acc 75.00 (36.52)	Lr 0.000083	eta 3:32:19
Epoch: [8/150][20/185]	Time 0.506 (0.502)	Data 0.000 (0.019)	Loss_t 0.1633 (0.1488)	Loss_x 3.0435 (3.1379)	Acc 21.88 (24.53)	Lr 0.000092	eta 3:41:02
Epoch: [8/150][40/185]	Time 0.485 (0.481)	Data 0.000 (0.010)	Loss_t 0.2072 (0.1442)	Loss_x 3.2776 (3.0886)	Acc 9.38 (26.21)	Lr 0.000092	eta 3:31:47
Epoch: [8/150][60/185]	Time 0.502 (0.483)	Data 0.000 (0.007)	Loss_t 0.1064 (0.1448)	Loss_x 3.0843 (3.0411)	Acc 20.31 (29.48)	Lr 0.000092	eta 3:32:30
Epoch: [8/150][80/185]	Time 0.527 (0.483)	Data 0.000 (0.005)	Loss_t 0.1812 (0.1492)	Loss_x 3.0729 (3.0114)	Acc 26.56 (31.02)	Lr 0.000092	eta 3:32:26
Epoch: [8/150][100/185]	Time 0.529 (0.485)	Data 0.000 (0.004)	Loss_t 0.1263 (0.1499)	Loss_x 2.6992 (2.9829)	Acc 43.75 (33.02)	Lr 0.000092	eta 3:33:05
Epoch: [8/150][120/185]	Time 0.548 (0.480)	Data 0.000 (0.003)	Loss_t 0.1102 (0.1480)	Loss_x 2.5430 (2.9495)	Acc 56.25 (34.96)	Lr 0.000092	eta 3:30:52
Epoch: [8/150][140/185]	Time 0.474 (0.481)	Data 0.000 (0.003)	Loss_t 0.1866 (0.1482)	Loss_x 2.6856 (2.8981)	Acc 42.19 (37.86)	Lr 0.000092	eta 3:31:08
Epoch: [8/150][160/185]	Time 0.517 (0.483)	Data 0.000 (0.003)	Loss_t 0.1273 (0.1451)	Loss_x 2.4555 (2.8468)	Acc 65.62 (40.45)	Lr 0.000092	eta 3:31:33
Epoch: [8/150][180/185]	Time 0.448 (0.485)	Data 0.000 (0.002)	Loss_t 0.1279 (0.1428)	Loss_x 2.1453 (2.7770)	Acc 87.50 (44.29)	Lr 0.000092	eta 3:32:25
Epoch: [9/150][20/185]	Time 0.539 (0.517)	Data 0.000 (0.019)	Loss_t 0.1235 (0.1195)	Loss_x 2.4871 (2.7740)	Acc 51.56 (34.84)	Lr 0.000100	eta 3:46:21
Epoch: [9/150][40/185]	Time 0.518 (0.488)	Data 0.000 (0.010)	Loss_t 0.1335 (0.1247)	Loss_x 2.7270 (2.7780)	Acc 34.38 (35.59)	Lr 0.000100	eta 3:33:27
Epoch: [9/150][60/185]	Time 0.488 (0.482)	Data 0.000 (0.007)	Loss_t 0.0724 (0.1240)	Loss_x 2.9094 (2.7718)	Acc 26.56 (36.46)	Lr 0.000100	eta 3:30:40
Epoch: [9/150][80/185]	Time 0.426 (0.480)	Data 0.000 (0.005)	Loss_t 0.1961 (0.1255)	Loss_x 2.5804 (2.7427)	Acc 53.12 (38.98)	Lr 0.000100	eta 3:29:29
Epoch: [9/150][100/185]	Time 0.431 (0.475)	Data 0.000 (0.004)	Loss_t 0.0872 (0.1231)	Loss_x 2.6055 (2.7062)	Acc 53.12 (41.55)	Lr 0.000100	eta 3:27:07
Epoch: [9/150][120/185]	Time 0.448 (0.476)	Data 0.000 (0.003)	Loss_t 0.1413 (0.1230)	Loss_x 2.4314 (2.6683)	Acc 62.50 (43.70)	Lr 0.000100	eta 3:27:36
Epoch: [9/150][140/185]	Time 0.488 (0.476)	Data 0.000 (0.003)	Loss_t 0.1444 (0.1235)	Loss_x 2.1285 (2.6223)	Acc 84.38 (46.86)	Lr 0.000100	eta 3:27:17
Epoch: [9/150][160/185]	Time 0.479 (0.475)	Data 0.000 (0.003)	Loss_t 0.0861 (0.1217)	Loss_x 2.0848 (2.5689)	Acc 78.12 (50.10)	Lr 0.000100	eta 3:26:47
Epoch: [9/150][180/185]	Time 0.491 (0.474)	Data 0.000 (0.002)	Loss_t 0.1053 (0.1223)	Loss_x 1.7148 (2.5032)	Acc 92.19 (53.80)	Lr 0.000100	eta 3:26:17
Epoch: [10/150][20/185]	Time 0.415 (0.440)	Data 0.000 (0.017)	Loss_t 0.1070 (0.1120)	Loss_x 2.4069 (2.6589)	Acc 59.38 (36.09)	Lr 0.000108	eta 3:11:08
Epoch: [10/150][40/185]	Time 0.462 (0.455)	Data 0.000 (0.009)	Loss_t 0.0848 (0.1121)	Loss_x 2.2911 (2.6042)	Acc 60.94 (40.20)	Lr 0.000108	eta 3:17:32
Epoch: [10/150][60/185]	Time 0.452 (0.456)	Data 0.000 (0.006)	Loss_t 0.1007 (0.1109)	Loss_x 2.3215 (2.5558)	Acc 57.81 (43.88)	Lr 0.000108	eta 3:17:57
Epoch: [10/150][80/185]	Time 0.495 (0.460)	Data 0.000 (0.004)	Loss_t 0.1425 (0.1078)	Loss_x 2.2669 (2.5021)	Acc 65.62 (47.54)	Lr 0.000108	eta 3:19:22
Epoch: [10/150][100/185]	Time 0.498 (0.465)	Data 0.000 (0.004)	Loss_t 0.1418 (0.1061)	Loss_x 2.4258 (2.4565)	Acc 53.12 (50.20)	Lr 0.000108	eta 3:21:10
Epoch: [10/150][120/185]	Time 0.501 (0.466)	Data 0.000 (0.003)	Loss_t 0.1447 (0.1059)	Loss_x 2.1437 (2.4160)	Acc 68.75 (53.19)	Lr 0.000108	eta 3:21:38
Epoch: [10/150][140/185]	Time 0.443 (0.466)	Data 0.000 (0.003)	Loss_t 0.0753 (0.1070)	Loss_x 1.9928 (2.3745)	Acc 75.00 (55.81)	Lr 0.000108	eta 3:21:36
Epoch: [10/150][160/185]	Time 0.428 (0.467)	Data 0.000 (0.002)	Loss_t 0.0962 (0.1064)	Loss_x 1.8067 (2.3236)	Acc 90.62 (58.80)	Lr 0.000108	eta 3:21:38
Epoch: [10/150][180/185]	Time 0.491 (0.468)	Data 0.000 (0.002)	Loss_t 0.0919 (0.1044)	Loss_x 1.7712 (2.2632)	Acc 81.25 (61.97)	Lr 0.000108	eta 3:21:54
##### Evaluating market1501 (source) #####
Extracting features from query set ...
Done, obtained 3368-by-6656 matrix
Extracting features from gallery set ...
Done, obtained 15913-by-6656 matrix
Speed: 0.0967 sec/batch
Computing distance matrix with metric=euclidean ...
Computing CMC and mAP ...
** Results **
mAP: 47.9%
CMC curve
Rank-1  : 69.1%
Rank-5  : 85.2%
Rank-10 : 90.4%
Rank-20 : 94.2%
Checkpoint saved to "log/model.pth.tar-10"
Epoch: [11/150][20/185]	Time 0.531 (0.530)	Data 0.000 (0.026)	Loss_t 0.1614 (0.0913)	Loss_x 2.5470 (2.4291)	Acc 53.12 (47.50)	Lr 0.000116	eta 3:48:37
Epoch: [11/150][40/185]	Time 0.478 (0.502)	Data 0.000 (0.013)	Loss_t 0.1254 (0.0976)	Loss_x 2.1552 (2.3659)	Acc 67.19 (50.55)	Lr 0.000116	eta 3:36:18
Epoch: [11/150][60/185]	Time 0.462 (0.491)	Data 0.000 (0.009)	Loss_t 0.0980 (0.0989)	Loss_x 2.1771 (2.3351)	Acc 67.19 (52.60)	Lr 0.000116	eta 3:31:36
Epoch: [11/150][80/185]	Time 0.494 (0.484)	Data 0.000 (0.007)	Loss_t 0.0729 (0.1008)	Loss_x 1.9538 (2.3077)	Acc 71.88 (54.88)	Lr 0.000116	eta 3:28:24
Epoch: [11/150][100/185]	Time 0.472 (0.481)	Data 0.000 (0.005)	Loss_t 0.1521 (0.0996)	Loss_x 2.3018 (2.2668)	Acc 60.94 (57.91)	Lr 0.000116	eta 3:26:44
Epoch: [11/150][120/185]	Time 0.499 (0.479)	Data 0.000 (0.004)	Loss_t 0.0552 (0.0964)	Loss_x 1.6147 (2.2168)	Acc 98.44 (60.99)	Lr 0.000116	eta 3:25:51
Epoch: [11/150][140/185]	Time 0.429 (0.475)	Data 0.000 (0.004)	Loss_t 0.1205 (0.0955)	Loss_x 1.6954 (2.1649)	Acc 85.94 (64.06)	Lr 0.000116	eta 3:23:47
Epoch: [11/150][160/185]	Time 0.483 (0.474)	Data 0.000 (0.003)	Loss_t 0.0975 (0.0950)	Loss_x 1.5962 (2.1205)	Acc 89.06 (66.57)	Lr 0.000116	eta 3:23:25
Epoch: [11/150][180/185]	Time 0.495 (0.476)	Data 0.000 (0.003)	Loss_t 0.1129 (0.0942)	Loss_x 1.5457 (2.0681)	Acc 93.75 (69.32)	Lr 0.000116	eta 3:24:05
Epoch: [12/150][20/185]	Time 0.440 (0.491)	Data 0.000 (0.023)	Loss_t 0.1081 (0.0902)	Loss_x 2.3377 (2.2794)	Acc 53.12 (49.69)	Lr 0.000124	eta 3:30:19
Epoch: [12/150][40/185]	Time 0.473 (0.479)	Data 0.000 (0.012)	Loss_t 0.0694 (0.0966)	Loss_x 2.0911 (2.2266)	Acc 62.50 (54.41)	Lr 0.000124	eta 3:24:48
Epoch: [12/150][60/185]	Time 0.493 (0.483)	Data 0.000 (0.008)	Loss_t 0.0564 (0.0932)	Loss_x 1.8693 (2.1766)	Acc 79.69 (58.44)	Lr 0.000124	eta 3:26:20
Epoch: [12/150][80/185]	Time 0.473 (0.483)	Data 0.000 (0.006)	Loss_t 0.0747 (0.0921)	Loss_x 1.8475 (2.1246)	Acc 82.81 (61.66)	Lr 0.000124	eta 3:26:14
Epoch: [12/150][100/185]	Time 0.444 (0.484)	Data 0.000 (0.005)	Loss_t 0.0462 (0.0908)	Loss_x 1.6558 (2.0715)	Acc 89.06 (65.03)	Lr 0.000124	eta 3:26:28
Epoch: [12/150][120/185]	Time 0.465 (0.483)	Data 0.000 (0.004)	Loss_t 0.0506 (0.0889)	Loss_x 1.7514 (2.0312)	Acc 79.69 (67.72)	Lr 0.000124	eta 3:26:07
Epoch: [12/150][140/185]	Time 0.522 (0.484)	Data 0.000 (0.003)	Loss_t 0.0763 (0.0890)	Loss_x 1.6710 (1.9951)	Acc 82.81 (70.19)	Lr 0.000124	eta 3:26:05
Epoch: [12/150][160/185]	Time 0.436 (0.479)	Data 0.000 (0.003)	Loss_t 0.0655 (0.0866)	Loss_x 1.5233 (1.9475)	Acc 93.75 (72.48)	Lr 0.000124	eta 3:23:56
Epoch: [12/150][180/185]	Time 0.499 (0.478)	Data 0.000 (0.003)	Loss_t 0.0406 (0.0853)	Loss_x 1.3866 (1.8979)	Acc 98.44 (74.99)	Lr 0.000124	eta 3:23:26
Epoch: [13/150][20/185]	Time 0.497 (0.505)	Data 0.000 (0.017)	Loss_t 0.0860 (0.0729)	Loss_x 2.1937 (2.0471)	Acc 42.19 (60.23)	Lr 0.000132	eta 3:34:52
Epoch: [13/150][40/185]	Time 0.433 (0.481)	Data 0.000 (0.009)	Loss_t 0.0809 (0.0790)	Loss_x 1.9222 (2.0369)	Acc 60.94 (62.85)	Lr 0.000132	eta 3:24:27
Epoch: [13/150][60/185]	Time 0.444 (0.478)	Data 0.000 (0.006)	Loss_t 0.0812 (0.0795)	Loss_x 1.6689 (1.9720)	Acc 81.25 (67.06)	Lr 0.000132	eta 3:22:46
Epoch: [13/150][80/185]	Time 0.470 (0.477)	Data 0.000 (0.004)	Loss_t 0.0619 (0.0770)	Loss_x 1.8978 (1.9436)	Acc 76.56 (69.41)	Lr 0.000132	eta 3:22:25
Epoch: [13/150][100/185]	Time 0.427 (0.477)	Data 0.000 (0.004)	Loss_t 0.1141 (0.0791)	Loss_x 1.7168 (1.9131)	Acc 87.50 (71.81)	Lr 0.000132	eta 3:22:12
Epoch: [13/150][120/185]	Time 0.489 (0.478)	Data 0.000 (0.003)	Loss_t 0.1203 (0.0790)	Loss_x 1.7757 (1.8843)	Acc 84.38 (73.62)	Lr 0.000132	eta 3:22:18
Epoch: [13/150][140/185]	Time 0.414 (0.475)	Data 0.000 (0.003)	Loss_t 0.1053 (0.0803)	Loss_x 1.5452 (1.8442)	Acc 95.31 (75.80)	Lr 0.000132	eta 3:20:48
Epoch: [13/150][160/185]	Time 0.476 (0.471)	Data 0.000 (0.002)	Loss_t 0.0591 (0.0794)	Loss_x 1.4584 (1.8023)	Acc 95.31 (77.82)	Lr 0.000132	eta 3:19:03
Epoch: [13/150][180/185]	Time 0.453 (0.470)	Data 0.000 (0.002)	Loss_t 0.0675 (0.0783)	Loss_x 1.2908 (1.7583)	Acc 98.44 (79.89)	Lr 0.000132	eta 3:18:30
Epoch: [14/150][20/185]	Time 0.522 (0.508)	Data 0.000 (0.018)	Loss_t 0.0478 (0.0727)	Loss_x 1.9430 (1.9403)	Acc 65.62 (66.95)	Lr 0.000140	eta 3:34:30
Epoch: [14/150][40/185]	Time 0.434 (0.488)	Data 0.000 (0.009)	Loss_t 0.0907 (0.0699)	Loss_x 1.8721 (1.8748)	Acc 70.31 (70.62)	Lr 0.000140	eta 3:26:01
Epoch: [14/150][60/185]	Time 0.441 (0.485)	Data 0.000 (0.006)	Loss_t 0.0605 (0.0705)	Loss_x 1.6022 (1.8302)	Acc 84.38 (73.85)	Lr 0.000140	eta 3:24:27
Epoch: [14/150][80/185]	Time 0.508 (0.482)	Data 0.000 (0.005)	Loss_t 0.0757 (0.0708)	Loss_x 1.7925 (1.8068)	Acc 70.31 (75.45)	Lr 0.000140	eta 3:23:07
Epoch: [14/150][100/185]	Time 0.428 (0.478)	Data 0.000 (0.004)	Loss_t 0.0646 (0.0721)	Loss_x 1.4588 (1.7664)	Acc 90.62 (77.69)	Lr 0.000140	eta 3:21:10
Epoch: [14/150][120/185]	Time 0.506 (0.480)	Data 0.000 (0.003)	Loss_t 0.0419 (0.0700)	Loss_x 1.5116 (1.7296)	Acc 93.75 (79.82)	Lr 0.000140	eta 3:21:41
Epoch: [14/150][140/185]	Time 0.471 (0.478)	Data 0.000 (0.003)	Loss_t 0.0614 (0.0699)	Loss_x 1.4506 (1.6971)	Acc 90.62 (81.42)	Lr 0.000140	eta 3:20:56
Epoch: [14/150][160/185]	Time 0.506 (0.478)	Data 0.000 (0.002)	Loss_t 0.0757 (0.0696)	Loss_x 1.3642 (1.6659)	Acc 96.88 (82.82)	Lr 0.000140	eta 3:20:47
Epoch: [14/150][180/185]	Time 0.496 (0.479)	Data 0.000 (0.002)	Loss_t 0.0204 (0.0685)	Loss_x 1.2478 (1.6325)	Acc 100.00 (84.21)	Lr 0.000140	eta 3:20:46
Epoch: [15/150][20/185]	Time 0.435 (0.513)	Data 0.000 (0.019)	Loss_t 0.0781 (0.0745)	Loss_x 2.0149 (1.8080)	Acc 60.94 (72.89)	Lr 0.000148	eta 3:35:05
Epoch: [15/150][40/185]	Time 0.472 (0.498)	Data 0.000 (0.009)	Loss_t 0.0566 (0.0688)	Loss_x 1.7315 (1.7532)	Acc 85.94 (76.25)	Lr 0.000148	eta 3:28:20
Epoch: [15/150][60/185]	Time 0.489 (0.494)	Data 0.000 (0.006)	Loss_t 0.0331 (0.0695)	Loss_x 1.5880 (1.7103)	Acc 90.62 (79.38)	Lr 0.000148	eta 3:26:37
Epoch: [15/150][80/185]	Time 0.436 (0.493)	Data 0.000 (0.005)	Loss_t 0.1112 (0.0691)	Loss_x 1.6163 (1.6767)	Acc 85.94 (81.11)	Lr 0.000148	eta 3:26:08
Epoch: [15/150][100/185]	Time 0.530 (0.491)	Data 0.000 (0.004)	Loss_t 0.0501 (0.0684)	Loss_x 1.6334 (1.6514)	Acc 79.69 (82.55)	Lr 0.000148	eta 3:25:10
Epoch: [15/150][120/185]	Time 0.463 (0.492)	Data 0.000 (0.003)	Loss_t 0.0645 (0.0663)	Loss_x 1.4813 (1.6235)	Acc 93.75 (83.92)	Lr 0.000148	eta 3:25:25
Epoch: [15/150][140/185]	Time 0.528 (0.490)	Data 0.000 (0.003)	Loss_t 0.0229 (0.0665)	Loss_x 1.3393 (1.6017)	Acc 96.88 (84.93)	Lr 0.000148	eta 3:24:10
Epoch: [15/150][160/185]	Time 0.413 (0.490)	Data 0.000 (0.002)	Loss_t 0.0735 (0.0663)	Loss_x 1.3753 (1.5737)	Acc 95.31 (86.32)	Lr 0.000148	eta 3:24:06
Epoch: [15/150][180/185]	Time 0.450 (0.490)	Data 0.000 (0.002)	Loss_t 0.0567 (0.0646)	Loss_x 1.2684 (1.5423)	Acc 98.44 (87.60)	Lr 0.000148	eta 3:23:51
Epoch: [16/150][20/185]	Time 0.490 (0.502)	Data 0.000 (0.018)	Loss_t 0.0579 (0.0620)	Loss_x 1.5869 (1.6662)	Acc 82.81 (78.59)	Lr 0.000156	eta 3:28:52
Epoch: [16/150][40/185]	Time 0.538 (0.506)	Data 0.000 (0.009)	Loss_t 0.0547 (0.0588)	Loss_x 1.4963 (1.6045)	Acc 93.75 (82.42)	Lr 0.000156	eta 3:30:17
Epoch: [16/150][60/185]	Time 0.440 (0.494)	Data 0.000 (0.006)	Loss_t 0.0394 (0.0579)	Loss_x 1.3759 (1.5746)	Acc 100.00 (84.17)	Lr 0.000156	eta 3:25:05
Epoch: [16/150][80/185]	Time 0.492 (0.490)	Data 0.000 (0.005)	Loss_t 0.0476 (0.0588)	Loss_x 1.4896 (1.5541)	Acc 96.88 (85.86)	Lr 0.000156	eta 3:23:21
Epoch: [16/150][100/185]	Time 0.462 (0.492)	Data 0.000 (0.004)	Loss_t 0.1191 (0.0594)	Loss_x 1.6436 (1.5437)	Acc 82.81 (86.30)	Lr 0.000156	eta 3:23:54
Epoch: [16/150][120/185]	Time 0.443 (0.489)	Data 0.000 (0.003)	Loss_t 0.0982 (0.0582)	Loss_x 1.4587 (1.5226)	Acc 93.75 (87.58)	Lr 0.000156	eta 3:22:37
Epoch: [16/150][140/185]	Time 0.466 (0.489)	Data 0.000 (0.003)	Loss_t 0.0375 (0.0585)	Loss_x 1.2532 (1.5017)	Acc 100.00 (88.63)	Lr 0.000156	eta 3:22:33
Epoch: [16/150][160/185]	Time 0.498 (0.488)	Data 0.000 (0.002)	Loss_t 0.0716 (0.0580)	Loss_x 1.3501 (1.4815)	Acc 93.75 (89.47)	Lr 0.000156	eta 3:21:56
Epoch: [16/150][180/185]	Time 0.489 (0.487)	Data 0.000 (0.002)	Loss_t 0.0313 (0.0572)	Loss_x 1.1848 (1.4552)	Acc 98.44 (90.42)	Lr 0.000156	eta 3:21:14
Epoch: [17/150][20/185]	Time 0.461 (0.511)	Data 0.000 (0.019)	Loss_t 0.0526 (0.0648)	Loss_x 1.4633 (1.5512)	Acc 93.75 (85.55)	Lr 0.000164	eta 3:30:49
Epoch: [17/150][40/185]	Time 0.450 (0.509)	Data 0.000 (0.010)	Loss_t 0.0279 (0.0620)	Loss_x 1.4804 (1.5437)	Acc 93.75 (85.94)	Lr 0.000164	eta 3:30:01
Epoch: [17/150][60/185]	Time 0.474 (0.513)	Data 0.000 (0.006)	Loss_t 0.0140 (0.0610)	Loss_x 1.3392 (1.5184)	Acc 93.75 (87.16)	Lr 0.000164	eta 3:31:18
Epoch: [17/150][80/185]	Time 0.520 (0.507)	Data 0.000 (0.005)	Loss_t 0.0746 (0.0601)	Loss_x 1.3749 (1.4979)	Acc 95.31 (88.34)	Lr 0.000164	eta 3:28:58
Epoch: [17/150][100/185]	Time 0.495 (0.504)	Data 0.000 (0.004)	Loss_t 0.0242 (0.0594)	Loss_x 1.2614 (1.4785)	Acc 100.00 (89.34)	Lr 0.000164	eta 3:27:18
Epoch: [17/150][120/185]	Time 0.467 (0.496)	Data 0.000 (0.003)	Loss_t 0.0625 (0.0578)	Loss_x 1.4554 (1.4592)	Acc 82.81 (90.16)	Lr 0.000164	eta 3:23:58
Epoch: [17/150][140/185]	Time 0.445 (0.495)	Data 0.000 (0.003)	Loss_t 0.0682 (0.0571)	Loss_x 1.2915 (1.4414)	Acc 96.88 (90.92)	Lr 0.000164	eta 3:23:16
Epoch: [17/150][160/185]	Time 0.491 (0.492)	Data 0.000 (0.003)	Loss_t 0.0731 (0.0565)	Loss_x 1.3073 (1.4231)	Acc 95.31 (91.66)	Lr 0.000164	eta 3:21:50
Epoch: [17/150][180/185]	Time 0.433 (0.492)	Data 0.000 (0.002)	Loss_t 0.0580 (0.0562)	Loss_x 1.2330 (1.4053)	Acc 100.00 (92.34)	Lr 0.000164	eta 3:21:39
Epoch: [18/150][20/185]	Time 0.453 (0.489)	Data 0.001 (0.020)	Loss_t 0.0695 (0.0590)	Loss_x 1.4769 (1.4601)	Acc 93.75 (90.00)	Lr 0.000172	eta 3:20:22
Epoch: [18/150][40/185]	Time 0.509 (0.486)	Data 0.000 (0.010)	Loss_t 0.0515 (0.0541)	Loss_x 1.3558 (1.4686)	Acc 93.75 (89.61)	Lr 0.000172	eta 3:18:46
Epoch: [18/150][60/185]	Time 0.462 (0.482)	Data 0.000 (0.007)	Loss_t 0.0445 (0.0533)	Loss_x 1.3783 (1.4420)	Acc 100.00 (91.02)	Lr 0.000172	eta 3:17:15
Epoch: [18/150][80/185]	Time 0.451 (0.482)	Data 0.000 (0.005)	Loss_t 0.0259 (0.0528)	Loss_x 1.2934 (1.4225)	Acc 96.88 (91.68)	Lr 0.000172	eta 3:16:51
Epoch: [18/150][100/185]	Time 0.523 (0.483)	Data 0.000 (0.004)	Loss_t 0.0277 (0.0520)	Loss_x 1.2563 (1.4076)	Acc 96.88 (92.36)	Lr 0.000172	eta 3:17:11
Epoch: [18/150][120/185]	Time 0.456 (0.481)	Data 0.000 (0.003)	Loss_t 0.0376 (0.0509)	Loss_x 1.3940 (1.3948)	Acc 100.00 (92.93)	Lr 0.000172	eta 3:16:10
Epoch: [18/150][140/185]	Time 0.495 (0.482)	Data 0.000 (0.003)	Loss_t 0.0698 (0.0533)	Loss_x 1.2349 (1.3834)	Acc 98.44 (93.29)	Lr 0.000172	eta 3:16:28
Epoch: [18/150][160/185]	Time 0.490 (0.480)	Data 0.000 (0.003)	Loss_t 0.0158 (0.0537)	Loss_x 1.2227 (1.3719)	Acc 98.44 (93.84)	Lr 0.000172	eta 3:15:33
Epoch: [18/150][180/185]	Time 0.468 (0.479)	Data 0.000 (0.002)	Loss_t 0.0411 (0.0529)	Loss_x 1.2332 (1.3566)	Acc 98.44 (94.28)	Lr 0.000172	eta 3:14:56
