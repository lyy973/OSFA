Show configuration
adam:
  beta1: 0.9
  beta2: 0.999
cuhk03:
  classic_split: False
  labeled_images: True
  use_metric_cuhk03: False
data:
  combineall: False
  height: 256
  norm_mean: [0.485, 0.456, 0.406]
  norm_std: [0.229, 0.224, 0.225]
  root: /home/s2019020843/changeeeee/data/
  save_dir: log
  sources: ['market1501']
  split_id: 0
  targets: ['market1501']
  transforms: ['random_flip', 'random_erase']
  type: image
  width: 128
  workers: 4
loss:
  name: triplet
  softmax:
    label_smooth: True
  triplet:
    margin: 0.15
    weight_t: 1.0
    weight_x: 1.0
market1501:
  use_500k_distractors: False
model:
  load_weights: 
  name: plr_osnet
  pretrained: True
  resume: 
rmsprop:
  alpha: 0.99
sampler:
  num_instances: 4
  train_sampler: RandomIdentitySampler
sgd:
  dampening: 0.0
  momentum: 0.9
  nesterov: False
test:
  batch_size: 300
  dist_metric: euclidean
  eval_freq: 10
  evaluate: False
  normalize_feature: False
  ranks: [1, 5, 10, 20]
  rerank: False
  start_eval: 0
  visactmap: False
  visrank: False
  visrank_topk: 10
train:
  base_lr_mult: 0.1
  batch_size: 64
  fixbase_epoch: 0
  gamma: 0.1
  lr: 3.5e-05
  lr_scheduler: warmup
  max_epoch: 150
  multiplier: 10
  new_layers: ['classifier']
  open_layers: ['classifier']
  optim: adam
  print_freq: 20
  seed: 1
  staged_lr: False
  start_epoch: 0
  stepsize: [60, 90]
  total_epoch: 39
  weight_decay: 0.0005
use_gpu: True
video:
  pooling_method: avg
  sample_method: evenly
  seq_len: 15

Collecting env info ...
** System info **
PyTorch version: 1.5.1
Is debug build: No
CUDA used to build PyTorch: 10.2

OS: CentOS Linux 7 (Core)
GCC version: (GCC) 4.8.5 20150623 (Red Hat 4.8.5-39)
CMake version: Could not collect

Python version: 3.7
Is CUDA available: Yes
CUDA runtime version: 10.2.89
GPU models and configuration: 
GPU 0: GeForce RTX 2080 Ti
GPU 1: GeForce RTX 2080 Ti
GPU 2: GeForce RTX 2080 Ti
GPU 3: GeForce RTX 2080 Ti

Nvidia driver version: 440.82
cuDNN version: Could not collect

Versions of relevant libraries:
[pip3] numpy==1.19.0
[pip3] torch==1.5.1
[pip3] torchvision==0.6.1
[conda] torch                     1.5.1                     <pip>
[conda] torchvision               0.6.1                     <pip>
        Pillow (7.1.2)

Building train transforms ...
+ resize to 256x128
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
+ random erase
Building test transforms ...
+ resize to 256x128
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
=> Loading train (source) dataset
=> Loaded Market1501
  ----------------------------------------
  subset   | # ids | # images | # cameras
  ----------------------------------------
  train    |   751 |    12936 |         6
  query    |   750 |     3368 |         6
  gallery  |   751 |    15913 |         6
  ----------------------------------------
=> Loading test (target) dataset
=> Loaded Market1501
  ----------------------------------------
  subset   | # ids | # images | # cameras
  ----------------------------------------
  train    |   751 |    12936 |         6
  query    |   750 |     3368 |         6
  gallery  |   751 |    15913 |         6
  ----------------------------------------


  **************** Summary ****************
  train            : ['market1501']
  # train datasets : 1
  # train ids      : 751
  # train images   : 12936
  # train cameras  : 6
  test             : ['market1501']
  *****************************************


Building model: plr_osnet
Successfully loaded imagenet pretrained weights from "/home/s2019020843/.cache/torch/checkpoints/osnet_x1_0_imagenet.pth"
Model complexity: params=4,272,808 flops=1,959,041,312
Model structure: PLR_OSNet(
  (layer0): Sequential(
    (0): ConvLayer(
      (conv): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (1): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
  )
  (layer1): Sequential(
    (0): OSBlock(
      (conv1): Conv1x1(
        (conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (conv2a): LightConv3x3(
        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)
        (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (conv2b): Sequential(
        (0): LightConv3x3(
          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)
          (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (1): LightConv3x3(
          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)
          (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
      )
      (conv2c): Sequential(
        (0): LightConv3x3(
          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)
          (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (1): LightConv3x3(
          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)
          (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (2): LightConv3x3(
          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)
          (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
      )
      (conv2d): Sequential(
        (0): LightConv3x3(
          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)
          (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (1): LightConv3x3(
          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)
          (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (2): LightConv3x3(
          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)
          (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (3): LightConv3x3(
          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)
          (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
      )
      (gate): ChannelGate(
        (global_avgpool): AdaptiveAvgPool2d(output_size=1)
        (fc1): Conv2d(64, 4, kernel_size=(1, 1), stride=(1, 1))
        (relu): ReLU(inplace=True)
        (fc2): Conv2d(4, 64, kernel_size=(1, 1), stride=(1, 1))
        (gate_activation): Sigmoid()
      )
      (conv3): Conv1x1Linear(
        (conv): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (downsample): Conv1x1Linear(
        (conv): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): OSBlock(
      (conv1): Conv1x1(
        (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (conv2a): LightConv3x3(
        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)
        (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (conv2b): Sequential(
        (0): LightConv3x3(
          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)
          (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (1): LightConv3x3(
          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)
          (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
      )
      (conv2c): Sequential(
        (0): LightConv3x3(
          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)
          (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (1): LightConv3x3(
          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)
          (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (2): LightConv3x3(
          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)
          (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
      )
      (conv2d): Sequential(
        (0): LightConv3x3(
          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)
          (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (1): LightConv3x3(
          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)
          (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (2): LightConv3x3(
          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)
          (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (3): LightConv3x3(
          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)
          (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
      )
      (gate): ChannelGate(
        (global_avgpool): AdaptiveAvgPool2d(output_size=1)
        (fc1): Conv2d(64, 4, kernel_size=(1, 1), stride=(1, 1))
        (relu): ReLU(inplace=True)
        (fc2): Conv2d(4, 64, kernel_size=(1, 1), stride=(1, 1))
        (gate_activation): Sigmoid()
      )
      (conv3): Conv1x1Linear(
        (conv): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (2): Sequential(
      (0): Conv1x1(
        (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (1): AvgPool2d(kernel_size=2, stride=2, padding=0)
    )
  )
  (attention_module1): Attention_Module(
    (pam): PAM_Module(
      (query_conv): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1))
      (key_conv): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1))
      (softmax): Softmax(dim=-1)
      (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (se): SEModule(
      (fc1): Conv2d(256, 16, kernel_size=(1, 1), stride=(1, 1))
      (relu): ReLU(inplace=True)
      (fc2): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1))
      (sigmoid): Sigmoid()
    )
  )
  (layer2): Sequential(
    (0): OSBlock(
      (conv1): Conv1x1(
        (conv): Conv2d(256, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (conv2a): LightConv3x3(
        (conv1): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv2): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
        (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (conv2b): Sequential(
        (0): LightConv3x3(
          (conv1): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (conv2): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
          (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (1): LightConv3x3(
          (conv1): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (conv2): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
          (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
      )
      (conv2c): Sequential(
        (0): LightConv3x3(
          (conv1): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (conv2): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
          (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (1): LightConv3x3(
          (conv1): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (conv2): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
          (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (2): LightConv3x3(
          (conv1): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (conv2): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
          (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
      )
      (conv2d): Sequential(
        (0): LightConv3x3(
          (conv1): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (conv2): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
          (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (1): LightConv3x3(
          (conv1): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (conv2): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
          (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (2): LightConv3x3(
          (conv1): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (conv2): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
          (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (3): LightConv3x3(
          (conv1): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (conv2): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
          (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
      )
      (gate): ChannelGate(
        (global_avgpool): AdaptiveAvgPool2d(output_size=1)
        (fc1): Conv2d(96, 6, kernel_size=(1, 1), stride=(1, 1))
        (relu): ReLU(inplace=True)
        (fc2): Conv2d(6, 96, kernel_size=(1, 1), stride=(1, 1))
        (gate_activation): Sigmoid()
      )
      (conv3): Conv1x1Linear(
        (conv): Conv2d(96, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (downsample): Conv1x1Linear(
        (conv): Conv2d(256, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): OSBlock(
      (conv1): Conv1x1(
        (conv): Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (conv2a): LightConv3x3(
        (conv1): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv2): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
        (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (conv2b): Sequential(
        (0): LightConv3x3(
          (conv1): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (conv2): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
          (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (1): LightConv3x3(
          (conv1): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (conv2): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
          (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
      )
      (conv2c): Sequential(
        (0): LightConv3x3(
          (conv1): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (conv2): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
          (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (1): LightConv3x3(
          (conv1): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (conv2): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
          (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (2): LightConv3x3(
          (conv1): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (conv2): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
          (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
      )
      (conv2d): Sequential(
        (0): LightConv3x3(
          (conv1): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (conv2): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
          (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (1): LightConv3x3(
          (conv1): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (conv2): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
          (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (2): LightConv3x3(
          (conv1): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (conv2): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
          (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (3): LightConv3x3(
          (conv1): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (conv2): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
          (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
      )
      (gate): ChannelGate(
        (global_avgpool): AdaptiveAvgPool2d(output_size=1)
        (fc1): Conv2d(96, 6, kernel_size=(1, 1), stride=(1, 1))
        (relu): ReLU(inplace=True)
        (fc2): Conv2d(6, 96, kernel_size=(1, 1), stride=(1, 1))
        (gate_activation): Sigmoid()
      )
      (conv3): Conv1x1Linear(
        (conv): Conv2d(96, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (2): Sequential(
      (0): Conv1x1(
        (conv): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (1): AvgPool2d(kernel_size=2, stride=2, padding=0)
    )
  )
  (attention_module2): Attention_Module(
    (pam): PAM_Module(
      (query_conv): Conv2d(384, 48, kernel_size=(1, 1), stride=(1, 1))
      (key_conv): Conv2d(384, 48, kernel_size=(1, 1), stride=(1, 1))
      (softmax): Softmax(dim=-1)
      (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (se): SEModule(
      (fc1): Conv2d(384, 24, kernel_size=(1, 1), stride=(1, 1))
      (relu): ReLU(inplace=True)
      (fc2): Conv2d(24, 384, kernel_size=(1, 1), stride=(1, 1))
      (sigmoid): Sigmoid()
    )
  )
  (layer3): Sequential(
    (0): OSBlock(
      (conv1): Conv1x1(
        (conv): Conv2d(384, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (conv2a): LightConv3x3(
        (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)
        (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (conv2b): Sequential(
        (0): LightConv3x3(
          (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)
          (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (1): LightConv3x3(
          (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)
          (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
      )
      (conv2c): Sequential(
        (0): LightConv3x3(
          (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)
          (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (1): LightConv3x3(
          (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)
          (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (2): LightConv3x3(
          (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)
          (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
      )
      (conv2d): Sequential(
        (0): LightConv3x3(
          (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)
          (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (1): LightConv3x3(
          (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)
          (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (2): LightConv3x3(
          (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)
          (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (3): LightConv3x3(
          (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)
          (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
      )
      (gate): ChannelGate(
        (global_avgpool): AdaptiveAvgPool2d(output_size=1)
        (fc1): Conv2d(128, 8, kernel_size=(1, 1), stride=(1, 1))
        (relu): ReLU(inplace=True)
        (fc2): Conv2d(8, 128, kernel_size=(1, 1), stride=(1, 1))
        (gate_activation): Sigmoid()
      )
      (conv3): Conv1x1Linear(
        (conv): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (downsample): Conv1x1Linear(
        (conv): Conv2d(384, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): OSBlock(
      (conv1): Conv1x1(
        (conv): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (conv2a): LightConv3x3(
        (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)
        (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (conv2b): Sequential(
        (0): LightConv3x3(
          (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)
          (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (1): LightConv3x3(
          (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)
          (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
      )
      (conv2c): Sequential(
        (0): LightConv3x3(
          (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)
          (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (1): LightConv3x3(
          (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)
          (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (2): LightConv3x3(
          (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)
          (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
      )
      (conv2d): Sequential(
        (0): LightConv3x3(
          (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)
          (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (1): LightConv3x3(
          (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)
          (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (2): LightConv3x3(
          (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)
          (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (3): LightConv3x3(
          (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)
          (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
      )
      (gate): ChannelGate(
        (global_avgpool): AdaptiveAvgPool2d(output_size=1)
        (fc1): Conv2d(128, 8, kernel_size=(1, 1), stride=(1, 1))
        (relu): ReLU(inplace=True)
        (fc2): Conv2d(8, 128, kernel_size=(1, 1), stride=(1, 1))
        (gate_activation): Sigmoid()
      )
      (conv3): Conv1x1Linear(
        (conv): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (layer4): Conv1x1(
    (conv): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
  )
  (conv10): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (conv20): Conv2d(384, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (global_avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
  (global_maxpool): AdaptiveMaxPool2d(output_size=(1, 1))
  (fc1): Linear(in_features=512, out_features=512, bias=True)
  (bn1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn2): BatchNorm1d(6144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (classifier1): Linear(in_features=512, out_features=751, bias=True)
  (classifier2): Linear(in_features=6144, out_features=751, bias=True)
)
Building triplet-engine for image-reid
=> Start training
Epoch: [1/150][20/185]	Time 0.568 (1.014)	Data 0.000 (0.025)	Loss_t 0.6288 (0.7064)	Loss_x 6.7551 (6.7565)	Acc 0.00 (0.16)	Lr 0.000035	eta 7:48:40
Epoch: [1/150][40/185]	Time 0.659 (0.834)	Data 0.000 (0.013)	Loss_t 0.5536 (0.6489)	Loss_x 6.6798 (6.7331)	Acc 0.00 (0.08)	Lr 0.000035	eta 6:25:10
Epoch: [1/150][60/185]	Time 0.479 (0.727)	Data 0.000 (0.009)	Loss_t 0.5457 (0.6137)	Loss_x 6.6296 (6.7089)	Acc 0.00 (0.05)	Lr 0.000035	eta 5:35:36
Epoch: [1/150][80/185]	Time 0.712 (0.703)	Data 0.000 (0.006)	Loss_t 0.4919 (0.5908)	Loss_x 6.6299 (6.6936)	Acc 0.00 (0.08)	Lr 0.000035	eta 5:24:05
Epoch: [1/150][100/185]	Time 0.652 (0.691)	Data 0.000 (0.005)	Loss_t 0.4677 (0.5705)	Loss_x 6.5341 (6.6698)	Acc 0.00 (0.12)	Lr 0.000035	eta 5:18:35
Epoch: [1/150][120/185]	Time 0.710 (0.684)	Data 0.000 (0.004)	Loss_t 0.4509 (0.5536)	Loss_x 6.4860 (6.6479)	Acc 1.56 (0.12)	Lr 0.000035	eta 5:15:03
Epoch: [1/150][140/185]	Time 0.637 (0.678)	Data 0.000 (0.004)	Loss_t 0.4738 (0.5405)	Loss_x 6.5042 (6.6271)	Acc 0.00 (0.13)	Lr 0.000035	eta 5:11:58
Epoch: [1/150][160/185]	Time 0.648 (0.672)	Data 0.000 (0.003)	Loss_t 0.4364 (0.5289)	Loss_x 6.4149 (6.6002)	Acc 0.00 (0.16)	Lr 0.000035	eta 5:09:09
Epoch: [1/150][180/185]	Time 0.582 (0.668)	Data 0.000 (0.003)	Loss_t 0.4312 (0.5186)	Loss_x 6.2326 (6.5676)	Acc 0.00 (0.21)	Lr 0.000035	eta 5:06:47
Epoch: [2/150][20/185]	Time 0.659 (0.660)	Data 0.000 (0.020)	Loss_t 0.3704 (0.4077)	Loss_x 6.2362 (6.2746)	Acc 0.00 (0.16)	Lr 0.000043	eta 5:02:54
Epoch: [2/150][40/185]	Time 0.652 (0.660)	Data 0.000 (0.010)	Loss_t 0.3922 (0.4004)	Loss_x 6.2353 (6.2319)	Acc 0.00 (0.35)	Lr 0.000043	eta 5:02:46
Epoch: [2/150][60/185]	Time 0.628 (0.643)	Data 0.000 (0.007)	Loss_t 0.3745 (0.3977)	Loss_x 6.1030 (6.2176)	Acc 0.00 (0.39)	Lr 0.000043	eta 4:54:49
Epoch: [2/150][80/185]	Time 0.663 (0.646)	Data 0.000 (0.005)	Loss_t 0.3906 (0.3955)	Loss_x 6.0648 (6.1835)	Acc 1.56 (0.53)	Lr 0.000043	eta 4:55:44
Epoch: [2/150][100/185]	Time 0.604 (0.646)	Data 0.000 (0.004)	Loss_t 0.3196 (0.3921)	Loss_x 5.9777 (6.1535)	Acc 0.00 (0.61)	Lr 0.000043	eta 4:55:47
Epoch: [2/150][120/185]	Time 0.550 (0.640)	Data 0.000 (0.004)	Loss_t 0.3753 (0.3894)	Loss_x 6.0685 (6.1192)	Acc 1.56 (0.78)	Lr 0.000043	eta 4:52:43
Epoch: [2/150][140/185]	Time 0.607 (0.636)	Data 0.000 (0.003)	Loss_t 0.3786 (0.3870)	Loss_x 5.7299 (6.0795)	Acc 1.56 (0.97)	Lr 0.000043	eta 4:50:30
Epoch: [2/150][160/185]	Time 0.632 (0.631)	Data 0.000 (0.003)	Loss_t 0.3938 (0.3846)	Loss_x 5.6994 (6.0334)	Acc 3.12 (1.23)	Lr 0.000043	eta 4:48:19
Epoch: [2/150][180/185]	Time 0.618 (0.634)	Data 0.000 (0.002)	Loss_t 0.3138 (0.3804)	Loss_x 5.1083 (5.9637)	Acc 10.94 (1.77)	Lr 0.000043	eta 4:49:17
Epoch: [3/150][20/185]	Time 0.656 (0.699)	Data 0.000 (0.023)	Loss_t 0.3473 (0.3392)	Loss_x 5.7926 (5.6370)	Acc 1.56 (4.53)	Lr 0.000051	eta 5:18:42
Epoch: [3/150][40/185]	Time 0.638 (0.670)	Data 0.000 (0.011)	Loss_t 0.3487 (0.3379)	Loss_x 5.5111 (5.6220)	Acc 4.69 (3.67)	Lr 0.000051	eta 5:05:23
Epoch: [3/150][60/185]	Time 0.707 (0.660)	Data 0.000 (0.008)	Loss_t 0.3320 (0.3345)	Loss_x 5.8024 (5.6049)	Acc 1.56 (3.62)	Lr 0.000051	eta 5:00:37
Epoch: [3/150][80/185]	Time 0.665 (0.660)	Data 0.000 (0.006)	Loss_t 0.3672 (0.3348)	Loss_x 5.3216 (5.5801)	Acc 3.12 (3.81)	Lr 0.000051	eta 5:00:12
Epoch: [3/150][100/185]	Time 0.675 (0.657)	Data 0.000 (0.005)	Loss_t 0.3401 (0.3333)	Loss_x 5.1965 (5.5251)	Acc 4.69 (4.38)	Lr 0.000051	eta 4:58:46
Epoch: [3/150][120/185]	Time 0.647 (0.657)	Data 0.000 (0.004)	Loss_t 0.3054 (0.3279)	Loss_x 5.0661 (5.4725)	Acc 9.38 (4.79)	Lr 0.000051	eta 4:58:28
Epoch: [3/150][140/185]	Time 0.678 (0.658)	Data 0.000 (0.003)	Loss_t 0.3372 (0.3277)	Loss_x 5.2391 (5.4292)	Acc 4.69 (5.28)	Lr 0.000051	eta 4:58:40
Epoch: [3/150][160/185]	Time 0.666 (0.658)	Data 0.000 (0.003)	Loss_t 0.2875 (0.3248)	Loss_x 4.8074 (5.3632)	Acc 23.44 (6.48)	Lr 0.000051	eta 4:58:40
Epoch: [3/150][180/185]	Time 0.644 (0.658)	Data 0.000 (0.003)	Loss_t 0.3109 (0.3237)	Loss_x 4.5797 (5.2873)	Acc 37.50 (8.18)	Lr 0.000051	eta 4:58:10
Epoch: [4/150][20/185]	Time 0.688 (0.705)	Data 0.000 (0.023)	Loss_t 0.3513 (0.2823)	Loss_x 5.0021 (5.0506)	Acc 6.25 (6.25)	Lr 0.000059	eta 5:19:12
Epoch: [4/150][40/185]	Time 0.634 (0.672)	Data 0.000 (0.012)	Loss_t 0.3661 (0.2895)	Loss_x 5.3079 (5.0612)	Acc 6.25 (6.13)	Lr 0.000059	eta 5:04:04
Epoch: [4/150][60/185]	Time 0.642 (0.670)	Data 0.000 (0.008)	Loss_t 0.2834 (0.2873)	Loss_x 5.0632 (5.0082)	Acc 1.56 (7.11)	Lr 0.000059	eta 5:02:57
Epoch: [4/150][80/185]	Time 0.694 (0.667)	Data 0.000 (0.006)	Loss_t 0.2533 (0.2842)	Loss_x 4.6109 (4.9657)	Acc 12.50 (7.79)	Lr 0.000059	eta 5:01:26
Epoch: [4/150][100/185]	Time 0.595 (0.655)	Data 0.000 (0.005)	Loss_t 0.2864 (0.2818)	Loss_x 4.5174 (4.9029)	Acc 20.31 (8.34)	Lr 0.000059	eta 4:55:54
Epoch: [4/150][120/185]	Time 0.544 (0.652)	Data 0.000 (0.004)	Loss_t 0.2501 (0.2803)	Loss_x 4.4600 (4.8622)	Acc 14.06 (9.00)	Lr 0.000059	eta 4:54:02
Epoch: [4/150][140/185]	Time 0.628 (0.649)	Data 0.000 (0.003)	Loss_t 0.2617 (0.2802)	Loss_x 4.5415 (4.8083)	Acc 21.88 (10.51)	Lr 0.000059	eta 4:52:37
Epoch: [4/150][160/185]	Time 0.674 (0.646)	Data 0.000 (0.003)	Loss_t 0.2627 (0.2794)	Loss_x 4.2088 (4.7504)	Acc 20.31 (11.74)	Lr 0.000059	eta 4:51:13
Epoch: [4/150][180/185]	Time 0.651 (0.648)	Data 0.000 (0.003)	Loss_t 0.3875 (0.2802)	Loss_x 4.1206 (4.6709)	Acc 26.56 (14.03)	Lr 0.000059	eta 4:51:40
Epoch: [5/150][20/185]	Time 0.681 (0.697)	Data 0.000 (0.024)	Loss_t 0.2573 (0.2676)	Loss_x 4.4920 (4.5745)	Acc 9.38 (7.34)	Lr 0.000067	eta 5:13:45
Epoch: [5/150][40/185]	Time 0.710 (0.687)	Data 0.000 (0.012)	Loss_t 0.2762 (0.2544)	Loss_x 4.4067 (4.5172)	Acc 21.88 (8.52)	Lr 0.000067	eta 5:08:52
Epoch: [5/150][60/185]	Time 0.675 (0.686)	Data 0.000 (0.008)	Loss_t 0.2230 (0.2552)	Loss_x 4.3773 (4.4751)	Acc 12.50 (9.19)	Lr 0.000067	eta 5:08:00
Epoch: [5/150][80/185]	Time 0.544 (0.682)	Data 0.000 (0.006)	Loss_t 0.1547 (0.2466)	Loss_x 4.0659 (4.4189)	Acc 14.06 (10.29)	Lr 0.000067	eta 5:06:08
Epoch: [5/150][100/185]	Time 0.560 (0.668)	Data 0.000 (0.005)	Loss_t 0.3067 (0.2455)	Loss_x 4.5476 (4.3731)	Acc 4.69 (11.44)	Lr 0.000067	eta 4:59:45
Epoch: [5/150][120/185]	Time 0.503 (0.644)	Data 0.000 (0.004)	Loss_t 0.2547 (0.2440)	Loss_x 4.0715 (4.3310)	Acc 15.62 (12.51)	Lr 0.000067	eta 4:48:49
Epoch: [5/150][140/185]	Time 0.652 (0.634)	Data 0.000 (0.004)	Loss_t 0.2653 (0.2428)	Loss_x 4.0246 (4.2808)	Acc 15.62 (13.76)	Lr 0.000067	eta 4:43:52
Epoch: [5/150][160/185]	Time 0.502 (0.631)	Data 0.000 (0.003)	Loss_t 0.2135 (0.2437)	Loss_x 3.7077 (4.2292)	Acc 28.12 (15.49)	Lr 0.000067	eta 4:42:33
Epoch: [5/150][180/185]	Time 0.574 (0.630)	Data 0.000 (0.003)	Loss_t 0.3391 (0.2429)	Loss_x 3.5569 (4.1517)	Acc 32.81 (18.75)	Lr 0.000067	eta 4:41:36
Epoch: [6/150][20/185]	Time 0.676 (0.653)	Data 0.000 (0.020)	Loss_t 0.2037 (0.2308)	Loss_x 4.2089 (4.1476)	Acc 4.69 (10.31)	Lr 0.000075	eta 4:51:30
Epoch: [6/150][40/185]	Time 0.693 (0.647)	Data 0.000 (0.010)	Loss_t 0.1703 (0.2213)	Loss_x 3.7713 (4.0711)	Acc 12.50 (11.48)	Lr 0.000075	eta 4:48:37
Epoch: [6/150][60/185]	Time 0.650 (0.647)	Data 0.000 (0.007)	Loss_t 0.1982 (0.2184)	Loss_x 3.5142 (4.0146)	Acc 28.12 (12.29)	Lr 0.000075	eta 4:48:31
Epoch: [6/150][80/185]	Time 0.573 (0.645)	Data 0.000 (0.005)	Loss_t 0.1541 (0.2167)	Loss_x 3.9734 (3.9828)	Acc 0.00 (12.87)	Lr 0.000075	eta 4:47:31
Epoch: [6/150][100/185]	Time 0.558 (0.633)	Data 0.000 (0.004)	Loss_t 0.2268 (0.2188)	Loss_x 3.7052 (3.9340)	Acc 23.44 (14.38)	Lr 0.000075	eta 4:41:57
Epoch: [6/150][120/185]	Time 0.594 (0.629)	Data 0.000 (0.004)	Loss_t 0.2113 (0.2163)	Loss_x 3.8761 (3.8975)	Acc 21.88 (15.59)	Lr 0.000075	eta 4:40:07
Epoch: [6/150][140/185]	Time 0.665 (0.622)	Data 0.000 (0.003)	Loss_t 0.2271 (0.2151)	Loss_x 3.4813 (3.8428)	Acc 18.75 (17.33)	Lr 0.000075	eta 4:36:26
Epoch: [6/150][160/185]	Time 0.573 (0.619)	Data 0.000 (0.003)	Loss_t 0.1647 (0.2125)	Loss_x 3.3166 (3.7832)	Acc 40.62 (19.95)	Lr 0.000075	eta 4:34:59
Epoch: [6/150][180/185]	Time 0.689 (0.616)	Data 0.000 (0.002)	Loss_t 0.1801 (0.2126)	Loss_x 2.8873 (3.7164)	Acc 57.81 (23.36)	Lr 0.000075	eta 4:33:23
Epoch: [7/150][20/185]	Time 0.576 (0.602)	Data 0.000 (0.020)	Loss_t 0.1660 (0.1992)	Loss_x 3.5947 (3.7048)	Acc 21.88 (17.50)	Lr 0.000083	eta 4:27:14
Epoch: [7/150][40/185]	Time 0.590 (0.595)	Data 0.000 (0.010)	Loss_t 0.2360 (0.1944)	Loss_x 3.5249 (3.6543)	Acc 34.38 (16.72)	Lr 0.000083	eta 4:23:37
Epoch: [7/150][60/185]	Time 0.636 (0.608)	Data 0.000 (0.007)	Loss_t 0.1400 (0.1931)	Loss_x 3.6745 (3.6355)	Acc 4.69 (16.82)	Lr 0.000083	eta 4:29:33
Epoch: [7/150][80/185]	Time 0.628 (0.620)	Data 0.000 (0.005)	Loss_t 0.1653 (0.1894)	Loss_x 3.2927 (3.5962)	Acc 34.38 (17.83)	Lr 0.000083	eta 4:34:31
Epoch: [7/150][100/185]	Time 0.627 (0.622)	Data 0.000 (0.004)	Loss_t 0.2358 (0.1912)	Loss_x 3.4715 (3.5579)	Acc 26.56 (19.77)	Lr 0.000083	eta 4:35:02
Epoch: [7/150][120/185]	Time 0.639 (0.622)	Data 0.000 (0.003)	Loss_t 0.1102 (0.1888)	Loss_x 3.0838 (3.5133)	Acc 53.12 (21.97)	Lr 0.000083	eta 4:34:46
Epoch: [7/150][140/185]	Time 0.622 (0.621)	Data 0.000 (0.003)	Loss_t 0.2116 (0.1873)	Loss_x 3.0482 (3.4664)	Acc 34.38 (23.67)	Lr 0.000083	eta 4:34:18
Epoch: [7/150][160/185]	Time 0.600 (0.622)	Data 0.000 (0.003)	Loss_t 0.1093 (0.1845)	Loss_x 2.9375 (3.4089)	Acc 42.19 (26.77)	Lr 0.000083	eta 4:34:24
Epoch: [7/150][180/185]	Time 0.601 (0.621)	Data 0.000 (0.002)	Loss_t 0.1734 (0.1838)	Loss_x 2.5223 (3.3364)	Acc 73.44 (30.70)	Lr 0.000083	eta 4:33:57
Epoch: [8/150][20/185]	Time 0.575 (0.617)	Data 0.000 (0.022)	Loss_t 0.1680 (0.1657)	Loss_x 3.2615 (3.3708)	Acc 25.00 (20.70)	Lr 0.000092	eta 4:31:45
Epoch: [8/150][40/185]	Time 0.623 (0.619)	Data 0.000 (0.011)	Loss_t 0.2102 (0.1608)	Loss_x 3.5451 (3.3150)	Acc 4.69 (21.37)	Lr 0.000092	eta 4:32:33
Epoch: [8/150][60/185]	Time 0.683 (0.619)	Data 0.000 (0.007)	Loss_t 0.1202 (0.1603)	Loss_x 3.2973 (3.2694)	Acc 18.75 (24.30)	Lr 0.000092	eta 4:32:09
Epoch: [8/150][80/185]	Time 0.667 (0.626)	Data 0.000 (0.006)	Loss_t 0.1901 (0.1645)	Loss_x 3.3175 (3.2431)	Acc 21.88 (25.35)	Lr 0.000092	eta 4:35:09
Epoch: [8/150][100/185]	Time 0.649 (0.629)	Data 0.000 (0.005)	Loss_t 0.1314 (0.1643)	Loss_x 2.8343 (3.2162)	Acc 34.38 (26.88)	Lr 0.000092	eta 4:36:25
Epoch: [8/150][120/185]	Time 0.602 (0.631)	Data 0.000 (0.004)	Loss_t 0.1242 (0.1625)	Loss_x 2.7522 (3.1812)	Acc 45.31 (28.71)	Lr 0.000092	eta 4:36:49
Epoch: [8/150][140/185]	Time 0.522 (0.630)	Data 0.000 (0.003)	Loss_t 0.1808 (0.1620)	Loss_x 2.8682 (3.1283)	Acc 43.75 (31.56)	Lr 0.000092	eta 4:36:16
Epoch: [8/150][160/185]	Time 0.521 (0.627)	Data 0.000 (0.003)	Loss_t 0.1462 (0.1593)	Loss_x 2.6575 (3.0769)	Acc 54.69 (33.88)	Lr 0.000092	eta 4:34:46
Epoch: [8/150][180/185]	Time 0.658 (0.618)	Data 0.000 (0.003)	Loss_t 0.1383 (0.1567)	Loss_x 2.3614 (3.0059)	Acc 76.56 (37.35)	Lr 0.000092	eta 4:30:47
Epoch: [9/150][20/185]	Time 0.574 (0.630)	Data 0.000 (0.024)	Loss_t 0.1408 (0.1337)	Loss_x 2.6936 (2.9913)	Acc 46.88 (30.08)	Lr 0.000100	eta 4:35:32
Epoch: [9/150][40/185]	Time 0.631 (0.610)	Data 0.000 (0.012)	Loss_t 0.1500 (0.1400)	Loss_x 2.8803 (2.9974)	Acc 34.38 (29.69)	Lr 0.000100	eta 4:26:37
Epoch: [9/150][60/185]	Time 0.648 (0.609)	Data 0.000 (0.008)	Loss_t 0.0856 (0.1401)	Loss_x 3.1136 (2.9952)	Acc 23.44 (30.21)	Lr 0.000100	eta 4:25:53
Epoch: [9/150][80/185]	Time 0.523 (0.600)	Data 0.000 (0.006)	Loss_t 0.1968 (0.1413)	Loss_x 2.7542 (2.9660)	Acc 39.06 (32.40)	Lr 0.000100	eta 4:21:54
Epoch: [9/150][100/185]	Time 0.571 (0.599)	Data 0.000 (0.005)	Loss_t 0.1158 (0.1390)	Loss_x 2.8431 (2.9311)	Acc 46.88 (34.48)	Lr 0.000100	eta 4:21:09
Epoch: [9/150][120/185]	Time 0.610 (0.602)	Data 0.000 (0.004)	Loss_t 0.1661 (0.1388)	Loss_x 2.6599 (2.8953)	Acc 54.69 (36.29)	Lr 0.000100	eta 4:22:14
Epoch: [9/150][140/185]	Time 0.577 (0.603)	Data 0.000 (0.004)	Loss_t 0.1724 (0.1393)	Loss_x 2.3467 (2.8515)	Acc 73.44 (39.14)	Lr 0.000100	eta 4:22:48
Epoch: [9/150][160/185]	Time 0.671 (0.604)	Data 0.000 (0.003)	Loss_t 0.0989 (0.1374)	Loss_x 2.3858 (2.7984)	Acc 64.06 (42.29)	Lr 0.000100	eta 4:22:55
Epoch: [9/150][180/185]	Time 0.605 (0.605)	Data 0.000 (0.003)	Loss_t 0.1217 (0.1377)	Loss_x 1.8521 (2.7301)	Acc 92.19 (46.10)	Lr 0.000100	eta 4:22:57
Epoch: [10/150][20/185]	Time 0.640 (0.608)	Data 0.000 (0.018)	Loss_t 0.1250 (0.1294)	Loss_x 2.6655 (2.8761)	Acc 40.62 (29.30)	Lr 0.000108	eta 4:23:58
Epoch: [10/150][40/185]	Time 0.566 (0.605)	Data 0.000 (0.009)	Loss_t 0.1026 (0.1299)	Loss_x 2.5746 (2.8241)	Acc 54.69 (33.09)	Lr 0.000108	eta 4:22:39
Epoch: [10/150][60/185]	Time 0.582 (0.608)	Data 0.000 (0.006)	Loss_t 0.1277 (0.1284)	Loss_x 2.5249 (2.7746)	Acc 48.44 (36.02)	Lr 0.000108	eta 4:23:53
Epoch: [10/150][80/185]	Time 0.668 (0.616)	Data 0.000 (0.005)	Loss_t 0.1605 (0.1244)	Loss_x 2.4978 (2.7207)	Acc 54.69 (38.83)	Lr 0.000108	eta 4:26:49
Epoch: [10/150][100/185]	Time 0.688 (0.612)	Data 0.000 (0.004)	Loss_t 0.1466 (0.1224)	Loss_x 2.6800 (2.6725)	Acc 53.12 (41.70)	Lr 0.000108	eta 4:25:04
Epoch: [10/150][120/185]	Time 0.603 (0.614)	Data 0.000 (0.003)	Loss_t 0.1726 (0.1222)	Loss_x 2.4671 (2.6335)	Acc 56.25 (44.60)	Lr 0.000108	eta 4:25:41
