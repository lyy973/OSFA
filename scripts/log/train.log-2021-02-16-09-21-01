Show configuration
adam:
  beta1: 0.9
  beta2: 0.999
cuhk03:
  classic_split: False
  labeled_images: True
  use_metric_cuhk03: False
data:
  combineall: False
  height: 256
  norm_mean: [0.485, 0.456, 0.406]
  norm_std: [0.229, 0.224, 0.225]
  root: /home/s2019020843/changeeeee/data/
  save_dir: log
  sources: ['market1501']
  split_id: 0
  targets: ['market1501']
  transforms: ['random_flip', 'random_erase']
  type: image
  width: 128
  workers: 4
loss:
  name: triplet
  softmax:
    label_smooth: True
  triplet:
    margin: 0.1
    weight_t: 1.0
    weight_x: 1.0
market1501:
  use_500k_distractors: False
model:
  load_weights: 
  name: plr_osnet
  pretrained: True
  resume: 
rmsprop:
  alpha: 0.99
sampler:
  num_instances: 4
  train_sampler: RandomIdentitySampler
sgd:
  dampening: 0.0
  momentum: 0.9
  nesterov: False
test:
  batch_size: 300
  dist_metric: euclidean
  eval_freq: 10
  evaluate: False
  normalize_feature: False
  ranks: [1, 5, 10, 20]
  rerank: False
  start_eval: 0
  visactmap: False
  visrank: False
  visrank_topk: 10
train:
  base_lr_mult: 0.1
  batch_size: 64
  fixbase_epoch: 0
  gamma: 0.1
  lr: 3.5e-05
  lr_scheduler: warmup
  max_epoch: 150
  multiplier: 10
  new_layers: ['classifier']
  open_layers: ['classifier']
  optim: adam
  print_freq: 20
  seed: 1
  staged_lr: False
  start_epoch: 0
  stepsize: [60, 90]
  total_epoch: 39
  weight_decay: 0.0005
use_gpu: True
video:
  pooling_method: avg
  sample_method: evenly
  seq_len: 15

Collecting env info ...
** System info **
PyTorch version: 1.5.1
Is debug build: No
CUDA used to build PyTorch: 10.2

OS: CentOS Linux 7 (Core)
GCC version: (GCC) 4.8.5 20150623 (Red Hat 4.8.5-39)
CMake version: Could not collect

Python version: 3.7
Is CUDA available: Yes
CUDA runtime version: 10.2.89
GPU models and configuration: 
GPU 0: GeForce RTX 2080 Ti
GPU 1: GeForce RTX 2080 Ti
GPU 2: GeForce RTX 2080 Ti
GPU 3: GeForce RTX 2080 Ti

Nvidia driver version: 440.82
cuDNN version: Could not collect

Versions of relevant libraries:
[pip3] numpy==1.19.0
[pip3] torch==1.5.1
[pip3] torchvision==0.6.1
[conda] torch                     1.5.1                     <pip>
[conda] torchvision               0.6.1                     <pip>
        Pillow (7.1.2)

Building train transforms ...
+ resize to 256x128
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
+ random erase
Building test transforms ...
+ resize to 256x128
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
=> Loading train (source) dataset
=> Loaded Market1501
  ----------------------------------------
  subset   | # ids | # images | # cameras
  ----------------------------------------
  train    |   751 |    12936 |         6
  query    |   750 |     3368 |         6
  gallery  |   751 |    15913 |         6
  ----------------------------------------
=> Loading test (target) dataset
=> Loaded Market1501
  ----------------------------------------
  subset   | # ids | # images | # cameras
  ----------------------------------------
  train    |   751 |    12936 |         6
  query    |   750 |     3368 |         6
  gallery  |   751 |    15913 |         6
  ----------------------------------------


  **************** Summary ****************
  train            : ['market1501']
  # train datasets : 1
  # train ids      : 751
  # train images   : 12936
  # train cameras  : 6
  test             : ['market1501']
  *****************************************


Building model: plr_osnet
Successfully loaded imagenet pretrained weights from "/home/s2019020843/.cache/torch/checkpoints/osnet_x1_0_imagenet.pth"
Model complexity: params=3,409,416 flops=1,179,828,176
Model structure: PLR_OSNet(
  (layer0): Sequential(
    (0): ConvLayer(
      (conv): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (1): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
  )
  (layer1): Sequential(
    (0): OSBlock(
      (conv1): Conv1x1(
        (conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (conv2a): LightConv3x3(
        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)
        (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (conv2b): Sequential(
        (0): LightConv3x3(
          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)
          (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (1): LightConv3x3(
          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)
          (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
      )
      (conv2c): Sequential(
        (0): LightConv3x3(
          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)
          (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (1): LightConv3x3(
          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)
          (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (2): LightConv3x3(
          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)
          (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
      )
      (conv2d): Sequential(
        (0): LightConv3x3(
          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)
          (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (1): LightConv3x3(
          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)
          (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (2): LightConv3x3(
          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)
          (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (3): LightConv3x3(
          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)
          (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
      )
      (gate): ChannelGate(
        (global_avgpool): AdaptiveAvgPool2d(output_size=1)
        (fc1): Conv2d(64, 4, kernel_size=(1, 1), stride=(1, 1))
        (relu): ReLU(inplace=True)
        (fc2): Conv2d(4, 64, kernel_size=(1, 1), stride=(1, 1))
        (gate_activation): Sigmoid()
      )
      (conv3): Conv1x1Linear(
        (conv): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (downsample): Conv1x1Linear(
        (conv): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): OSBlock(
      (conv1): Conv1x1(
        (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (conv2a): LightConv3x3(
        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)
        (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (conv2b): Sequential(
        (0): LightConv3x3(
          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)
          (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (1): LightConv3x3(
          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)
          (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
      )
      (conv2c): Sequential(
        (0): LightConv3x3(
          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)
          (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (1): LightConv3x3(
          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)
          (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (2): LightConv3x3(
          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)
          (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
      )
      (conv2d): Sequential(
        (0): LightConv3x3(
          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)
          (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (1): LightConv3x3(
          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)
          (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (2): LightConv3x3(
          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)
          (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (3): LightConv3x3(
          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)
          (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
      )
      (gate): ChannelGate(
        (global_avgpool): AdaptiveAvgPool2d(output_size=1)
        (fc1): Conv2d(64, 4, kernel_size=(1, 1), stride=(1, 1))
        (relu): ReLU(inplace=True)
        (fc2): Conv2d(4, 64, kernel_size=(1, 1), stride=(1, 1))
        (gate_activation): Sigmoid()
      )
      (conv3): Conv1x1Linear(
        (conv): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (2): Sequential(
      (0): Conv1x1(
        (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (1): AvgPool2d(kernel_size=2, stride=2, padding=0)
    )
  )
  (layer2): Sequential(
    (0): OSBlock(
      (conv1): Conv1x1(
        (conv): Conv2d(256, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (conv2a): LightConv3x3(
        (conv1): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv2): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
        (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (conv2b): Sequential(
        (0): LightConv3x3(
          (conv1): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (conv2): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
          (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (1): LightConv3x3(
          (conv1): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (conv2): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
          (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
      )
      (conv2c): Sequential(
        (0): LightConv3x3(
          (conv1): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (conv2): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
          (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (1): LightConv3x3(
          (conv1): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (conv2): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
          (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (2): LightConv3x3(
          (conv1): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (conv2): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
          (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
      )
      (conv2d): Sequential(
        (0): LightConv3x3(
          (conv1): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (conv2): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
          (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (1): LightConv3x3(
          (conv1): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (conv2): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
          (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (2): LightConv3x3(
          (conv1): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (conv2): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
          (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (3): LightConv3x3(
          (conv1): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (conv2): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
          (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
      )
      (gate): ChannelGate(
        (global_avgpool): AdaptiveAvgPool2d(output_size=1)
        (fc1): Conv2d(96, 6, kernel_size=(1, 1), stride=(1, 1))
        (relu): ReLU(inplace=True)
        (fc2): Conv2d(6, 96, kernel_size=(1, 1), stride=(1, 1))
        (gate_activation): Sigmoid()
      )
      (conv3): Conv1x1Linear(
        (conv): Conv2d(96, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (downsample): Conv1x1Linear(
        (conv): Conv2d(256, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): OSBlock(
      (conv1): Conv1x1(
        (conv): Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (conv2a): LightConv3x3(
        (conv1): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv2): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
        (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (conv2b): Sequential(
        (0): LightConv3x3(
          (conv1): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (conv2): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
          (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (1): LightConv3x3(
          (conv1): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (conv2): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
          (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
      )
      (conv2c): Sequential(
        (0): LightConv3x3(
          (conv1): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (conv2): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
          (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (1): LightConv3x3(
          (conv1): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (conv2): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
          (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (2): LightConv3x3(
          (conv1): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (conv2): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
          (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
      )
      (conv2d): Sequential(
        (0): LightConv3x3(
          (conv1): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (conv2): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
          (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (1): LightConv3x3(
          (conv1): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (conv2): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
          (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (2): LightConv3x3(
          (conv1): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (conv2): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
          (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (3): LightConv3x3(
          (conv1): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (conv2): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
          (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
      )
      (gate): ChannelGate(
        (global_avgpool): AdaptiveAvgPool2d(output_size=1)
        (fc1): Conv2d(96, 6, kernel_size=(1, 1), stride=(1, 1))
        (relu): ReLU(inplace=True)
        (fc2): Conv2d(6, 96, kernel_size=(1, 1), stride=(1, 1))
        (gate_activation): Sigmoid()
      )
      (conv3): Conv1x1Linear(
        (conv): Conv2d(96, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (2): Sequential(
      (0): Conv1x1(
        (conv): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (1): AvgPool2d(kernel_size=2, stride=2, padding=0)
    )
  )
  (attention_module2): Attention_Module(
    (pam): PAM_Module(
      (query_conv): Conv2d(384, 48, kernel_size=(1, 1), stride=(1, 1))
      (key_conv): Conv2d(384, 48, kernel_size=(1, 1), stride=(1, 1))
      (softmax): Softmax(dim=-1)
      (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (se): SEModule(
      (fc1): Conv2d(384, 24, kernel_size=(1, 1), stride=(1, 1))
      (relu): ReLU(inplace=True)
      (fc2): Conv2d(24, 384, kernel_size=(1, 1), stride=(1, 1))
      (sigmoid): Sigmoid()
    )
  )
  (layer3): Sequential(
    (0): OSBlock(
      (conv1): Conv1x1(
        (conv): Conv2d(384, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (conv2a): LightConv3x3(
        (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)
        (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (conv2b): Sequential(
        (0): LightConv3x3(
          (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)
          (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (1): LightConv3x3(
          (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)
          (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
      )
      (conv2c): Sequential(
        (0): LightConv3x3(
          (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)
          (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (1): LightConv3x3(
          (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)
          (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (2): LightConv3x3(
          (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)
          (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
      )
      (conv2d): Sequential(
        (0): LightConv3x3(
          (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)
          (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (1): LightConv3x3(
          (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)
          (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (2): LightConv3x3(
          (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)
          (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (3): LightConv3x3(
          (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)
          (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
      )
      (gate): ChannelGate(
        (global_avgpool): AdaptiveAvgPool2d(output_size=1)
        (fc1): Conv2d(128, 8, kernel_size=(1, 1), stride=(1, 1))
        (relu): ReLU(inplace=True)
        (fc2): Conv2d(8, 128, kernel_size=(1, 1), stride=(1, 1))
        (gate_activation): Sigmoid()
      )
      (conv3): Conv1x1Linear(
        (conv): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (downsample): Conv1x1Linear(
        (conv): Conv2d(384, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): OSBlock(
      (conv1): Conv1x1(
        (conv): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (conv2a): LightConv3x3(
        (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)
        (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (conv2b): Sequential(
        (0): LightConv3x3(
          (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)
          (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (1): LightConv3x3(
          (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)
          (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
      )
      (conv2c): Sequential(
        (0): LightConv3x3(
          (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)
          (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (1): LightConv3x3(
          (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)
          (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (2): LightConv3x3(
          (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)
          (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
      )
      (conv2d): Sequential(
        (0): LightConv3x3(
          (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)
          (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (1): LightConv3x3(
          (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)
          (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (2): LightConv3x3(
          (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)
          (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (3): LightConv3x3(
          (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)
          (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
      )
      (gate): ChannelGate(
        (global_avgpool): AdaptiveAvgPool2d(output_size=1)
        (fc1): Conv2d(128, 8, kernel_size=(1, 1), stride=(1, 1))
        (relu): ReLU(inplace=True)
        (fc2): Conv2d(8, 128, kernel_size=(1, 1), stride=(1, 1))
        (gate_activation): Sigmoid()
      )
      (conv3): Conv1x1Linear(
        (conv): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (layer4): Conv1x1(
    (conv): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
  )
  (conv10): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (conv20): Conv2d(384, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (global_avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
  (global_maxpool): AdaptiveMaxPool2d(output_size=(1, 1))
  (fc1): Linear(in_features=512, out_features=512, bias=True)
  (bn1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn2): BatchNorm1d(6144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (classifier1): Linear(in_features=512, out_features=751, bias=True)
  (classifier2): Linear(in_features=6144, out_features=751, bias=True)
)
Building triplet-engine for image-reid
=> Start training
Epoch: [1/150][20/185]	Time 0.397 (0.509)	Data 0.000 (0.026)	Loss_t 0.6010 (0.6988)	Loss_x 6.6504 (6.7555)	Acc 0.00 (0.23)	Lr 0.000035	eta 3:55:18
Epoch: [1/150][40/185]	Time 0.405 (0.478)	Data 0.000 (0.013)	Loss_t 0.5456 (0.6394)	Loss_x 6.6975 (6.7334)	Acc 0.00 (0.20)	Lr 0.000035	eta 3:40:38
Epoch: [1/150][60/185]	Time 0.441 (0.465)	Data 0.000 (0.009)	Loss_t 0.5248 (0.6009)	Loss_x 6.6224 (6.7061)	Acc 0.00 (0.13)	Lr 0.000035	eta 3:34:43
Epoch: [1/150][80/185]	Time 0.414 (0.458)	Data 0.000 (0.007)	Loss_t 0.4866 (0.5743)	Loss_x 6.5304 (6.6870)	Acc 0.00 (0.14)	Lr 0.000035	eta 3:31:04
Epoch: [1/150][100/185]	Time 0.433 (0.452)	Data 0.000 (0.005)	Loss_t 0.4126 (0.5537)	Loss_x 6.5209 (6.6623)	Acc 0.00 (0.11)	Lr 0.000035	eta 3:28:21
Epoch: [1/150][120/185]	Time 0.421 (0.449)	Data 0.000 (0.004)	Loss_t 0.4359 (0.5364)	Loss_x 6.4793 (6.6384)	Acc 0.00 (0.17)	Lr 0.000035	eta 3:26:54
Epoch: [1/150][140/185]	Time 0.431 (0.448)	Data 0.000 (0.004)	Loss_t 0.4253 (0.5216)	Loss_x 6.4273 (6.6142)	Acc 1.56 (0.18)	Lr 0.000035	eta 3:26:08
Epoch: [1/150][160/185]	Time 0.432 (0.446)	Data 0.000 (0.003)	Loss_t 0.3968 (0.5100)	Loss_x 6.3085 (6.5854)	Acc 0.00 (0.19)	Lr 0.000035	eta 3:25:01
Epoch: [1/150][180/185]	Time 0.432 (0.445)	Data 0.000 (0.003)	Loss_t 0.4201 (0.4973)	Loss_x 6.1696 (6.5466)	Acc 0.00 (0.25)	Lr 0.000035	eta 3:24:27
Epoch: [2/150][20/185]	Time 0.456 (0.463)	Data 0.000 (0.025)	Loss_t 0.3441 (0.3741)	Loss_x 6.1021 (6.1839)	Acc 0.00 (1.02)	Lr 0.000043	eta 3:32:35
Epoch: [2/150][40/185]	Time 0.440 (0.450)	Data 0.000 (0.012)	Loss_t 0.3848 (0.3665)	Loss_x 6.1539 (6.1371)	Acc 1.56 (1.21)	Lr 0.000043	eta 3:26:34
Epoch: [2/150][60/185]	Time 0.448 (0.446)	Data 0.000 (0.008)	Loss_t 0.3307 (0.3624)	Loss_x 6.0622 (6.1287)	Acc 0.00 (1.09)	Lr 0.000043	eta 3:24:15
Epoch: [2/150][80/185]	Time 0.418 (0.442)	Data 0.000 (0.006)	Loss_t 0.3293 (0.3574)	Loss_x 5.8888 (6.0925)	Acc 4.69 (1.09)	Lr 0.000043	eta 3:22:34
Epoch: [2/150][100/185]	Time 0.430 (0.439)	Data 0.000 (0.005)	Loss_t 0.3017 (0.3554)	Loss_x 5.8658 (6.0587)	Acc 0.00 (1.31)	Lr 0.000043	eta 3:21:03
Epoch: [2/150][120/185]	Time 0.439 (0.436)	Data 0.000 (0.004)	Loss_t 0.3220 (0.3529)	Loss_x 5.9174 (6.0237)	Acc 3.12 (1.41)	Lr 0.000043	eta 3:19:17
Epoch: [2/150][140/185]	Time 0.399 (0.433)	Data 0.000 (0.004)	Loss_t 0.3431 (0.3508)	Loss_x 5.5780 (5.9798)	Acc 1.56 (1.64)	Lr 0.000043	eta 3:17:48
Epoch: [2/150][160/185]	Time 0.414 (0.433)	Data 0.000 (0.003)	Loss_t 0.3202 (0.3489)	Loss_x 5.4672 (5.9274)	Acc 3.12 (2.00)	Lr 0.000043	eta 3:17:50
Epoch: [2/150][180/185]	Time 0.393 (0.430)	Data 0.000 (0.003)	Loss_t 0.2886 (0.3436)	Loss_x 4.8306 (5.8472)	Acc 17.19 (2.86)	Lr 0.000043	eta 3:16:07
Epoch: [3/150][20/185]	Time 0.459 (0.458)	Data 0.000 (0.026)	Loss_t 0.3047 (0.2952)	Loss_x 5.6579 (5.4985)	Acc 4.69 (4.84)	Lr 0.000051	eta 3:28:57
Epoch: [3/150][40/185]	Time 0.456 (0.456)	Data 0.000 (0.013)	Loss_t 0.3340 (0.3014)	Loss_x 5.3760 (5.4780)	Acc 9.38 (4.61)	Lr 0.000051	eta 3:27:56
Epoch: [3/150][60/185]	Time 0.428 (0.451)	Data 0.000 (0.009)	Loss_t 0.3299 (0.2996)	Loss_x 5.7096 (5.4575)	Acc 1.56 (4.58)	Lr 0.000051	eta 3:25:10
Epoch: [3/150][80/185]	Time 0.511 (0.447)	Data 0.000 (0.007)	Loss_t 0.3373 (0.2994)	Loss_x 5.2659 (5.4251)	Acc 3.12 (4.61)	Lr 0.000051	eta 3:23:20
Epoch: [3/150][100/185]	Time 0.459 (0.445)	Data 0.000 (0.006)	Loss_t 0.3024 (0.2987)	Loss_x 5.0536 (5.3688)	Acc 9.38 (5.28)	Lr 0.000051	eta 3:22:14
Epoch: [3/150][120/185]	Time 0.437 (0.435)	Data 0.000 (0.005)	Loss_t 0.2976 (0.2921)	Loss_x 4.9516 (5.3116)	Acc 14.06 (5.94)	Lr 0.000051	eta 3:17:39
Epoch: [3/150][140/185]	Time 0.439 (0.427)	Data 0.000 (0.004)	Loss_t 0.3126 (0.2926)	Loss_x 5.0423 (5.2662)	Acc 10.94 (6.61)	Lr 0.000051	eta 3:13:45
Epoch: [3/150][160/185]	Time 0.519 (0.426)	Data 0.000 (0.004)	Loss_t 0.2749 (0.2899)	Loss_x 4.6603 (5.1959)	Acc 26.56 (8.04)	Lr 0.000051	eta 3:13:22
Epoch: [3/150][180/185]	Time 0.421 (0.425)	Data 0.000 (0.003)	Loss_t 0.2782 (0.2885)	Loss_x 4.3525 (5.1127)	Acc 45.31 (10.14)	Lr 0.000051	eta 3:12:44
Epoch: [4/150][20/185]	Time 0.366 (0.424)	Data 0.000 (0.026)	Loss_t 0.2500 (0.2417)	Loss_x 4.6934 (4.8463)	Acc 4.69 (7.58)	Lr 0.000059	eta 3:11:57
Epoch: [4/150][40/185]	Time 0.361 (0.393)	Data 0.000 (0.013)	Loss_t 0.3324 (0.2530)	Loss_x 5.1047 (4.8590)	Acc 7.81 (7.34)	Lr 0.000059	eta 2:57:54
Epoch: [4/150][60/185]	Time 0.370 (0.383)	Data 0.000 (0.009)	Loss_t 0.2832 (0.2526)	Loss_x 4.9237 (4.8063)	Acc 4.69 (7.92)	Lr 0.000059	eta 2:53:10
Epoch: [4/150][80/185]	Time 0.444 (0.390)	Data 0.000 (0.007)	Loss_t 0.1974 (0.2487)	Loss_x 4.3934 (4.7531)	Acc 9.38 (8.34)	Lr 0.000059	eta 2:56:18
Epoch: [4/150][100/185]	Time 0.430 (0.398)	Data 0.000 (0.005)	Loss_t 0.2108 (0.2479)	Loss_x 4.2710 (4.6945)	Acc 17.19 (9.30)	Lr 0.000059	eta 2:59:39
Epoch: [4/150][120/185]	Time 0.450 (0.404)	Data 0.000 (0.004)	Loss_t 0.2014 (0.2453)	Loss_x 4.2733 (4.6488)	Acc 17.19 (10.26)	Lr 0.000059	eta 3:02:23
Epoch: [4/150][140/185]	Time 0.425 (0.409)	Data 0.000 (0.004)	Loss_t 0.2328 (0.2463)	Loss_x 4.2297 (4.5920)	Acc 17.19 (11.62)	Lr 0.000059	eta 3:04:27
Epoch: [4/150][160/185]	Time 0.443 (0.414)	Data 0.000 (0.003)	Loss_t 0.2379 (0.2472)	Loss_x 3.9188 (4.5342)	Acc 23.44 (12.79)	Lr 0.000059	eta 3:06:26
Epoch: [4/150][180/185]	Time 0.411 (0.416)	Data 0.000 (0.003)	Loss_t 0.3852 (0.2473)	Loss_x 3.8886 (4.4522)	Acc 34.38 (15.13)	Lr 0.000059	eta 3:07:19
Epoch: [5/150][20/185]	Time 0.400 (0.442)	Data 0.000 (0.020)	Loss_t 0.2352 (0.2315)	Loss_x 4.2046 (4.3563)	Acc 3.12 (6.80)	Lr 0.000067	eta 3:18:57
Epoch: [5/150][40/185]	Time 0.486 (0.439)	Data 0.000 (0.010)	Loss_t 0.2504 (0.2176)	Loss_x 4.1974 (4.2828)	Acc 20.31 (8.63)	Lr 0.000067	eta 3:17:30
Epoch: [5/150][60/185]	Time 0.465 (0.438)	Data 0.000 (0.007)	Loss_t 0.1642 (0.2165)	Loss_x 4.0325 (4.2400)	Acc 18.75 (10.13)	Lr 0.000067	eta 3:16:31
Epoch: [5/150][80/185]	Time 0.486 (0.433)	Data 0.000 (0.005)	Loss_t 0.1231 (0.2101)	Loss_x 3.8313 (4.1845)	Acc 15.62 (11.23)	Lr 0.000067	eta 3:14:34
Epoch: [5/150][100/185]	Time 0.439 (0.433)	Data 0.000 (0.004)	Loss_t 0.2881 (0.2095)	Loss_x 4.1872 (4.1457)	Acc 10.94 (12.39)	Lr 0.000067	eta 3:14:23
Epoch: [5/150][120/185]	Time 0.430 (0.435)	Data 0.000 (0.003)	Loss_t 0.2314 (0.2088)	Loss_x 3.7879 (4.1051)	Acc 18.75 (13.55)	Lr 0.000067	eta 3:15:04
Epoch: [5/150][140/185]	Time 0.396 (0.435)	Data 0.000 (0.003)	Loss_t 0.2708 (0.2077)	Loss_x 3.8028 (4.0513)	Acc 18.75 (15.06)	Lr 0.000067	eta 3:14:53
Epoch: [5/150][160/185]	Time 0.415 (0.434)	Data 0.000 (0.003)	Loss_t 0.2017 (0.2072)	Loss_x 3.4942 (3.9937)	Acc 34.38 (17.08)	Lr 0.000067	eta 3:14:13
Epoch: [5/150][180/185]	Time 0.435 (0.436)	Data 0.000 (0.002)	Loss_t 0.2450 (0.2063)	Loss_x 3.1887 (3.9166)	Acc 39.06 (20.23)	Lr 0.000067	eta 3:14:54
Epoch: [6/150][20/185]	Time 0.441 (0.470)	Data 0.000 (0.022)	Loss_t 0.1484 (0.1957)	Loss_x 3.9823 (3.8924)	Acc 6.25 (10.55)	Lr 0.000075	eta 3:29:49
Epoch: [6/150][40/185]	Time 0.510 (0.468)	Data 0.000 (0.011)	Loss_t 0.1776 (0.1872)	Loss_x 3.6336 (3.8204)	Acc 20.31 (12.81)	Lr 0.000075	eta 3:28:58
Epoch: [6/150][60/185]	Time 0.540 (0.458)	Data 0.000 (0.007)	Loss_t 0.1938 (0.1849)	Loss_x 3.3534 (3.7730)	Acc 28.12 (13.70)	Lr 0.000075	eta 3:24:10
Epoch: [6/150][80/185]	Time 0.463 (0.449)	Data 0.000 (0.006)	Loss_t 0.1116 (0.1837)	Loss_x 3.7055 (3.7450)	Acc 4.69 (14.22)	Lr 0.000075	eta 3:20:14
Epoch: [6/150][100/185]	Time 0.444 (0.449)	Data 0.000 (0.004)	Loss_t 0.1565 (0.1852)	Loss_x 3.4429 (3.6978)	Acc 25.00 (15.70)	Lr 0.000075	eta 3:19:59
Epoch: [6/150][120/185]	Time 0.411 (0.450)	Data 0.000 (0.004)	Loss_t 0.1741 (0.1826)	Loss_x 3.5958 (3.6614)	Acc 28.12 (17.29)	Lr 0.000075	eta 3:20:11
Epoch: [6/150][140/185]	Time 0.421 (0.450)	Data 0.000 (0.003)	Loss_t 0.1385 (0.1798)	Loss_x 3.1840 (3.6090)	Acc 21.88 (19.31)	Lr 0.000075	eta 3:19:56
Epoch: [6/150][160/185]	Time 0.409 (0.450)	Data 0.000 (0.003)	Loss_t 0.1360 (0.1763)	Loss_x 2.9928 (3.5453)	Acc 46.88 (22.18)	Lr 0.000075	eta 3:19:46
Epoch: [6/150][180/185]	Time 0.436 (0.445)	Data 0.000 (0.003)	Loss_t 0.1943 (0.1765)	Loss_x 2.7047 (3.4794)	Acc 68.75 (25.87)	Lr 0.000075	eta 3:17:31
Epoch: [7/150][20/185]	Time 0.399 (0.437)	Data 0.000 (0.021)	Loss_t 0.1301 (0.1721)	Loss_x 3.3306 (3.4684)	Acc 25.00 (19.30)	Lr 0.000083	eta 3:13:54
Epoch: [7/150][40/185]	Time 0.377 (0.425)	Data 0.000 (0.011)	Loss_t 0.1848 (0.1657)	Loss_x 3.1818 (3.4307)	Acc 42.19 (18.20)	Lr 0.000083	eta 3:08:32
Epoch: [7/150][60/185]	Time 0.381 (0.409)	Data 0.000 (0.007)	Loss_t 0.1531 (0.1610)	Loss_x 3.4769 (3.4141)	Acc 6.25 (18.33)	Lr 0.000083	eta 3:01:10
Epoch: [7/150][80/185]	Time 0.375 (0.414)	Data 0.000 (0.005)	Loss_t 0.1159 (0.1595)	Loss_x 3.1036 (3.3710)	Acc 29.69 (19.73)	Lr 0.000083	eta 3:03:17
Epoch: [7/150][100/185]	Time 0.444 (0.415)	Data 0.000 (0.004)	Loss_t 0.2267 (0.1590)	Loss_x 3.3090 (3.3317)	Acc 35.94 (22.14)	Lr 0.000083	eta 3:03:24
Epoch: [7/150][120/185]	Time 0.416 (0.411)	Data 0.000 (0.004)	Loss_t 0.1038 (0.1579)	Loss_x 2.7734 (3.2922)	Acc 54.69 (24.49)	Lr 0.000083	eta 3:01:45
Epoch: [7/150][140/185]	Time 0.444 (0.413)	Data 0.000 (0.003)	Loss_t 0.1456 (0.1559)	Loss_x 2.7525 (3.2450)	Acc 48.44 (26.74)	Lr 0.000083	eta 3:02:21
Epoch: [7/150][160/185]	Time 0.434 (0.415)	Data 0.000 (0.003)	Loss_t 0.0861 (0.1542)	Loss_x 2.7235 (3.1856)	Acc 50.00 (29.81)	Lr 0.000083	eta 3:03:05
Epoch: [7/150][180/185]	Time 0.398 (0.416)	Data 0.000 (0.002)	Loss_t 0.1190 (0.1534)	Loss_x 2.3230 (3.1131)	Acc 78.12 (33.95)	Lr 0.000083	eta 3:03:28
Epoch: [8/150][20/185]	Time 0.451 (0.455)	Data 0.000 (0.024)	Loss_t 0.1235 (0.1352)	Loss_x 3.1070 (3.1470)	Acc 15.62 (20.16)	Lr 0.000092	eta 3:20:30
Epoch: [8/150][40/185]	Time 0.427 (0.450)	Data 0.000 (0.012)	Loss_t 0.2077 (0.1314)	Loss_x 3.2905 (3.1053)	Acc 4.69 (22.30)	Lr 0.000092	eta 3:18:01
Epoch: [8/150][60/185]	Time 0.408 (0.440)	Data 0.000 (0.008)	Loss_t 0.1061 (0.1341)	Loss_x 3.1163 (3.0672)	Acc 17.19 (25.96)	Lr 0.000092	eta 3:13:32
Epoch: [8/150][80/185]	Time 0.377 (0.427)	Data 0.000 (0.006)	Loss_t 0.1609 (0.1363)	Loss_x 3.1688 (3.0363)	Acc 21.88 (27.68)	Lr 0.000092	eta 3:07:31
Epoch: [8/150][100/185]	Time 0.448 (0.425)	Data 0.000 (0.005)	Loss_t 0.0899 (0.1360)	Loss_x 2.7167 (3.0047)	Acc 31.25 (29.89)	Lr 0.000092	eta 3:06:41
Epoch: [8/150][120/185]	Time 0.444 (0.427)	Data 0.000 (0.004)	Loss_t 0.1073 (0.1361)	Loss_x 2.6316 (2.9742)	Acc 54.69 (31.64)	Lr 0.000092	eta 3:07:19
Epoch: [8/150][140/185]	Time 0.438 (0.425)	Data 0.000 (0.004)	Loss_t 0.1410 (0.1353)	Loss_x 2.6874 (2.9216)	Acc 51.56 (35.04)	Lr 0.000092	eta 3:06:28
Epoch: [8/150][160/185]	Time 0.454 (0.426)	Data 0.000 (0.003)	Loss_t 0.1218 (0.1323)	Loss_x 2.4489 (2.8701)	Acc 65.62 (37.90)	Lr 0.000092	eta 3:06:30
Epoch: [8/150][180/185]	Time 0.454 (0.427)	Data 0.000 (0.003)	Loss_t 0.1435 (0.1302)	Loss_x 2.2364 (2.7990)	Acc 79.69 (41.89)	Lr 0.000092	eta 3:06:50
Epoch: [9/150][20/185]	Time 0.413 (0.418)	Data 0.000 (0.022)	Loss_t 0.1025 (0.1133)	Loss_x 2.5374 (2.7993)	Acc 51.56 (32.97)	Lr 0.000100	eta 3:03:02
Epoch: [9/150][40/185]	Time 0.462 (0.418)	Data 0.000 (0.011)	Loss_t 0.1490 (0.1170)	Loss_x 2.8594 (2.8113)	Acc 29.69 (33.95)	Lr 0.000100	eta 3:02:47
Epoch: [9/150][60/185]	Time 0.406 (0.424)	Data 0.000 (0.007)	Loss_t 0.0470 (0.1158)	Loss_x 2.8718 (2.8021)	Acc 17.19 (34.43)	Lr 0.000100	eta 3:05:13
Epoch: [9/150][80/185]	Time 0.362 (0.412)	Data 0.000 (0.006)	Loss_t 0.1354 (0.1157)	Loss_x 2.5965 (2.7703)	Acc 48.44 (36.66)	Lr 0.000100	eta 2:59:54
Epoch: [9/150][100/185]	Time 0.367 (0.403)	Data 0.000 (0.004)	Loss_t 0.0920 (0.1155)	Loss_x 2.6349 (2.7329)	Acc 50.00 (39.22)	Lr 0.000100	eta 2:55:40
Epoch: [9/150][120/185]	Time 0.427 (0.403)	Data 0.000 (0.004)	Loss_t 0.1062 (0.1160)	Loss_x 2.4365 (2.6988)	Acc 64.06 (41.41)	Lr 0.000100	eta 2:55:35
Epoch: [9/150][140/185]	Time 0.449 (0.406)	Data 0.000 (0.003)	Loss_t 0.1798 (0.1168)	Loss_x 2.2504 (2.6536)	Acc 76.56 (44.38)	Lr 0.000100	eta 2:56:54
Epoch: [9/150][160/185]	Time 0.416 (0.408)	Data 0.000 (0.003)	Loss_t 0.0664 (0.1144)	Loss_x 2.1590 (2.6023)	Acc 73.44 (47.68)	Lr 0.000100	eta 2:57:42
Epoch: [9/150][180/185]	Time 0.445 (0.409)	Data 0.000 (0.003)	Loss_t 0.0646 (0.1151)	Loss_x 1.7478 (2.5376)	Acc 98.44 (51.39)	Lr 0.000100	eta 2:57:55
Epoch: [10/150][20/185]	Time 0.445 (0.463)	Data 0.000 (0.022)	Loss_t 0.0708 (0.1025)	Loss_x 2.3473 (2.6790)	Acc 59.38 (33.44)	Lr 0.000108	eta 3:21:02
Epoch: [10/150][40/185]	Time 0.451 (0.442)	Data 0.000 (0.011)	Loss_t 0.0779 (0.1019)	Loss_x 2.2650 (2.6290)	Acc 65.62 (37.62)	Lr 0.000108	eta 3:11:57
Epoch: [10/150][60/185]	Time 0.449 (0.438)	Data 0.000 (0.007)	Loss_t 0.0732 (0.1016)	Loss_x 2.2830 (2.5819)	Acc 50.00 (40.89)	Lr 0.000108	eta 3:10:09
Epoch: [10/150][80/185]	Time 0.429 (0.428)	Data 0.000 (0.006)	Loss_t 0.0832 (0.0992)	Loss_x 2.2511 (2.5298)	Acc 65.62 (44.43)	Lr 0.000108	eta 3:05:26
Epoch: [10/150][100/185]	Time 0.444 (0.426)	Data 0.000 (0.005)	Loss_t 0.1250 (0.0992)	Loss_x 2.4667 (2.4877)	Acc 51.56 (47.12)	Lr 0.000108	eta 3:04:37
Epoch: [10/150][120/185]	Time 0.404 (0.421)	Data 0.000 (0.004)	Loss_t 0.0840 (0.0977)	Loss_x 2.1247 (2.4495)	Acc 64.06 (49.93)	Lr 0.000108	eta 3:02:08
Epoch: [10/150][140/185]	Time 0.381 (0.419)	Data 0.000 (0.003)	Loss_t 0.0840 (0.0988)	Loss_x 2.0603 (2.4067)	Acc 71.88 (52.87)	Lr 0.000108	eta 3:01:19
Epoch: [10/150][160/185]	Time 0.442 (0.421)	Data 0.000 (0.003)	Loss_t 0.0790 (0.0988)	Loss_x 1.8647 (2.3575)	Acc 90.62 (55.72)	Lr 0.000108	eta 3:02:00
Epoch: [10/150][180/185]	Time 0.411 (0.422)	Data 0.000 (0.003)	Loss_t 0.1107 (0.0978)	Loss_x 1.8501 (2.2998)	Acc 78.12 (59.03)	Lr 0.000108	eta 3:02:02
##### Evaluating market1501 (source) #####
Extracting features from query set ...
Done, obtained 3368-by-6656 matrix
Extracting features from gallery set ...
Done, obtained 15913-by-6656 matrix
Speed: 0.0714 sec/batch
Computing distance matrix with metric=euclidean ...
Computing CMC and mAP ...
** Results **
mAP: 54.7%
CMC curve
Rank-1  : 74.4%
Rank-5  : 89.2%
Rank-10 : 93.4%
Rank-20 : 96.4%
Checkpoint saved to "log/model.pth.tar-10"
Epoch: [11/150][20/185]	Time 0.352 (0.453)	Data 0.000 (0.028)	Loss_t 0.0471 (0.0783)	Loss_x 2.3683 (2.4463)	Acc 51.56 (42.50)	Lr 0.000116	eta 3:15:35
Epoch: [11/150][40/185]	Time 0.366 (0.407)	Data 0.000 (0.014)	Loss_t 0.1201 (0.0845)	Loss_x 2.1161 (2.3852)	Acc 65.62 (46.84)	Lr 0.000116	eta 2:55:23
Epoch: [11/150][60/185]	Time 0.374 (0.403)	Data 0.000 (0.009)	Loss_t 0.1055 (0.0866)	Loss_x 2.2403 (2.3576)	Acc 62.50 (49.64)	Lr 0.000116	eta 2:53:24
Epoch: [11/150][80/185]	Time 0.410 (0.401)	Data 0.000 (0.007)	Loss_t 0.1000 (0.0903)	Loss_x 2.0514 (2.3324)	Acc 73.44 (52.44)	Lr 0.000116	eta 2:52:26
Epoch: [11/150][100/185]	Time 0.379 (0.401)	Data 0.000 (0.006)	Loss_t 0.1650 (0.0887)	Loss_x 2.3802 (2.2925)	Acc 57.81 (55.48)	Lr 0.000116	eta 2:52:15
Epoch: [11/150][120/185]	Time 0.362 (0.400)	Data 0.000 (0.005)	Loss_t 0.0444 (0.0877)	Loss_x 1.6911 (2.2430)	Acc 93.75 (58.67)	Lr 0.000116	eta 2:51:58
Epoch: [11/150][140/185]	Time 0.457 (0.402)	Data 0.000 (0.004)	Loss_t 0.1337 (0.0869)	Loss_x 1.8012 (2.1939)	Acc 84.38 (61.70)	Lr 0.000116	eta 2:52:41
Epoch: [11/150][160/185]	Time 0.369 (0.399)	Data 0.000 (0.004)	Loss_t 0.1006 (0.0855)	Loss_x 1.7453 (2.1481)	Acc 82.81 (64.07)	Lr 0.000116	eta 2:51:18
Epoch: [11/150][180/185]	Time 0.400 (0.399)	Data 0.000 (0.003)	Loss_t 0.1027 (0.0855)	Loss_x 1.5367 (2.0962)	Acc 98.44 (66.85)	Lr 0.000116	eta 2:51:04
Epoch: [12/150][20/185]	Time 0.397 (0.436)	Data 0.000 (0.023)	Loss_t 0.1038 (0.0814)	Loss_x 2.4883 (2.3149)	Acc 42.19 (46.48)	Lr 0.000124	eta 3:06:55
Epoch: [12/150][40/185]	Time 0.407 (0.418)	Data 0.000 (0.011)	Loss_t 0.0427 (0.0842)	Loss_x 2.1108 (2.2504)	Acc 60.94 (52.58)	Lr 0.000124	eta 2:58:46
Epoch: [12/150][60/185]	Time 0.374 (0.412)	Data 0.000 (0.008)	Loss_t 0.0581 (0.0833)	Loss_x 1.9534 (2.1998)	Acc 68.75 (56.80)	Lr 0.000124	eta 2:56:02
Epoch: [12/150][80/185]	Time 0.418 (0.411)	Data 0.000 (0.006)	Loss_t 0.0878 (0.0823)	Loss_x 1.8618 (2.1496)	Acc 79.69 (60.23)	Lr 0.000124	eta 2:55:37
Epoch: [12/150][100/185]	Time 0.448 (0.416)	Data 0.000 (0.005)	Loss_t 0.0367 (0.0797)	Loss_x 1.6358 (2.0960)	Acc 95.31 (64.02)	Lr 0.000124	eta 2:57:28
Epoch: [12/150][120/185]	Time 0.446 (0.419)	Data 0.000 (0.004)	Loss_t 0.0517 (0.0802)	Loss_x 1.8010 (2.0572)	Acc 81.25 (66.65)	Lr 0.000124	eta 2:58:37
Epoch: [12/150][140/185]	Time 0.391 (0.421)	Data 0.000 (0.003)	Loss_t 0.0462 (0.0793)	Loss_x 1.6615 (2.0197)	Acc 82.81 (69.04)	Lr 0.000124	eta 2:59:31
Epoch: [12/150][160/185]	Time 0.409 (0.423)	Data 0.000 (0.003)	Loss_t 0.0537 (0.0779)	Loss_x 1.5310 (1.9730)	Acc 95.31 (71.39)	Lr 0.000124	eta 3:00:00
Epoch: [12/150][180/185]	Time 0.406 (0.424)	Data 0.000 (0.003)	Loss_t 0.0325 (0.0775)	Loss_x 1.4112 (1.9256)	Acc 90.62 (73.71)	Lr 0.000124	eta 3:00:18
Epoch: [13/150][20/185]	Time 0.433 (0.454)	Data 0.000 (0.023)	Loss_t 0.0617 (0.0616)	Loss_x 2.1552 (2.0567)	Acc 43.75 (59.06)	Lr 0.000132	eta 3:12:59
Epoch: [13/150][40/185]	Time 0.476 (0.444)	Data 0.000 (0.011)	Loss_t 0.0862 (0.0674)	Loss_x 1.9881 (2.0564)	Acc 62.50 (61.95)	Lr 0.000132	eta 3:08:34
Epoch: [13/150][60/185]	Time 0.380 (0.438)	Data 0.000 (0.008)	Loss_t 0.1179 (0.0686)	Loss_x 1.7425 (1.9940)	Acc 79.69 (65.89)	Lr 0.000132	eta 3:05:50
Epoch: [13/150][80/185]	Time 0.437 (0.430)	Data 0.000 (0.006)	Loss_t 0.0512 (0.0672)	Loss_x 2.0596 (1.9676)	Acc 68.75 (68.01)	Lr 0.000132	eta 3:02:35
Epoch: [13/150][100/185]	Time 0.427 (0.424)	Data 0.000 (0.005)	Loss_t 0.0747 (0.0703)	Loss_x 1.7833 (1.9423)	Acc 81.25 (70.02)	Lr 0.000132	eta 2:59:35
Epoch: [13/150][120/185]	Time 0.426 (0.421)	Data 0.000 (0.004)	Loss_t 0.1213 (0.0713)	Loss_x 1.7709 (1.9146)	Acc 81.25 (71.71)	Lr 0.000132	eta 2:58:06
Epoch: [13/150][140/185]	Time 0.400 (0.419)	Data 0.000 (0.003)	Loss_t 0.0918 (0.0714)	Loss_x 1.5692 (1.8702)	Acc 95.31 (74.15)	Lr 0.000132	eta 2:57:30
Epoch: [13/150][160/185]	Time 0.429 (0.421)	Data 0.000 (0.003)	Loss_t 0.0477 (0.0698)	Loss_x 1.4544 (1.8269)	Acc 95.31 (76.38)	Lr 0.000132	eta 2:57:55
Epoch: [13/150][180/185]	Time 0.409 (0.419)	Data 0.000 (0.003)	Loss_t 0.0676 (0.0693)	Loss_x 1.3001 (1.7800)	Acc 98.44 (78.50)	Lr 0.000132	eta 2:56:54
Epoch: [14/150][20/185]	Time 0.408 (0.432)	Data 0.000 (0.023)	Loss_t 0.0807 (0.0704)	Loss_x 2.0227 (1.9897)	Acc 68.75 (63.52)	Lr 0.000140	eta 3:02:10
Epoch: [14/150][40/185]	Time 0.379 (0.412)	Data 0.000 (0.012)	Loss_t 0.1054 (0.0649)	Loss_x 1.9563 (1.9191)	Acc 68.75 (67.81)	Lr 0.000140	eta 2:53:52
Epoch: [14/150][60/185]	Time 0.430 (0.414)	Data 0.000 (0.008)	Loss_t 0.0604 (0.0641)	Loss_x 1.7011 (1.8696)	Acc 82.81 (71.04)	Lr 0.000140	eta 2:54:37
Epoch: [14/150][80/185]	Time 0.378 (0.418)	Data 0.000 (0.006)	Loss_t 0.0605 (0.0637)	Loss_x 1.7086 (1.8392)	Acc 78.12 (72.95)	Lr 0.000140	eta 2:55:53
Epoch: [14/150][100/185]	Time 0.440 (0.421)	Data 0.000 (0.005)	Loss_t 0.0771 (0.0655)	Loss_x 1.5280 (1.7990)	Acc 93.75 (75.70)	Lr 0.000140	eta 2:57:16
Epoch: [14/150][120/185]	Time 0.445 (0.422)	Data 0.000 (0.004)	Loss_t 0.0499 (0.0631)	Loss_x 1.6094 (1.7607)	Acc 92.19 (77.81)	Lr 0.000140	eta 2:57:33
Epoch: [14/150][140/185]	Time 0.451 (0.423)	Data 0.000 (0.003)	Loss_t 0.0325 (0.0634)	Loss_x 1.3871 (1.7268)	Acc 96.88 (79.64)	Lr 0.000140	eta 2:57:50
Epoch: [14/150][160/185]	Time 0.388 (0.423)	Data 0.000 (0.003)	Loss_t 0.0689 (0.0633)	Loss_x 1.3680 (1.6939)	Acc 98.44 (81.34)	Lr 0.000140	eta 2:57:35
Epoch: [14/150][180/185]	Time 0.417 (0.424)	Data 0.000 (0.003)	Loss_t 0.0123 (0.0618)	Loss_x 1.2115 (1.6564)	Acc 96.88 (83.00)	Lr 0.000140	eta 2:57:42
Epoch: [15/150][20/185]	Time 0.456 (0.449)	Data 0.000 (0.022)	Loss_t 0.0810 (0.0710)	Loss_x 1.9983 (1.8445)	Acc 57.81 (71.41)	Lr 0.000148	eta 3:08:13
Epoch: [15/150][40/185]	Time 0.417 (0.444)	Data 0.000 (0.011)	Loss_t 0.1008 (0.0674)	Loss_x 1.8882 (1.7880)	Acc 79.69 (74.88)	Lr 0.000148	eta 3:05:59
Epoch: [15/150][60/185]	Time 0.413 (0.432)	Data 0.000 (0.007)	Loss_t 0.0227 (0.0659)	Loss_x 1.5990 (1.7419)	Acc 89.06 (77.92)	Lr 0.000148	eta 3:00:48
Epoch: [15/150][80/185]	Time 0.358 (0.422)	Data 0.000 (0.006)	Loss_t 0.0776 (0.0636)	Loss_x 1.6479 (1.7008)	Acc 82.81 (79.88)	Lr 0.000148	eta 2:56:12
Epoch: [15/150][100/185]	Time 0.354 (0.417)	Data 0.000 (0.004)	Loss_t 0.0632 (0.0619)	Loss_x 1.7808 (1.6742)	Acc 76.56 (81.27)	Lr 0.000148	eta 2:54:05
Epoch: [15/150][120/185]	Time 0.434 (0.415)	Data 0.000 (0.004)	Loss_t 0.0639 (0.0598)	Loss_x 1.5489 (1.6462)	Acc 84.38 (82.68)	Lr 0.000148	eta 2:53:14
Epoch: [15/150][140/185]	Time 0.430 (0.413)	Data 0.000 (0.003)	Loss_t 0.0105 (0.0592)	Loss_x 1.3193 (1.6220)	Acc 95.31 (83.77)	Lr 0.000148	eta 2:52:09
Epoch: [15/150][160/185]	Time 0.445 (0.415)	Data 0.000 (0.003)	Loss_t 0.0301 (0.0583)	Loss_x 1.4008 (1.5918)	Acc 92.19 (85.36)	Lr 0.000148	eta 2:52:58
Epoch: [15/150][180/185]	Time 0.443 (0.416)	Data 0.000 (0.003)	Loss_t 0.0355 (0.0566)	Loss_x 1.2701 (1.5604)	Acc 98.44 (86.65)	Lr 0.000148	eta 2:53:09
Epoch: [16/150][20/185]	Time 0.448 (0.446)	Data 0.000 (0.025)	Loss_t 0.0587 (0.0506)	Loss_x 1.6903 (1.6687)	Acc 84.38 (77.81)	Lr 0.000156	eta 3:05:26
Epoch: [16/150][40/185]	Time 0.393 (0.435)	Data 0.000 (0.012)	Loss_t 0.0532 (0.0523)	Loss_x 1.5608 (1.6217)	Acc 89.06 (81.88)	Lr 0.000156	eta 3:00:46
Epoch: [16/150][60/185]	Time 0.428 (0.424)	Data 0.000 (0.008)	Loss_t 0.0546 (0.0516)	Loss_x 1.4689 (1.5918)	Acc 98.44 (83.67)	Lr 0.000156	eta 2:55:54
Epoch: [16/150][80/185]	Time 0.402 (0.418)	Data 0.000 (0.006)	Loss_t 0.0584 (0.0534)	Loss_x 1.5149 (1.5747)	Acc 89.06 (84.73)	Lr 0.000156	eta 2:53:24
Epoch: [16/150][100/185]	Time 0.359 (0.413)	Data 0.000 (0.005)	Loss_t 0.1061 (0.0532)	Loss_x 1.6315 (1.5649)	Acc 84.38 (85.30)	Lr 0.000156	eta 2:51:15
Epoch: [16/150][120/185]	Time 0.384 (0.411)	Data 0.000 (0.004)	Loss_t 0.1052 (0.0519)	Loss_x 1.5174 (1.5443)	Acc 92.19 (86.42)	Lr 0.000156	eta 2:50:22
Epoch: [16/150][140/185]	Time 0.445 (0.413)	Data 0.000 (0.004)	Loss_t 0.0292 (0.0526)	Loss_x 1.2561 (1.5226)	Acc 96.88 (87.65)	Lr 0.000156	eta 2:50:47
Epoch: [16/150][160/185]	Time 0.369 (0.414)	Data 0.000 (0.003)	Loss_t 0.0765 (0.0523)	Loss_x 1.3824 (1.5031)	Acc 95.31 (88.54)	Lr 0.000156	eta 2:51:03
Epoch: [16/150][180/185]	Time 0.381 (0.412)	Data 0.000 (0.003)	Loss_t 0.0285 (0.0510)	Loss_x 1.2022 (1.4762)	Acc 98.44 (89.54)	Lr 0.000156	eta 2:50:25
Epoch: [17/150][20/185]	Time 0.392 (0.427)	Data 0.000 (0.023)	Loss_t 0.0447 (0.0522)	Loss_x 1.4779 (1.5486)	Acc 93.75 (85.16)	Lr 0.000164	eta 2:56:04
Epoch: [17/150][40/185]	Time 0.451 (0.429)	Data 0.000 (0.011)	Loss_t 0.0297 (0.0508)	Loss_x 1.5151 (1.5492)	Acc 89.06 (84.84)	Lr 0.000164	eta 2:56:45
Epoch: [17/150][60/185]	Time 0.434 (0.423)	Data 0.000 (0.008)	Loss_t 0.0249 (0.0514)	Loss_x 1.4498 (1.5289)	Acc 85.94 (85.57)	Lr 0.000164	eta 2:54:16
Epoch: [17/150][80/185]	Time 0.420 (0.424)	Data 0.000 (0.006)	Loss_t 0.0607 (0.0497)	Loss_x 1.3753 (1.5134)	Acc 96.88 (86.95)	Lr 0.000164	eta 2:54:41
Epoch: [17/150][100/185]	Time 0.448 (0.424)	Data 0.000 (0.005)	Loss_t 0.0409 (0.0497)	Loss_x 1.2884 (1.4933)	Acc 98.44 (88.14)	Lr 0.000164	eta 2:54:39
Epoch: [17/150][120/185]	Time 0.444 (0.427)	Data 0.000 (0.004)	Loss_t 0.0283 (0.0486)	Loss_x 1.4271 (1.4728)	Acc 87.50 (89.21)	Lr 0.000164	eta 2:55:27
Epoch: [17/150][140/185]	Time 0.428 (0.426)	Data 0.000 (0.003)	Loss_t 0.0529 (0.0484)	Loss_x 1.2959 (1.4567)	Acc 96.88 (89.94)	Lr 0.000164	eta 2:54:51
Epoch: [17/150][160/185]	Time 0.442 (0.425)	Data 0.000 (0.003)	Loss_t 0.0447 (0.0476)	Loss_x 1.3157 (1.4365)	Acc 96.88 (90.80)	Lr 0.000164	eta 2:54:23
Epoch: [17/150][180/185]	Time 0.374 (0.420)	Data 0.000 (0.003)	Loss_t 0.0461 (0.0477)	Loss_x 1.2153 (1.4169)	Acc 98.44 (91.59)	Lr 0.000164	eta 2:52:23
Epoch: [18/150][20/185]	Time 0.453 (0.411)	Data 0.000 (0.021)	Loss_t 0.0386 (0.0489)	Loss_x 1.4797 (1.4727)	Acc 89.06 (87.58)	Lr 0.000172	eta 2:48:23
Epoch: [18/150][40/185]	Time 0.370 (0.394)	Data 0.000 (0.010)	Loss_t 0.0328 (0.0436)	Loss_x 1.3085 (1.4781)	Acc 93.75 (87.73)	Lr 0.000172	eta 2:41:12
Epoch: [18/150][60/185]	Time 0.453 (0.403)	Data 0.000 (0.007)	Loss_t 0.0264 (0.0446)	Loss_x 1.3629 (1.4548)	Acc 98.44 (88.93)	Lr 0.000172	eta 2:44:53
Epoch: [18/150][80/185]	Time 0.490 (0.420)	Data 0.000 (0.005)	Loss_t 0.0326 (0.0449)	Loss_x 1.3262 (1.4372)	Acc 95.31 (89.90)	Lr 0.000172	eta 2:51:38
Epoch: [18/150][100/185]	Time 0.447 (0.426)	Data 0.000 (0.004)	Loss_t 0.0287 (0.0437)	Loss_x 1.2484 (1.4166)	Acc 98.44 (91.03)	Lr 0.000172	eta 2:53:58
Epoch: [18/150][120/185]	Time 0.464 (0.428)	Data 0.000 (0.004)	Loss_t 0.0262 (0.0424)	Loss_x 1.3478 (1.4024)	Acc 95.31 (91.59)	Lr 0.000172	eta 2:54:36
Epoch: [18/150][140/185]	Time 0.526 (0.428)	Data 0.000 (0.003)	Loss_t 0.0962 (0.0444)	Loss_x 1.3319 (1.3904)	Acc 96.88 (92.10)	Lr 0.000172	eta 2:54:22
Epoch: [18/150][160/185]	Time 0.408 (0.426)	Data 0.000 (0.003)	Loss_t 0.0332 (0.0454)	Loss_x 1.2659 (1.3794)	Acc 98.44 (92.69)	Lr 0.000172	eta 2:53:34
Epoch: [18/150][180/185]	Time 0.453 (0.424)	Data 0.000 (0.002)	Loss_t 0.0201 (0.0443)	Loss_x 1.1804 (1.3627)	Acc 100.00 (93.31)	Lr 0.000172	eta 2:52:45
Epoch: [19/150][20/185]	Time 0.456 (0.455)	Data 0.000 (0.021)	Loss_t 0.0798 (0.0434)	Loss_x 1.7239 (1.4542)	Acc 81.25 (89.69)	Lr 0.000180	eta 3:05:02
Epoch: [19/150][40/185]	Time 0.454 (0.458)	Data 0.000 (0.011)	Loss_t 0.0332 (0.0403)	Loss_x 1.2592 (1.4150)	Acc 98.44 (91.33)	Lr 0.000180	eta 3:06:01
Epoch: [19/150][60/185]	Time 0.412 (0.453)	Data 0.000 (0.007)	Loss_t 0.0285 (0.0419)	Loss_x 1.2378 (1.4063)	Acc 98.44 (92.08)	Lr 0.000180	eta 3:03:47
Epoch: [19/150][80/185]	Time 0.491 (0.454)	Data 0.000 (0.005)	Loss_t 0.0064 (0.0412)	Loss_x 1.3443 (1.3866)	Acc 95.31 (93.16)	Lr 0.000180	eta 3:04:07
Epoch: [19/150][100/185]	Time 0.422 (0.449)	Data 0.000 (0.004)	Loss_t 0.0741 (0.0416)	Loss_x 1.3905 (1.3748)	Acc 96.88 (93.55)	Lr 0.000180	eta 3:02:06
Epoch: [19/150][120/185]	Time 0.414 (0.452)	Data 0.000 (0.004)	Loss_t 0.0440 (0.0426)	Loss_x 1.1961 (1.3676)	Acc 100.00 (93.82)	Lr 0.000180	eta 3:03:05
Epoch: [19/150][140/185]	Time 0.438 (0.454)	Data 0.000 (0.003)	Loss_t 0.0251 (0.0423)	Loss_x 1.2622 (1.3555)	Acc 98.44 (94.17)	Lr 0.000180	eta 3:03:33
Epoch: [19/150][160/185]	Time 0.449 (0.451)	Data 0.000 (0.003)	Loss_t 0.0519 (0.0425)	Loss_x 1.2201 (1.3425)	Acc 98.44 (94.59)	Lr 0.000180	eta 3:02:23
Epoch: [19/150][180/185]	Time 0.474 (0.450)	Data 0.000 (0.003)	Loss_t 0.0436 (0.0423)	Loss_x 1.1965 (1.3293)	Acc 96.88 (95.05)	Lr 0.000180	eta 3:01:49
Epoch: [20/150][20/185]	Time 0.445 (0.447)	Data 0.000 (0.022)	Loss_t 0.0324 (0.0446)	Loss_x 1.4262 (1.4035)	Acc 92.19 (92.50)	Lr 0.000188	eta 3:00:35
Epoch: [20/150][40/185]	Time 0.411 (0.434)	Data 0.000 (0.011)	Loss_t 0.0363 (0.0451)	Loss_x 1.2933 (1.3885)	Acc 98.44 (93.52)	Lr 0.000188	eta 2:55:04
Epoch: [20/150][60/185]	Time 0.447 (0.434)	Data 0.000 (0.007)	Loss_t 0.0315 (0.0413)	Loss_x 1.3484 (1.3749)	Acc 95.31 (93.85)	Lr 0.000188	eta 2:54:45
Epoch: [20/150][80/185]	Time 0.444 (0.433)	Data 0.000 (0.006)	Loss_t 0.0353 (0.0404)	Loss_x 1.3145 (1.3614)	Acc 96.88 (94.08)	Lr 0.000188	eta 2:54:10
Epoch: [20/150][100/185]	Time 0.383 (0.428)	Data 0.000 (0.004)	Loss_t 0.0347 (0.0405)	Loss_x 1.2838 (1.3527)	Acc 98.44 (94.08)	Lr 0.000188	eta 2:52:09
Epoch: [20/150][120/185]	Time 0.457 (0.426)	Data 0.000 (0.004)	Loss_t 0.0275 (0.0398)	Loss_x 1.2001 (1.3407)	Acc 98.44 (94.47)	Lr 0.000188	eta 2:51:05
Epoch: [20/150][140/185]	Time 0.378 (0.427)	Data 0.000 (0.003)	Loss_t 0.0070 (0.0402)	Loss_x 1.2611 (1.3317)	Acc 98.44 (94.81)	Lr 0.000188	eta 2:51:34
Epoch: [20/150][160/185]	Time 0.422 (0.428)	Data 0.000 (0.003)	Loss_t 0.0465 (0.0394)	Loss_x 1.1909 (1.3198)	Acc 100.00 (95.25)	Lr 0.000188	eta 2:51:34
Epoch: [20/150][180/185]	Time 0.445 (0.429)	Data 0.000 (0.003)	Loss_t 0.0272 (0.0391)	Loss_x 1.1600 (1.3066)	Acc 98.44 (95.56)	Lr 0.000188	eta 2:52:08
##### Evaluating market1501 (source) #####
Extracting features from query set ...
Done, obtained 3368-by-6656 matrix
Extracting features from gallery set ...
Done, obtained 15913-by-6656 matrix
Speed: 0.0357 sec/batch
Computing distance matrix with metric=euclidean ...
Computing CMC and mAP ...
** Results **
mAP: 67.0%
CMC curve
Rank-1  : 83.4%
Rank-5  : 94.4%
Rank-10 : 96.7%
Rank-20 : 98.3%
Checkpoint saved to "log/model.pth.tar-20"
Epoch: [21/150][20/185]	Time 0.457 (0.468)	Data 0.000 (0.027)	Loss_t 0.0310 (0.0355)	Loss_x 1.3001 (1.3638)	Acc 93.75 (94.14)	Lr 0.000197	eta 3:07:21
Epoch: [21/150][40/185]	Time 0.388 (0.446)	Data 0.000 (0.014)	Loss_t 0.0266 (0.0344)	Loss_x 1.3334 (1.3456)	Acc 95.31 (95.12)	Lr 0.000197	eta 2:58:38
Epoch: [21/150][60/185]	Time 0.416 (0.438)	Data 0.000 (0.009)	Loss_t 0.0488 (0.0351)	Loss_x 1.3140 (1.3325)	Acc 98.44 (95.47)	Lr 0.000197	eta 2:55:10
Epoch: [21/150][80/185]	Time 0.416 (0.437)	Data 0.000 (0.007)	Loss_t 0.0579 (0.0363)	Loss_x 1.3779 (1.3234)	Acc 90.62 (95.80)	Lr 0.000197	eta 2:54:41
Epoch: [21/150][100/185]	Time 0.434 (0.438)	Data 0.000 (0.006)	Loss_t 0.0231 (0.0364)	Loss_x 1.2590 (1.3089)	Acc 100.00 (96.12)	Lr 0.000197	eta 2:55:00
Epoch: [21/150][120/185]	Time 0.434 (0.441)	Data 0.000 (0.005)	Loss_t 0.0224 (0.0364)	Loss_x 1.2322 (1.3017)	Acc 100.00 (96.35)	Lr 0.000197	eta 2:55:59
Epoch: [21/150][140/185]	Time 0.380 (0.438)	Data 0.000 (0.004)	Loss_t 0.0483 (0.0367)	Loss_x 1.2194 (1.2953)	Acc 96.88 (96.57)	Lr 0.000197	eta 2:54:30
Epoch: [21/150][160/185]	Time 0.403 (0.438)	Data 0.000 (0.004)	Loss_t 0.0253 (0.0359)	Loss_x 1.2438 (1.2859)	Acc 98.44 (96.82)	Lr 0.000197	eta 2:54:30
Epoch: [21/150][180/185]	Time 0.410 (0.436)	Data 0.000 (0.003)	Loss_t 0.0059 (0.0364)	Loss_x 1.1671 (1.2775)	Acc 98.44 (96.93)	Lr 0.000197	eta 2:53:34
Epoch: [22/150][20/185]	Time 0.444 (0.453)	Data 0.000 (0.022)	Loss_t 0.0576 (0.0417)	Loss_x 1.3590 (1.3337)	Acc 95.31 (95.39)	Lr 0.000205	eta 3:00:00
Epoch: [22/150][40/185]	Time 0.392 (0.421)	Data 0.000 (0.011)	Loss_t 0.0306 (0.0379)	Loss_x 1.2905 (1.3132)	Acc 96.88 (95.94)	Lr 0.000205	eta 2:47:09
Epoch: [22/150][60/185]	Time 0.433 (0.427)	Data 0.000 (0.007)	Loss_t 0.0455 (0.0365)	Loss_x 1.3268 (1.3070)	Acc 98.44 (96.38)	Lr 0.000205	eta 2:49:21
Epoch: [22/150][80/185]	Time 0.407 (0.426)	Data 0.000 (0.006)	Loss_t 0.0625 (0.0373)	Loss_x 1.3657 (1.3010)	Acc 95.31 (96.56)	Lr 0.000205	eta 2:48:45
Epoch: [22/150][100/185]	Time 0.430 (0.426)	Data 0.000 (0.004)	Loss_t 0.0131 (0.0378)	Loss_x 1.1852 (1.2933)	Acc 100.00 (96.77)	Lr 0.000205	eta 2:48:38
Epoch: [22/150][120/185]	Time 0.420 (0.429)	Data 0.000 (0.004)	Loss_t 0.0173 (0.0373)	Loss_x 1.1936 (1.2865)	Acc 100.00 (96.84)	Lr 0.000205	eta 2:49:36
Epoch: [22/150][140/185]	Time 0.413 (0.429)	Data 0.000 (0.003)	Loss_t 0.0476 (0.0363)	Loss_x 1.2292 (1.2793)	Acc 100.00 (97.01)	Lr 0.000205	eta 2:49:42
Epoch: [22/150][160/185]	Time 0.427 (0.431)	Data 0.000 (0.003)	Loss_t 0.0341 (0.0356)	Loss_x 1.2187 (1.2715)	Acc 96.88 (97.19)	Lr 0.000205	eta 2:50:18
Epoch: [22/150][180/185]	Time 0.384 (0.427)	Data 0.000 (0.003)	Loss_t 0.0421 (0.0356)	Loss_x 1.1609 (1.2641)	Acc 100.00 (97.31)	Lr 0.000205	eta 2:48:34
Epoch: [23/150][20/185]	Time 0.392 (0.426)	Data 0.000 (0.021)	Loss_t 0.0104 (0.0313)	Loss_x 1.1927 (1.2780)	Acc 98.44 (97.42)	Lr 0.000213	eta 2:48:05
Epoch: [23/150][40/185]	Time 0.377 (0.426)	Data 0.000 (0.010)	Loss_t 0.0096 (0.0309)	Loss_x 1.2228 (1.2860)	Acc 100.00 (96.99)	Lr 0.000213	eta 2:47:44
Epoch: [23/150][60/185]	Time 0.496 (0.428)	Data 0.000 (0.007)	Loss_t 0.0173 (0.0309)	Loss_x 1.2321 (1.2760)	Acc 98.44 (97.32)	Lr 0.000213	eta 2:48:22
Epoch: [23/150][80/185]	Time 0.512 (0.427)	Data 0.000 (0.005)	Loss_t 0.0574 (0.0319)	Loss_x 1.2602 (1.2781)	Acc 96.88 (97.23)	Lr 0.000213	eta 2:47:48
Epoch: [23/150][100/185]	Time 0.478 (0.429)	Data 0.000 (0.004)	Loss_t 0.0115 (0.0325)	Loss_x 1.1808 (1.2745)	Acc 98.44 (97.33)	Lr 0.000213	eta 2:48:39
Epoch: [23/150][120/185]	Time 0.444 (0.431)	Data 0.000 (0.004)	Loss_t 0.0270 (0.0323)	Loss_x 1.1713 (1.2690)	Acc 100.00 (97.46)	Lr 0.000213	eta 2:49:19
Epoch: [23/150][140/185]	Time 0.440 (0.431)	Data 0.000 (0.003)	Loss_t 0.0293 (0.0329)	Loss_x 1.2679 (1.2633)	Acc 96.88 (97.52)	Lr 0.000213	eta 2:49:09
Epoch: [23/150][160/185]	Time 0.449 (0.431)	Data 0.000 (0.003)	Loss_t 0.0178 (0.0335)	Loss_x 1.1828 (1.2573)	Acc 96.88 (97.62)	Lr 0.000213	eta 2:48:52
Epoch: [23/150][180/185]	Time 0.429 (0.430)	Data 0.000 (0.002)	Loss_t 0.0206 (0.0333)	Loss_x 1.1422 (1.2498)	Acc 100.00 (97.74)	Lr 0.000213	eta 2:48:19
Epoch: [24/150][20/185]	Time 0.404 (0.423)	Data 0.000 (0.023)	Loss_t 0.0203 (0.0385)	Loss_x 1.2540 (1.2665)	Acc 98.44 (97.66)	Lr 0.000221	eta 2:45:40
Epoch: [24/150][40/185]	Time 0.449 (0.404)	Data 0.000 (0.012)	Loss_t 0.0103 (0.0329)	Loss_x 1.2369 (1.2641)	Acc 100.00 (97.46)	Lr 0.000221	eta 2:37:55
Epoch: [24/150][60/185]	Time 0.373 (0.410)	Data 0.000 (0.008)	Loss_t 0.0473 (0.0325)	Loss_x 1.2039 (1.2675)	Acc 96.88 (97.42)	Lr 0.000221	eta 2:40:10
Epoch: [24/150][80/185]	Time 0.396 (0.410)	Data 0.000 (0.006)	Loss_t 0.0293 (0.0307)	Loss_x 1.2118 (1.2635)	Acc 100.00 (97.34)	Lr 0.000221	eta 2:40:06
Epoch: [24/150][100/185]	Time 0.372 (0.408)	Data 0.000 (0.005)	Loss_t 0.0125 (0.0301)	Loss_x 1.1716 (1.2588)	Acc 100.00 (97.47)	Lr 0.000221	eta 2:38:58
Epoch: [24/150][120/185]	Time 0.435 (0.409)	Data 0.000 (0.004)	Loss_t 0.0386 (0.0298)	Loss_x 1.2512 (1.2554)	Acc 96.88 (97.51)	Lr 0.000221	eta 2:39:30
Epoch: [24/150][140/185]	Time 0.433 (0.412)	Data 0.000 (0.003)	Loss_t 0.0628 (0.0302)	Loss_x 1.2846 (1.2514)	Acc 96.88 (97.63)	Lr 0.000221	eta 2:40:12
Epoch: [24/150][160/185]	Time 0.435 (0.413)	Data 0.000 (0.003)	Loss_t 0.0410 (0.0306)	Loss_x 1.2562 (1.2460)	Acc 96.88 (97.74)	Lr 0.000221	eta 2:40:36
Epoch: [24/150][180/185]	Time 0.388 (0.413)	Data 0.000 (0.003)	Loss_t 0.0362 (0.0307)	Loss_x 1.1778 (1.2392)	Acc 96.88 (97.80)	Lr 0.000221	eta 2:40:34
Epoch: [25/150][20/185]	Time 0.458 (0.433)	Data 0.000 (0.023)	Loss_t 0.0173 (0.0299)	Loss_x 1.2993 (1.2639)	Acc 96.88 (97.34)	Lr 0.000229	eta 2:48:04
Epoch: [25/150][40/185]	Time 0.417 (0.418)	Data 0.000 (0.011)	Loss_t 0.0231 (0.0313)	Loss_x 1.2267 (1.2657)	Acc 100.00 (97.30)	Lr 0.000229	eta 2:42:04
Epoch: [25/150][60/185]	Time 0.399 (0.411)	Data 0.000 (0.008)	Loss_t 0.0600 (0.0294)	Loss_x 1.3057 (1.2599)	Acc 95.31 (97.45)	Lr 0.000229	eta 2:39:22
Epoch: [25/150][80/185]	Time 0.433 (0.418)	Data 0.000 (0.006)	Loss_t 0.0395 (0.0299)	Loss_x 1.1702 (1.2545)	Acc 100.00 (97.60)	Lr 0.000229	eta 2:41:45
Epoch: [25/150][100/185]	Time 0.419 (0.417)	Data 0.000 (0.005)	Loss_t 0.0218 (0.0305)	Loss_x 1.2460 (1.2538)	Acc 96.88 (97.55)	Lr 0.000229	eta 2:41:26
Epoch: [25/150][120/185]	Time 0.398 (0.417)	Data 0.000 (0.004)	Loss_t 0.0439 (0.0294)	Loss_x 1.2315 (1.2488)	Acc 98.44 (97.60)	Lr 0.000229	eta 2:41:16
Epoch: [25/150][140/185]	Time 0.447 (0.419)	Data 0.000 (0.003)	Loss_t 0.0270 (0.0292)	Loss_x 1.2025 (1.2453)	Acc 100.00 (97.68)	Lr 0.000229	eta 2:41:54
Epoch: [25/150][160/185]	Time 0.391 (0.419)	Data 0.000 (0.003)	Loss_t 0.0172 (0.0286)	Loss_x 1.1806 (1.2393)	Acc 96.88 (97.79)	Lr 0.000229	eta 2:41:51
Epoch: [25/150][180/185]	Time 0.447 (0.420)	Data 0.000 (0.003)	Loss_t 0.0385 (0.0287)	Loss_x 1.1674 (1.2322)	Acc 98.44 (97.95)	Lr 0.000229	eta 2:42:03
Epoch: [26/150][20/185]	Time 0.388 (0.456)	Data 0.000 (0.022)	Loss_t 0.0287 (0.0219)	Loss_x 1.1998 (1.2550)	Acc 98.44 (98.59)	Lr 0.000237	eta 2:55:41
Epoch: [26/150][40/185]	Time 0.420 (0.442)	Data 0.000 (0.011)	Loss_t 0.0024 (0.0229)	Loss_x 1.1666 (1.2462)	Acc 100.00 (98.44)	Lr 0.000237	eta 2:49:59
Epoch: [26/150][60/185]	Time 0.386 (0.436)	Data 0.000 (0.007)	Loss_t 0.0580 (0.0248)	Loss_x 1.2279 (1.2406)	Acc 98.44 (98.39)	Lr 0.000237	eta 2:47:40
Epoch: [26/150][80/185]	Time 0.445 (0.429)	Data 0.000 (0.006)	Loss_t 0.0190 (0.0251)	Loss_x 1.2639 (1.2356)	Acc 98.44 (98.48)	Lr 0.000237	eta 2:44:50
Epoch: [26/150][100/185]	Time 0.492 (0.433)	Data 0.000 (0.004)	Loss_t 0.0192 (0.0266)	Loss_x 1.2176 (1.2388)	Acc 96.88 (98.31)	Lr 0.000237	eta 2:46:05
Epoch: [26/150][120/185]	Time 0.505 (0.435)	Data 0.000 (0.004)	Loss_t 0.0199 (0.0264)	Loss_x 1.2150 (1.2372)	Acc 100.00 (98.28)	Lr 0.000237	eta 2:46:36
Epoch: [26/150][140/185]	Time 0.425 (0.439)	Data 0.000 (0.003)	Loss_t 0.0173 (0.0276)	Loss_x 1.1759 (1.2372)	Acc 100.00 (98.23)	Lr 0.000237	eta 2:48:06
Epoch: [26/150][160/185]	Time 0.383 (0.437)	Data 0.000 (0.003)	Loss_t 0.0393 (0.0275)	Loss_x 1.2508 (1.2333)	Acc 96.88 (98.28)	Lr 0.000237	eta 2:47:19
Epoch: [26/150][180/185]	Time 0.444 (0.436)	Data 0.000 (0.003)	Loss_t 0.0033 (0.0276)	Loss_x 1.1587 (1.2267)	Acc 98.44 (98.36)	Lr 0.000237	eta 2:46:54
Epoch: [27/150][20/185]	Time 0.448 (0.457)	Data 0.000 (0.024)	Loss_t 0.0418 (0.0332)	Loss_x 1.3337 (1.2814)	Acc 96.88 (97.42)	Lr 0.000245	eta 2:54:37
Epoch: [27/150][40/185]	Time 0.408 (0.445)	Data 0.000 (0.012)	Loss_t 0.0041 (0.0298)	Loss_x 1.2212 (1.2530)	Acc 98.44 (98.12)	Lr 0.000245	eta 2:49:46
Epoch: [27/150][60/185]	Time 0.470 (0.439)	Data 0.000 (0.008)	Loss_t 0.0342 (0.0284)	Loss_x 1.1730 (1.2489)	Acc 100.00 (98.12)	Lr 0.000245	eta 2:47:25
Epoch: [27/150][80/185]	Time 0.367 (0.434)	Data 0.000 (0.006)	Loss_t 0.0385 (0.0281)	Loss_x 1.1778 (1.2452)	Acc 100.00 (98.24)	Lr 0.000245	eta 2:45:24
Epoch: [27/150][100/185]	Time 0.360 (0.423)	Data 0.000 (0.005)	Loss_t 0.0471 (0.0282)	Loss_x 1.2841 (1.2412)	Acc 95.31 (98.28)	Lr 0.000245	eta 2:41:08
Epoch: [27/150][120/185]	Time 0.371 (0.413)	Data 0.000 (0.004)	Loss_t 0.0230 (0.0281)	Loss_x 1.2517 (1.2403)	Acc 100.00 (98.35)	Lr 0.000245	eta 2:37:00
Epoch: [27/150][140/185]	Time 0.509 (0.412)	Data 0.000 (0.003)	Loss_t 0.0224 (0.0272)	Loss_x 1.3280 (1.2353)	Acc 98.44 (98.40)	Lr 0.000245	eta 2:36:42
Epoch: [27/150][160/185]	Time 0.445 (0.412)	Data 0.000 (0.003)	Loss_t 0.0208 (0.0275)	Loss_x 1.1745 (1.2298)	Acc 98.44 (98.41)	Lr 0.000245	eta 2:36:30
Epoch: [27/150][180/185]	Time 0.426 (0.413)	Data 0.000 (0.003)	Loss_t 0.0489 (0.0275)	Loss_x 1.1764 (1.2248)	Acc 98.44 (98.43)	Lr 0.000245	eta 2:36:34
Epoch: [28/150][20/185]	Time 0.393 (0.427)	Data 0.000 (0.023)	Loss_t 0.0244 (0.0224)	Loss_x 1.2976 (1.2522)	Acc 98.44 (97.81)	Lr 0.000253	eta 2:41:45
Epoch: [28/150][40/185]	Time 0.416 (0.425)	Data 0.000 (0.012)	Loss_t 0.0243 (0.0220)	Loss_x 1.2446 (1.2470)	Acc 95.31 (97.93)	Lr 0.000253	eta 2:40:46
Epoch: [28/150][60/185]	Time 0.369 (0.423)	Data 0.000 (0.008)	Loss_t 0.0425 (0.0262)	Loss_x 1.2087 (1.2484)	Acc 100.00 (98.02)	Lr 0.000253	eta 2:39:59
Epoch: [28/150][80/185]	Time 0.435 (0.423)	Data 0.000 (0.006)	Loss_t 0.0275 (0.0256)	Loss_x 1.2292 (1.2463)	Acc 100.00 (98.16)	Lr 0.000253	eta 2:39:46
Epoch: [28/150][100/185]	Time 0.364 (0.416)	Data 0.000 (0.005)	Loss_t 0.0204 (0.0251)	Loss_x 1.2082 (1.2426)	Acc 98.44 (98.14)	Lr 0.000253	eta 2:36:55
Epoch: [28/150][120/185]	Time 0.356 (0.407)	Data 0.000 (0.004)	Loss_t 0.0214 (0.0245)	Loss_x 1.2114 (1.2376)	Acc 98.44 (98.19)	Lr 0.000253	eta 2:33:38
