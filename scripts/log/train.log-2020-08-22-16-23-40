Show configuration
adam:
  beta1: 0.9
  beta2: 0.999
cuhk03:
  classic_split: False
  labeled_images: True
  use_metric_cuhk03: False
data:
  combineall: False
  height: 256
  norm_mean: [0.485, 0.456, 0.406]
  norm_std: [0.229, 0.224, 0.225]
  root: /home/s2019020843/changeeeee/data/
  save_dir: log
  sources: ['market1501']
  split_id: 0
  targets: ['market1501']
  transforms: ['random_flip', 'random_erase']
  type: image
  width: 128
  workers: 4
loss:
  name: triplet
  softmax:
    label_smooth: True
  triplet:
    margin: 0.0
    weight_t: 1.0
    weight_x: 1.0
market1501:
  use_500k_distractors: False
model:
  load_weights: 
  name: plr_osnet
  pretrained: True
  resume: 
rmsprop:
  alpha: 0.99
sampler:
  num_instances: 4
  train_sampler: RandomIdentitySampler
sgd:
  dampening: 0.0
  momentum: 0.9
  nesterov: False
test:
  batch_size: 300
  dist_metric: euclidean
  eval_freq: 10
  evaluate: False
  normalize_feature: False
  ranks: [1, 5, 10, 20]
  rerank: False
  start_eval: 0
  visactmap: False
  visrank: False
  visrank_topk: 10
train:
  base_lr_mult: 0.1
  batch_size: 64
  fixbase_epoch: 0
  gamma: 0.1
  lr: 3.5e-05
  lr_scheduler: warmup
  max_epoch: 150
  multiplier: 10
  new_layers: ['classifier']
  open_layers: ['classifier']
  optim: adam
  print_freq: 20
  seed: 1
  staged_lr: False
  start_epoch: 0
  stepsize: [60, 90]
  total_epoch: 39
  weight_decay: 0.0005
use_gpu: True
video:
  pooling_method: avg
  sample_method: evenly
  seq_len: 15

Collecting env info ...
** System info **
PyTorch version: 1.5.1
Is debug build: No
CUDA used to build PyTorch: 10.2

OS: CentOS Linux 7 (Core)
GCC version: (GCC) 4.8.5 20150623 (Red Hat 4.8.5-39)
CMake version: Could not collect

Python version: 3.7
Is CUDA available: Yes
CUDA runtime version: 10.2.89
GPU models and configuration: 
GPU 0: GeForce RTX 2080 Ti
GPU 1: GeForce RTX 2080 Ti
GPU 2: GeForce RTX 2080 Ti
GPU 3: GeForce RTX 2080 Ti

Nvidia driver version: 440.82
cuDNN version: Could not collect

Versions of relevant libraries:
[pip3] numpy==1.19.0
[pip3] torch==1.5.1
[pip3] torchvision==0.6.1
[conda] torch                     1.5.1                     <pip>
[conda] torchvision               0.6.1                     <pip>
        Pillow (7.1.2)

Building train transforms ...
+ resize to 256x128
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
+ random erase
Building test transforms ...
+ resize to 256x128
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
=> Loading train (source) dataset
=> Loaded Market1501
  ----------------------------------------
  subset   | # ids | # images | # cameras
  ----------------------------------------
  train    |   751 |    12936 |         6
  query    |   750 |     3368 |         6
  gallery  |   751 |    15913 |         6
  ----------------------------------------
=> Loading test (target) dataset
=> Loaded Market1501
  ----------------------------------------
  subset   | # ids | # images | # cameras
  ----------------------------------------
  train    |   751 |    12936 |         6
  query    |   750 |     3368 |         6
  gallery  |   751 |    15913 |         6
  ----------------------------------------


  **************** Summary ****************
  train            : ['market1501']
  # train datasets : 1
  # train ids      : 751
  # train images   : 12936
  # train cameras  : 6
  test             : ['market1501']
  *****************************************


Building model: plr_osnet
Successfully loaded imagenet pretrained weights from "/home/s2019020843/.cache/torch/checkpoints/osnet_x1_0_imagenet.pth"
Model complexity: params=2,529,488 flops=1,071,153,040
Model structure: PLR_OSNet(
  (layer0): Sequential(
    (0): ConvLayer(
      (conv): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (1): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
  )
  (layer10): Sequential(
    (0): OSBlock(
      (conv1): Conv1x1(
        (conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (conv2a): LightConv3x3(
        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)
        (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (conv2b): Sequential(
        (0): LightConv3x3(
          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)
          (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (1): LightConv3x3(
          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)
          (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
      )
      (conv2c): Sequential(
        (0): LightConv3x3(
          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)
          (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (1): LightConv3x3(
          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)
          (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (2): LightConv3x3(
          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)
          (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
      )
      (conv2d): Sequential(
        (0): LightConv3x3(
          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)
          (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (1): LightConv3x3(
          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)
          (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (2): LightConv3x3(
          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)
          (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (3): LightConv3x3(
          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)
          (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
      )
      (gate): ChannelGate(
        (global_avgpool): AdaptiveAvgPool2d(output_size=1)
        (fc1): Conv2d(64, 4, kernel_size=(1, 1), stride=(1, 1))
        (relu): ReLU(inplace=True)
        (fc2): Conv2d(4, 64, kernel_size=(1, 1), stride=(1, 1))
        (gate_activation): Sigmoid()
      )
      (conv3): Conv1x1Linear(
        (conv): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (downsample): Conv1x1Linear(
        (conv): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): OSBlock(
      (conv1): Conv1x1(
        (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (conv2a): LightConv3x3(
        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)
        (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (conv2b): Sequential(
        (0): LightConv3x3(
          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)
          (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (1): LightConv3x3(
          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)
          (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
      )
      (conv2c): Sequential(
        (0): LightConv3x3(
          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)
          (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (1): LightConv3x3(
          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)
          (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (2): LightConv3x3(
          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)
          (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
      )
      (conv2d): Sequential(
        (0): LightConv3x3(
          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)
          (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (1): LightConv3x3(
          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)
          (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (2): LightConv3x3(
          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)
          (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (3): LightConv3x3(
          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)
          (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
      )
      (gate): ChannelGate(
        (global_avgpool): AdaptiveAvgPool2d(output_size=1)
        (fc1): Conv2d(64, 4, kernel_size=(1, 1), stride=(1, 1))
        (relu): ReLU(inplace=True)
        (fc2): Conv2d(4, 64, kernel_size=(1, 1), stride=(1, 1))
        (gate_activation): Sigmoid()
      )
      (conv3): Conv1x1Linear(
        (conv): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (2): Sequential(
      (0): Conv1x1(
        (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (1): AvgPool2d(kernel_size=2, stride=2, padding=0)
    )
  )
  (layer11): Sequential(
    (0): Sequential(
      (0): OSBlock(
        (conv1): Conv1x1(
          (conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (conv2a): LightConv3x3(
          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)
          (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (conv2b): Sequential(
          (0): LightConv3x3(
            (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)
            (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (1): LightConv3x3(
            (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)
            (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
        )
        (conv2c): Sequential(
          (0): LightConv3x3(
            (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)
            (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (1): LightConv3x3(
            (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)
            (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (2): LightConv3x3(
            (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)
            (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
        )
        (conv2d): Sequential(
          (0): LightConv3x3(
            (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)
            (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (1): LightConv3x3(
            (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)
            (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (2): LightConv3x3(
            (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)
            (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (3): LightConv3x3(
            (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)
            (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
        )
        (gate): ChannelGate(
          (global_avgpool): AdaptiveAvgPool2d(output_size=1)
          (fc1): Conv2d(64, 4, kernel_size=(1, 1), stride=(1, 1))
          (relu): ReLU(inplace=True)
          (fc2): Conv2d(4, 64, kernel_size=(1, 1), stride=(1, 1))
          (gate_activation): Sigmoid()
        )
        (conv3): Conv1x1Linear(
          (conv): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (downsample): Conv1x1Linear(
          (conv): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): OSBlock(
        (conv1): Conv1x1(
          (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (conv2a): LightConv3x3(
          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)
          (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (conv2b): Sequential(
          (0): LightConv3x3(
            (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)
            (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (1): LightConv3x3(
            (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)
            (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
        )
        (conv2c): Sequential(
          (0): LightConv3x3(
            (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)
            (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (1): LightConv3x3(
            (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)
            (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (2): LightConv3x3(
            (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)
            (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
        )
        (conv2d): Sequential(
          (0): LightConv3x3(
            (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)
            (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (1): LightConv3x3(
            (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)
            (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (2): LightConv3x3(
            (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)
            (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (3): LightConv3x3(
            (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)
            (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
        )
        (gate): ChannelGate(
          (global_avgpool): AdaptiveAvgPool2d(output_size=1)
          (fc1): Conv2d(64, 4, kernel_size=(1, 1), stride=(1, 1))
          (relu): ReLU(inplace=True)
          (fc2): Conv2d(4, 64, kernel_size=(1, 1), stride=(1, 1))
          (gate_activation): Sigmoid()
        )
        (conv3): Conv1x1Linear(
          (conv): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (2): Sequential(
        (0): Conv1x1(
          (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (1): AvgPool2d(kernel_size=2, stride=2, padding=0)
      )
    )
  )
  (layer20): Sequential(
    (0): OSBlock(
      (conv1): Conv1x1(
        (conv): Conv2d(256, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (conv2a): LightConv3x3(
        (conv1): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv2): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
        (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (conv2b): Sequential(
        (0): LightConv3x3(
          (conv1): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (conv2): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
          (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (1): LightConv3x3(
          (conv1): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (conv2): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
          (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
      )
      (conv2c): Sequential(
        (0): LightConv3x3(
          (conv1): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (conv2): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
          (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (1): LightConv3x3(
          (conv1): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (conv2): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
          (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (2): LightConv3x3(
          (conv1): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (conv2): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
          (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
      )
      (conv2d): Sequential(
        (0): LightConv3x3(
          (conv1): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (conv2): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
          (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (1): LightConv3x3(
          (conv1): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (conv2): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
          (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (2): LightConv3x3(
          (conv1): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (conv2): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
          (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (3): LightConv3x3(
          (conv1): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (conv2): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
          (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
      )
      (gate): ChannelGate(
        (global_avgpool): AdaptiveAvgPool2d(output_size=1)
        (fc1): Conv2d(96, 6, kernel_size=(1, 1), stride=(1, 1))
        (relu): ReLU(inplace=True)
        (fc2): Conv2d(6, 96, kernel_size=(1, 1), stride=(1, 1))
        (gate_activation): Sigmoid()
      )
      (conv3): Conv1x1Linear(
        (conv): Conv2d(96, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (downsample): Conv1x1Linear(
        (conv): Conv2d(256, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): OSBlock(
      (conv1): Conv1x1(
        (conv): Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (conv2a): LightConv3x3(
        (conv1): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv2): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
        (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (conv2b): Sequential(
        (0): LightConv3x3(
          (conv1): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (conv2): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
          (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (1): LightConv3x3(
          (conv1): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (conv2): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
          (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
      )
      (conv2c): Sequential(
        (0): LightConv3x3(
          (conv1): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (conv2): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
          (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (1): LightConv3x3(
          (conv1): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (conv2): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
          (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (2): LightConv3x3(
          (conv1): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (conv2): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
          (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
      )
      (conv2d): Sequential(
        (0): LightConv3x3(
          (conv1): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (conv2): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
          (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (1): LightConv3x3(
          (conv1): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (conv2): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
          (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (2): LightConv3x3(
          (conv1): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (conv2): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
          (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (3): LightConv3x3(
          (conv1): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (conv2): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
          (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
      )
      (gate): ChannelGate(
        (global_avgpool): AdaptiveAvgPool2d(output_size=1)
        (fc1): Conv2d(96, 6, kernel_size=(1, 1), stride=(1, 1))
        (relu): ReLU(inplace=True)
        (fc2): Conv2d(6, 96, kernel_size=(1, 1), stride=(1, 1))
        (gate_activation): Sigmoid()
      )
      (conv3): Conv1x1Linear(
        (conv): Conv2d(96, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (2): Sequential(
      (0): Conv1x1(
        (conv): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (1): AvgPool2d(kernel_size=2, stride=2, padding=0)
    )
  )
  (layer21): Sequential(
    (0): Sequential(
      (0): OSBlock(
        (conv1): Conv1x1(
          (conv): Conv2d(256, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (conv2a): LightConv3x3(
          (conv1): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (conv2): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
          (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (conv2b): Sequential(
          (0): LightConv3x3(
            (conv1): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (conv2): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (1): LightConv3x3(
            (conv1): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (conv2): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
        )
        (conv2c): Sequential(
          (0): LightConv3x3(
            (conv1): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (conv2): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (1): LightConv3x3(
            (conv1): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (conv2): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (2): LightConv3x3(
            (conv1): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (conv2): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
        )
        (conv2d): Sequential(
          (0): LightConv3x3(
            (conv1): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (conv2): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (1): LightConv3x3(
            (conv1): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (conv2): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (2): LightConv3x3(
            (conv1): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (conv2): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (3): LightConv3x3(
            (conv1): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (conv2): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
        )
        (gate): ChannelGate(
          (global_avgpool): AdaptiveAvgPool2d(output_size=1)
          (fc1): Conv2d(96, 6, kernel_size=(1, 1), stride=(1, 1))
          (relu): ReLU(inplace=True)
          (fc2): Conv2d(6, 96, kernel_size=(1, 1), stride=(1, 1))
          (gate_activation): Sigmoid()
        )
        (conv3): Conv1x1Linear(
          (conv): Conv2d(96, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (downsample): Conv1x1Linear(
          (conv): Conv2d(256, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): OSBlock(
        (conv1): Conv1x1(
          (conv): Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (conv2a): LightConv3x3(
          (conv1): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (conv2): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
          (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (conv2b): Sequential(
          (0): LightConv3x3(
            (conv1): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (conv2): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (1): LightConv3x3(
            (conv1): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (conv2): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
        )
        (conv2c): Sequential(
          (0): LightConv3x3(
            (conv1): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (conv2): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (1): LightConv3x3(
            (conv1): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (conv2): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (2): LightConv3x3(
            (conv1): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (conv2): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
        )
        (conv2d): Sequential(
          (0): LightConv3x3(
            (conv1): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (conv2): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (1): LightConv3x3(
            (conv1): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (conv2): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (2): LightConv3x3(
            (conv1): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (conv2): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (3): LightConv3x3(
            (conv1): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (conv2): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
        )
        (gate): ChannelGate(
          (global_avgpool): AdaptiveAvgPool2d(output_size=1)
          (fc1): Conv2d(96, 6, kernel_size=(1, 1), stride=(1, 1))
          (relu): ReLU(inplace=True)
          (fc2): Conv2d(6, 96, kernel_size=(1, 1), stride=(1, 1))
          (gate_activation): Sigmoid()
        )
        (conv3): Conv1x1Linear(
          (conv): Conv2d(96, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (2): Sequential(
        (0): Conv1x1(
          (conv): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (1): AvgPool2d(kernel_size=2, stride=2, padding=0)
      )
    )
  )
  (layer30): Sequential(
    (0): OSBlock(
      (conv1): Conv1x1(
        (conv): Conv2d(384, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (conv2a): LightConv3x3(
        (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)
        (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (conv2b): Sequential(
        (0): LightConv3x3(
          (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)
          (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (1): LightConv3x3(
          (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)
          (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
      )
      (conv2c): Sequential(
        (0): LightConv3x3(
          (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)
          (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (1): LightConv3x3(
          (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)
          (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (2): LightConv3x3(
          (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)
          (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
      )
      (conv2d): Sequential(
        (0): LightConv3x3(
          (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)
          (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (1): LightConv3x3(
          (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)
          (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (2): LightConv3x3(
          (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)
          (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (3): LightConv3x3(
          (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)
          (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
      )
      (gate): ChannelGate(
        (global_avgpool): AdaptiveAvgPool2d(output_size=1)
        (fc1): Conv2d(128, 8, kernel_size=(1, 1), stride=(1, 1))
        (relu): ReLU(inplace=True)
        (fc2): Conv2d(8, 128, kernel_size=(1, 1), stride=(1, 1))
        (gate_activation): Sigmoid()
      )
      (conv3): Conv1x1Linear(
        (conv): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (downsample): Conv1x1Linear(
        (conv): Conv2d(384, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): OSBlock(
      (conv1): Conv1x1(
        (conv): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (conv2a): LightConv3x3(
        (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)
        (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (conv2b): Sequential(
        (0): LightConv3x3(
          (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)
          (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (1): LightConv3x3(
          (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)
          (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
      )
      (conv2c): Sequential(
        (0): LightConv3x3(
          (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)
          (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (1): LightConv3x3(
          (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)
          (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (2): LightConv3x3(
          (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)
          (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
      )
      (conv2d): Sequential(
        (0): LightConv3x3(
          (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)
          (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (1): LightConv3x3(
          (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)
          (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (2): LightConv3x3(
          (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)
          (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (3): LightConv3x3(
          (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)
          (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
      )
      (gate): ChannelGate(
        (global_avgpool): AdaptiveAvgPool2d(output_size=1)
        (fc1): Conv2d(128, 8, kernel_size=(1, 1), stride=(1, 1))
        (relu): ReLU(inplace=True)
        (fc2): Conv2d(8, 128, kernel_size=(1, 1), stride=(1, 1))
        (gate_activation): Sigmoid()
      )
      (conv3): Conv1x1Linear(
        (conv): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (layer31): Sequential(
    (0): Sequential(
      (0): OSBlock(
        (conv1): Conv1x1(
          (conv): Conv2d(384, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (conv2a): LightConv3x3(
          (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)
          (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (conv2b): Sequential(
          (0): LightConv3x3(
            (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)
            (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (1): LightConv3x3(
            (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)
            (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
        )
        (conv2c): Sequential(
          (0): LightConv3x3(
            (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)
            (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (1): LightConv3x3(
            (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)
            (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (2): LightConv3x3(
            (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)
            (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
        )
        (conv2d): Sequential(
          (0): LightConv3x3(
            (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)
            (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (1): LightConv3x3(
            (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)
            (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (2): LightConv3x3(
            (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)
            (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (3): LightConv3x3(
            (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)
            (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
        )
        (gate): ChannelGate(
          (global_avgpool): AdaptiveAvgPool2d(output_size=1)
          (fc1): Conv2d(128, 8, kernel_size=(1, 1), stride=(1, 1))
          (relu): ReLU(inplace=True)
          (fc2): Conv2d(8, 128, kernel_size=(1, 1), stride=(1, 1))
          (gate_activation): Sigmoid()
        )
        (conv3): Conv1x1Linear(
          (conv): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (downsample): Conv1x1Linear(
          (conv): Conv2d(384, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): OSBlock(
        (conv1): Conv1x1(
          (conv): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (conv2a): LightConv3x3(
          (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)
          (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (conv2b): Sequential(
          (0): LightConv3x3(
            (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)
            (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (1): LightConv3x3(
            (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)
            (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
        )
        (conv2c): Sequential(
          (0): LightConv3x3(
            (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)
            (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (1): LightConv3x3(
            (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)
            (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (2): LightConv3x3(
            (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)
            (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
        )
        (conv2d): Sequential(
          (0): LightConv3x3(
            (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)
            (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (1): LightConv3x3(
            (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)
            (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (2): LightConv3x3(
            (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)
            (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (3): LightConv3x3(
            (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)
            (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
        )
        (gate): ChannelGate(
          (global_avgpool): AdaptiveAvgPool2d(output_size=1)
          (fc1): Conv2d(128, 8, kernel_size=(1, 1), stride=(1, 1))
          (relu): ReLU(inplace=True)
          (fc2): Conv2d(8, 128, kernel_size=(1, 1), stride=(1, 1))
          (gate_activation): Sigmoid()
        )
        (conv3): Conv1x1Linear(
          (conv): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
    )
  )
  (layer40): Conv1x1(
    (conv): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
  )
  (layer41): Sequential(
    (0): Conv1x1(
      (conv): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (conv10): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (conv20): Conv2d(384, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (global_avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
  (global_maxpool): AdaptiveMaxPool2d(output_size=(1, 1))
  (fc1): Linear(in_features=512, out_features=512, bias=True)
  (bn1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn2): BatchNorm1d(4096, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (classifier1): Linear(in_features=512, out_features=751, bias=True)
  (classifier2): Linear(in_features=4096, out_features=751, bias=True)
)
Building triplet-engine for image-reid
=> Start training
Epoch: [1/150][20/185]	Time 0.412 (0.511)	Data 0.000 (0.028)	Loss_t 0.9626 (1.0084)	Loss_x 6.7359 (6.7130)	Acc 0.00 (0.08)	Lr 0.000035	eta 3:56:16
Epoch: [1/150][40/185]	Time 0.398 (0.465)	Data 0.000 (0.014)	Loss_t 0.9141 (0.9724)	Loss_x 6.6558 (6.6954)	Acc 0.00 (0.04)	Lr 0.000035	eta 3:34:39
Epoch: [1/150][60/185]	Time 0.366 (0.445)	Data 0.000 (0.009)	Loss_t 0.8938 (0.9483)	Loss_x 6.6348 (6.6754)	Acc 0.00 (0.08)	Lr 0.000035	eta 3:25:26
Epoch: [1/150][80/185]	Time 0.411 (0.435)	Data 0.000 (0.007)	Loss_t 0.8784 (0.9335)	Loss_x 6.5329 (6.6535)	Acc 0.00 (0.06)	Lr 0.000035	eta 3:20:23
Epoch: [1/150][100/185]	Time 0.436 (0.433)	Data 0.000 (0.006)	Loss_t 0.8559 (0.9208)	Loss_x 6.5215 (6.6347)	Acc 0.00 (0.06)	Lr 0.000035	eta 3:19:26
Epoch: [1/150][120/185]	Time 0.422 (0.432)	Data 0.000 (0.005)	Loss_t 0.8672 (0.9108)	Loss_x 6.5045 (6.6138)	Acc 0.00 (0.09)	Lr 0.000035	eta 3:18:54
Epoch: [1/150][140/185]	Time 0.349 (0.425)	Data 0.000 (0.004)	Loss_t 0.8536 (0.9027)	Loss_x 6.4503 (6.5942)	Acc 0.00 (0.16)	Lr 0.000035	eta 3:15:29
Epoch: [1/150][160/185]	Time 0.422 (0.427)	Data 0.000 (0.004)	Loss_t 0.8513 (0.8964)	Loss_x 6.3772 (6.5711)	Acc 0.00 (0.16)	Lr 0.000035	eta 3:16:14
Epoch: [1/150][180/185]	Time 0.448 (0.429)	Data 0.000 (0.003)	Loss_t 0.8347 (0.8906)	Loss_x 6.1599 (6.5377)	Acc 0.00 (0.18)	Lr 0.000035	eta 3:17:14
Epoch: [2/150][20/185]	Time 0.395 (0.459)	Data 0.000 (0.023)	Loss_t 0.7983 (0.8267)	Loss_x 6.2201 (6.2488)	Acc 0.00 (0.39)	Lr 0.000043	eta 3:30:51
Epoch: [2/150][40/185]	Time 0.413 (0.431)	Data 0.000 (0.012)	Loss_t 0.8078 (0.8223)	Loss_x 6.2275 (6.2187)	Acc 0.00 (0.51)	Lr 0.000043	eta 3:17:34
Epoch: [2/150][60/185]	Time 0.427 (0.427)	Data 0.000 (0.008)	Loss_t 0.8031 (0.8209)	Loss_x 6.0852 (6.2005)	Acc 3.12 (0.60)	Lr 0.000043	eta 3:15:37
Epoch: [2/150][80/185]	Time 0.422 (0.423)	Data 0.000 (0.006)	Loss_t 0.8269 (0.8195)	Loss_x 6.0175 (6.1680)	Acc 0.00 (0.74)	Lr 0.000043	eta 3:13:38
Epoch: [2/150][100/185]	Time 0.429 (0.423)	Data 0.000 (0.005)	Loss_t 0.8006 (0.8184)	Loss_x 5.9938 (6.1391)	Acc 0.00 (0.97)	Lr 0.000043	eta 3:13:47
Epoch: [2/150][120/185]	Time 0.460 (0.425)	Data 0.000 (0.004)	Loss_t 0.8059 (0.8170)	Loss_x 6.0116 (6.1100)	Acc 4.69 (1.25)	Lr 0.000043	eta 3:14:29
Epoch: [2/150][140/185]	Time 0.444 (0.425)	Data 0.000 (0.003)	Loss_t 0.8146 (0.8161)	Loss_x 5.7099 (6.0720)	Acc 4.69 (1.72)	Lr 0.000043	eta 3:14:07
Epoch: [2/150][160/185]	Time 0.436 (0.426)	Data 0.000 (0.003)	Loss_t 0.8083 (0.8153)	Loss_x 5.5644 (6.0249)	Acc 4.69 (2.03)	Lr 0.000043	eta 3:14:23
Epoch: [2/150][180/185]	Time 0.433 (0.427)	Data 0.000 (0.003)	Loss_t 0.7905 (0.8135)	Loss_x 5.0405 (5.9529)	Acc 15.62 (3.06)	Lr 0.000043	eta 3:14:45
Epoch: [3/150][20/185]	Time 0.456 (0.475)	Data 0.000 (0.024)	Loss_t 0.7936 (0.7992)	Loss_x 5.7712 (5.6482)	Acc 6.25 (6.56)	Lr 0.000051	eta 3:36:37
Epoch: [3/150][40/185]	Time 0.346 (0.436)	Data 0.000 (0.012)	Loss_t 0.8048 (0.7996)	Loss_x 5.4796 (5.6312)	Acc 6.25 (5.86)	Lr 0.000051	eta 3:18:26
Epoch: [3/150][60/185]	Time 0.445 (0.423)	Data 0.000 (0.008)	Loss_t 0.7955 (0.7972)	Loss_x 5.8186 (5.6138)	Acc 3.12 (5.89)	Lr 0.000051	eta 3:12:23
Epoch: [3/150][80/185]	Time 0.437 (0.426)	Data 0.000 (0.006)	Loss_t 0.7988 (0.7966)	Loss_x 5.4466 (5.5882)	Acc 6.25 (6.00)	Lr 0.000051	eta 3:13:53
Epoch: [3/150][100/185]	Time 0.452 (0.431)	Data 0.000 (0.005)	Loss_t 0.7684 (0.7960)	Loss_x 5.0475 (5.5381)	Acc 12.50 (6.41)	Lr 0.000051	eta 3:15:56
Epoch: [3/150][120/185]	Time 0.424 (0.432)	Data 0.000 (0.004)	Loss_t 0.7978 (0.7927)	Loss_x 5.1743 (5.4797)	Acc 9.38 (7.27)	Lr 0.000051	eta 3:16:21
Epoch: [3/150][140/185]	Time 0.408 (0.432)	Data 0.000 (0.004)	Loss_t 0.7897 (0.7924)	Loss_x 5.1714 (5.4325)	Acc 12.50 (7.68)	Lr 0.000051	eta 3:15:58
Epoch: [3/150][160/185]	Time 0.407 (0.432)	Data 0.000 (0.003)	Loss_t 0.7643 (0.7908)	Loss_x 4.6942 (5.3648)	Acc 31.25 (8.89)	Lr 0.000051	eta 3:15:57
Epoch: [3/150][180/185]	Time 0.426 (0.429)	Data 0.000 (0.003)	Loss_t 0.8024 (0.7902)	Loss_x 4.5674 (5.2859)	Acc 39.06 (10.50)	Lr 0.000051	eta 3:14:35
Epoch: [4/150][20/185]	Time 0.421 (0.448)	Data 0.000 (0.027)	Loss_t 0.8076 (0.7584)	Loss_x 4.9187 (5.0620)	Acc 6.25 (8.67)	Lr 0.000059	eta 3:22:55
Epoch: [4/150][40/185]	Time 0.436 (0.438)	Data 0.000 (0.013)	Loss_t 0.8222 (0.7646)	Loss_x 5.3270 (5.0758)	Acc 7.81 (7.97)	Lr 0.000059	eta 3:18:03
Epoch: [4/150][60/185]	Time 0.404 (0.431)	Data 0.000 (0.009)	Loss_t 0.7757 (0.7655)	Loss_x 5.1612 (5.0214)	Acc 6.25 (8.49)	Lr 0.000059	eta 3:14:48
Epoch: [4/150][80/185]	Time 0.427 (0.431)	Data 0.000 (0.007)	Loss_t 0.7284 (0.7655)	Loss_x 4.6222 (4.9720)	Acc 10.94 (9.22)	Lr 0.000059	eta 3:14:39
Epoch: [4/150][100/185]	Time 0.435 (0.432)	Data 0.000 (0.005)	Loss_t 0.7651 (0.7642)	Loss_x 4.5124 (4.9113)	Acc 17.19 (9.89)	Lr 0.000059	eta 3:14:54
Epoch: [4/150][120/185]	Time 0.437 (0.432)	Data 0.000 (0.005)	Loss_t 0.7477 (0.7641)	Loss_x 4.5158 (4.8656)	Acc 21.88 (10.76)	Lr 0.000059	eta 3:14:48
Epoch: [4/150][140/185]	Time 0.445 (0.432)	Data 0.000 (0.004)	Loss_t 0.7414 (0.7639)	Loss_x 4.4280 (4.8053)	Acc 25.00 (12.11)	Lr 0.000059	eta 3:14:58
Epoch: [4/150][160/185]	Time 0.433 (0.432)	Data 0.000 (0.003)	Loss_t 0.7471 (0.7634)	Loss_x 4.0977 (4.7416)	Acc 23.44 (13.18)	Lr 0.000059	eta 3:14:36
Epoch: [4/150][180/185]	Time 0.391 (0.431)	Data 0.000 (0.003)	Loss_t 0.8059 (0.7629)	Loss_x 3.9487 (4.6558)	Acc 37.50 (15.65)	Lr 0.000059	eta 3:14:00
Epoch: [5/150][20/185]	Time 0.525 (0.459)	Data 0.000 (0.025)	Loss_t 0.7718 (0.7508)	Loss_x 4.5399 (4.5914)	Acc 10.94 (9.06)	Lr 0.000067	eta 3:26:26
Epoch: [5/150][40/185]	Time 0.353 (0.450)	Data 0.000 (0.012)	Loss_t 0.7812 (0.7435)	Loss_x 4.4711 (4.5237)	Acc 20.31 (9.61)	Lr 0.000067	eta 3:22:19
Epoch: [5/150][60/185]	Time 0.349 (0.424)	Data 0.000 (0.008)	Loss_t 0.7106 (0.7443)	Loss_x 4.3947 (4.4864)	Acc 12.50 (10.36)	Lr 0.000067	eta 3:10:26
Epoch: [5/150][80/185]	Time 0.377 (0.410)	Data 0.000 (0.006)	Loss_t 0.7095 (0.7393)	Loss_x 4.1335 (4.4238)	Acc 17.19 (11.46)	Lr 0.000067	eta 3:04:06
Epoch: [5/150][100/185]	Time 0.427 (0.399)	Data 0.000 (0.005)	Loss_t 0.7791 (0.7405)	Loss_x 4.5004 (4.3798)	Acc 7.81 (12.55)	Lr 0.000067	eta 2:59:10
Epoch: [5/150][120/185]	Time 0.426 (0.405)	Data 0.000 (0.004)	Loss_t 0.7408 (0.7400)	Loss_x 3.9746 (4.3315)	Acc 23.44 (13.84)	Lr 0.000067	eta 3:01:42
Epoch: [5/150][140/185]	Time 0.419 (0.409)	Data 0.000 (0.004)	Loss_t 0.7556 (0.7374)	Loss_x 3.9872 (4.2683)	Acc 21.88 (15.78)	Lr 0.000067	eta 3:02:57
Epoch: [5/150][160/185]	Time 0.419 (0.409)	Data 0.000 (0.003)	Loss_t 0.7445 (0.7366)	Loss_x 3.6155 (4.2033)	Acc 31.25 (17.99)	Lr 0.000067	eta 3:03:09
Epoch: [5/150][180/185]	Time 0.413 (0.410)	Data 0.000 (0.003)	Loss_t 0.7349 (0.7357)	Loss_x 3.2669 (4.1141)	Acc 43.75 (21.25)	Lr 0.000067	eta 3:03:32
Epoch: [6/150][20/185]	Time 0.419 (0.448)	Data 0.000 (0.028)	Loss_t 0.7242 (0.7222)	Loss_x 4.3500 (4.1313)	Acc 7.81 (13.67)	Lr 0.000075	eta 3:20:12
Epoch: [6/150][40/185]	Time 0.420 (0.438)	Data 0.000 (0.014)	Loss_t 0.6818 (0.7139)	Loss_x 3.7549 (4.0397)	Acc 18.75 (14.22)	Lr 0.000075	eta 3:15:33
Epoch: [6/150][60/185]	Time 0.428 (0.432)	Data 0.000 (0.010)	Loss_t 0.6897 (0.7151)	Loss_x 3.4361 (3.9898)	Acc 29.69 (14.74)	Lr 0.000075	eta 3:12:37
Epoch: [6/150][80/185]	Time 0.421 (0.428)	Data 0.000 (0.007)	Loss_t 0.6842 (0.7125)	Loss_x 4.0124 (3.9503)	Acc 6.25 (15.23)	Lr 0.000075	eta 3:10:58
Epoch: [6/150][100/185]	Time 0.410 (0.427)	Data 0.000 (0.006)	Loss_t 0.7175 (0.7137)	Loss_x 3.6434 (3.8955)	Acc 20.31 (16.98)	Lr 0.000075	eta 3:10:14
Epoch: [6/150][120/185]	Time 0.420 (0.426)	Data 0.000 (0.005)	Loss_t 0.7307 (0.7130)	Loss_x 3.8668 (3.8531)	Acc 21.88 (18.36)	Lr 0.000075	eta 3:09:40
Epoch: [6/150][140/185]	Time 0.430 (0.426)	Data 0.000 (0.004)	Loss_t 0.6700 (0.7092)	Loss_x 3.3303 (3.7885)	Acc 29.69 (20.38)	Lr 0.000075	eta 3:09:19
Epoch: [6/150][160/185]	Time 0.405 (0.425)	Data 0.000 (0.004)	Loss_t 0.6840 (0.7053)	Loss_x 3.1393 (3.7161)	Acc 51.56 (23.54)	Lr 0.000075	eta 3:08:51
Epoch: [6/150][180/185]	Time 0.410 (0.424)	Data 0.000 (0.003)	Loss_t 0.7106 (0.7043)	Loss_x 2.7335 (3.6387)	Acc 73.44 (27.30)	Lr 0.000075	eta 3:08:14
Epoch: [7/150][20/185]	Time 0.423 (0.451)	Data 0.000 (0.026)	Loss_t 0.6343 (0.6883)	Loss_x 3.3664 (3.6523)	Acc 23.44 (20.00)	Lr 0.000083	eta 3:20:17
Epoch: [7/150][40/185]	Time 0.403 (0.439)	Data 0.000 (0.013)	Loss_t 0.7213 (0.6848)	Loss_x 3.2803 (3.5997)	Acc 40.62 (19.22)	Lr 0.000083	eta 3:14:24
Epoch: [7/150][60/185]	Time 0.421 (0.434)	Data 0.000 (0.009)	Loss_t 0.7020 (0.6868)	Loss_x 3.7042 (3.5780)	Acc 15.62 (19.77)	Lr 0.000083	eta 3:12:20
Epoch: [7/150][80/185]	Time 0.446 (0.430)	Data 0.000 (0.007)	Loss_t 0.7042 (0.6857)	Loss_x 3.1803 (3.5310)	Acc 34.38 (20.84)	Lr 0.000083	eta 3:10:14
Epoch: [7/150][100/185]	Time 0.403 (0.431)	Data 0.000 (0.005)	Loss_t 0.7417 (0.6876)	Loss_x 3.4557 (3.4874)	Acc 29.69 (23.06)	Lr 0.000083	eta 3:10:39
Epoch: [7/150][120/185]	Time 0.443 (0.431)	Data 0.000 (0.005)	Loss_t 0.6004 (0.6861)	Loss_x 2.9123 (3.4397)	Acc 62.50 (25.40)	Lr 0.000083	eta 3:10:32
Epoch: [7/150][140/185]	Time 0.427 (0.431)	Data 0.000 (0.004)	Loss_t 0.6725 (0.6841)	Loss_x 2.7998 (3.3852)	Acc 56.25 (27.58)	Lr 0.000083	eta 3:10:09
Epoch: [7/150][160/185]	Time 0.434 (0.431)	Data 0.000 (0.003)	Loss_t 0.6263 (0.6802)	Loss_x 2.8142 (3.3182)	Acc 50.00 (30.94)	Lr 0.000083	eta 3:10:03
Epoch: [7/150][180/185]	Time 0.417 (0.431)	Data 0.000 (0.003)	Loss_t 0.6257 (0.6788)	Loss_x 2.2873 (3.2381)	Acc 81.25 (35.03)	Lr 0.000083	eta 3:09:58
Epoch: [8/150][20/185]	Time 0.372 (0.438)	Data 0.000 (0.023)	Loss_t 0.6664 (0.6603)	Loss_x 3.1810 (3.2903)	Acc 21.88 (22.66)	Lr 0.000092	eta 3:12:49
Epoch: [8/150][40/185]	Time 0.381 (0.425)	Data 0.000 (0.012)	Loss_t 0.7059 (0.6556)	Loss_x 3.3616 (3.2316)	Acc 10.94 (24.45)	Lr 0.000092	eta 3:07:15
Epoch: [8/150][60/185]	Time 0.474 (0.424)	Data 0.000 (0.008)	Loss_t 0.6089 (0.6566)	Loss_x 3.0805 (3.1791)	Acc 21.88 (27.89)	Lr 0.000092	eta 3:06:39
Epoch: [8/150][80/185]	Time 0.439 (0.424)	Data 0.000 (0.006)	Loss_t 0.6865 (0.6581)	Loss_x 3.2462 (3.1450)	Acc 25.00 (29.24)	Lr 0.000092	eta 3:06:23
Epoch: [8/150][100/185]	Time 0.410 (0.424)	Data 0.000 (0.005)	Loss_t 0.6094 (0.6574)	Loss_x 2.7354 (3.1133)	Acc 39.06 (31.09)	Lr 0.000092	eta 3:06:09
Epoch: [8/150][120/185]	Time 0.406 (0.423)	Data 0.000 (0.004)	Loss_t 0.6276 (0.6561)	Loss_x 2.6788 (3.0756)	Acc 56.25 (32.96)	Lr 0.000092	eta 3:05:47
Epoch: [8/150][140/185]	Time 0.416 (0.421)	Data 0.000 (0.003)	Loss_t 0.6662 (0.6531)	Loss_x 2.7933 (3.0155)	Acc 45.31 (36.13)	Lr 0.000092	eta 3:04:46
Epoch: [8/150][160/185]	Time 0.384 (0.421)	Data 0.000 (0.003)	Loss_t 0.6586 (0.6484)	Loss_x 2.5809 (2.9575)	Acc 64.06 (38.80)	Lr 0.000092	eta 3:04:22
Epoch: [8/150][180/185]	Time 0.432 (0.418)	Data 0.000 (0.003)	Loss_t 0.6029 (0.6446)	Loss_x 2.2421 (2.8804)	Acc 85.94 (42.65)	Lr 0.000092	eta 3:03:09
Epoch: [9/150][20/185]	Time 0.418 (0.450)	Data 0.000 (0.025)	Loss_t 0.5903 (0.6140)	Loss_x 2.5485 (2.8720)	Acc 48.44 (34.22)	Lr 0.000100	eta 3:16:52
Epoch: [9/150][40/185]	Time 0.453 (0.433)	Data 0.000 (0.012)	Loss_t 0.6241 (0.6253)	Loss_x 2.8691 (2.8804)	Acc 31.25 (34.22)	Lr 0.000100	eta 3:09:23
Epoch: [9/150][60/185]	Time 0.419 (0.428)	Data 0.000 (0.008)	Loss_t 0.5551 (0.6255)	Loss_x 2.9607 (2.8721)	Acc 17.19 (34.97)	Lr 0.000100	eta 3:06:54
Epoch: [9/150][80/185]	Time 0.413 (0.431)	Data 0.000 (0.006)	Loss_t 0.6892 (0.6280)	Loss_x 2.6716 (2.8424)	Acc 50.00 (36.91)	Lr 0.000100	eta 3:08:07
Epoch: [9/150][100/185]	Time 0.407 (0.428)	Data 0.000 (0.005)	Loss_t 0.5436 (0.6235)	Loss_x 2.6332 (2.8003)	Acc 53.12 (39.62)	Lr 0.000100	eta 3:06:35
Epoch: [9/150][120/185]	Time 0.389 (0.426)	Data 0.000 (0.004)	Loss_t 0.6284 (0.6236)	Loss_x 2.4449 (2.7629)	Acc 62.50 (41.71)	Lr 0.000100	eta 3:05:42
Epoch: [9/150][140/185]	Time 0.444 (0.427)	Data 0.000 (0.004)	Loss_t 0.6667 (0.6243)	Loss_x 2.2078 (2.7148)	Acc 81.25 (44.70)	Lr 0.000100	eta 3:05:45
Epoch: [9/150][160/185]	Time 0.382 (0.426)	Data 0.000 (0.003)	Loss_t 0.5503 (0.6215)	Loss_x 2.1267 (2.6610)	Acc 78.12 (47.90)	Lr 0.000100	eta 3:05:31
Epoch: [9/150][180/185]	Time 0.443 (0.427)	Data 0.000 (0.003)	Loss_t 0.5383 (0.6191)	Loss_x 1.7141 (2.5897)	Acc 90.62 (51.60)	Lr 0.000100	eta 3:05:32
Epoch: [10/150][20/185]	Time 0.437 (0.438)	Data 0.000 (0.025)	Loss_t 0.5995 (0.6010)	Loss_x 2.4693 (2.7644)	Acc 53.12 (33.20)	Lr 0.000108	eta 3:10:18
Epoch: [10/150][40/185]	Time 0.423 (0.439)	Data 0.000 (0.012)	Loss_t 0.5357 (0.6009)	Loss_x 2.2460 (2.6944)	Acc 62.50 (38.16)	Lr 0.000108	eta 3:10:29
Epoch: [10/150][60/185]	Time 0.446 (0.436)	Data 0.000 (0.008)	Loss_t 0.5897 (0.6019)	Loss_x 2.3790 (2.6422)	Acc 54.69 (41.41)	Lr 0.000108	eta 3:09:06
Epoch: [10/150][80/185]	Time 0.441 (0.435)	Data 0.000 (0.006)	Loss_t 0.5813 (0.5977)	Loss_x 2.2633 (2.5844)	Acc 62.50 (44.96)	Lr 0.000108	eta 3:08:39
Epoch: [10/150][100/185]	Time 0.452 (0.434)	Data 0.000 (0.005)	Loss_t 0.6627 (0.5949)	Loss_x 2.5513 (2.5353)	Acc 50.00 (47.64)	Lr 0.000108	eta 3:07:55
Epoch: [10/150][120/185]	Time 0.432 (0.433)	Data 0.000 (0.004)	Loss_t 0.6024 (0.5921)	Loss_x 2.2210 (2.4938)	Acc 62.50 (50.52)	Lr 0.000108	eta 3:07:29
Epoch: [10/150][140/185]	Time 0.418 (0.431)	Data 0.000 (0.004)	Loss_t 0.5597 (0.5903)	Loss_x 2.0673 (2.4467)	Acc 70.31 (53.39)	Lr 0.000108	eta 3:06:19
Epoch: [10/150][160/185]	Time 0.431 (0.430)	Data 0.000 (0.003)	Loss_t 0.5973 (0.5892)	Loss_x 1.8451 (2.3931)	Acc 90.62 (56.46)	Lr 0.000108	eta 3:05:41
Epoch: [10/150][180/185]	Time 0.421 (0.428)	Data 0.000 (0.003)	Loss_t 0.5725 (0.5857)	Loss_x 1.8962 (2.3321)	Acc 79.69 (59.56)	Lr 0.000108	eta 3:04:57
##### Evaluating market1501 (source) #####
Extracting features from query set ...
Done, obtained 3368-by-4608 matrix
Extracting features from gallery set ...
Done, obtained 15913-by-4608 matrix
Speed: 0.0769 sec/batch
Computing distance matrix with metric=euclidean ...
Computing CMC and mAP ...
** Results **
mAP: 59.0%
CMC curve
Rank-1  : 76.8%
Rank-5  : 91.2%
Rank-10 : 94.6%
Rank-20 : 97.1%
Checkpoint saved to "log/model.pth.tar-10"
Epoch: [11/150][20/185]	Time 0.444 (0.470)	Data 0.000 (0.028)	Loss_t 0.5632 (0.5719)	Loss_x 2.4633 (2.4996)	Acc 53.12 (43.75)	Lr 0.000116	eta 3:22:37
Epoch: [11/150][40/185]	Time 0.431 (0.452)	Data 0.000 (0.014)	Loss_t 0.6393 (0.5780)	Loss_x 2.2524 (2.4290)	Acc 60.94 (47.58)	Lr 0.000116	eta 3:14:58
Epoch: [11/150][60/185]	Time 0.435 (0.450)	Data 0.000 (0.010)	Loss_t 0.6258 (0.5774)	Loss_x 2.3545 (2.3992)	Acc 67.19 (50.36)	Lr 0.000116	eta 3:13:49
Epoch: [11/150][80/185]	Time 0.429 (0.446)	Data 0.000 (0.007)	Loss_t 0.5431 (0.5771)	Loss_x 2.0643 (2.3715)	Acc 70.31 (52.73)	Lr 0.000116	eta 3:12:05
Epoch: [11/150][100/185]	Time 0.431 (0.442)	Data 0.000 (0.006)	Loss_t 0.6631 (0.5729)	Loss_x 2.3851 (2.3256)	Acc 60.94 (55.95)	Lr 0.000116	eta 3:10:02
Epoch: [11/150][120/185]	Time 0.437 (0.435)	Data 0.000 (0.005)	Loss_t 0.4586 (0.5681)	Loss_x 1.6794 (2.2746)	Acc 95.31 (58.88)	Lr 0.000116	eta 3:06:56
Epoch: [11/150][140/185]	Time 0.430 (0.434)	Data 0.000 (0.004)	Loss_t 0.6175 (0.5663)	Loss_x 1.8109 (2.2241)	Acc 87.50 (61.80)	Lr 0.000116	eta 3:06:21
Epoch: [11/150][160/185]	Time 0.410 (0.433)	Data 0.000 (0.004)	Loss_t 0.5073 (0.5639)	Loss_x 1.7041 (2.1789)	Acc 87.50 (64.34)	Lr 0.000116	eta 3:05:34
Epoch: [11/150][180/185]	Time 0.413 (0.431)	Data 0.000 (0.003)	Loss_t 0.6194 (0.5615)	Loss_x 1.6331 (2.1261)	Acc 93.75 (67.02)	Lr 0.000116	eta 3:04:39
Epoch: [12/150][20/185]	Time 0.414 (0.475)	Data 0.000 (0.027)	Loss_t 0.5928 (0.5534)	Loss_x 2.5253 (2.3495)	Acc 46.88 (48.36)	Lr 0.000124	eta 3:23:21
Epoch: [12/150][40/185]	Time 0.427 (0.454)	Data 0.000 (0.014)	Loss_t 0.5524 (0.5601)	Loss_x 2.2073 (2.2948)	Acc 57.81 (52.73)	Lr 0.000124	eta 3:14:21
Epoch: [12/150][60/185]	Time 0.396 (0.443)	Data 0.000 (0.009)	Loss_t 0.5246 (0.5547)	Loss_x 1.9157 (2.2357)	Acc 78.12 (57.24)	Lr 0.000124	eta 3:09:35
Epoch: [12/150][80/185]	Time 0.438 (0.437)	Data 0.000 (0.007)	Loss_t 0.5366 (0.5507)	Loss_x 1.9194 (2.1838)	Acc 78.12 (60.31)	Lr 0.000124	eta 3:06:52
Epoch: [12/150][100/185]	Time 0.365 (0.435)	Data 0.000 (0.006)	Loss_t 0.4882 (0.5483)	Loss_x 1.6875 (2.1311)	Acc 87.50 (63.55)	Lr 0.000124	eta 3:05:33
Epoch: [12/150][120/185]	Time 0.427 (0.434)	Data 0.000 (0.005)	Loss_t 0.5282 (0.5457)	Loss_x 1.8776 (2.0925)	Acc 78.12 (65.96)	Lr 0.000124	eta 3:04:55
Epoch: [12/150][140/185]	Time 0.432 (0.432)	Data 0.000 (0.004)	Loss_t 0.5115 (0.5452)	Loss_x 1.7177 (2.0529)	Acc 78.12 (68.44)	Lr 0.000124	eta 3:04:13
Epoch: [12/150][160/185]	Time 0.427 (0.432)	Data 0.000 (0.004)	Loss_t 0.5271 (0.5399)	Loss_x 1.5489 (2.0039)	Acc 93.75 (70.81)	Lr 0.000124	eta 3:03:56
Epoch: [12/150][180/185]	Time 0.433 (0.432)	Data 0.000 (0.003)	Loss_t 0.4577 (0.5370)	Loss_x 1.4609 (1.9557)	Acc 93.75 (73.35)	Lr 0.000124	eta 3:03:44
Epoch: [13/150][20/185]	Time 0.401 (0.465)	Data 0.000 (0.025)	Loss_t 0.5497 (0.5228)	Loss_x 2.2728 (2.1055)	Acc 46.88 (59.45)	Lr 0.000132	eta 3:17:35
Epoch: [13/150][40/185]	Time 0.423 (0.446)	Data 0.000 (0.012)	Loss_t 0.5621 (0.5272)	Loss_x 1.9933 (2.0936)	Acc 59.38 (61.33)	Lr 0.000132	eta 3:09:31
Epoch: [13/150][60/185]	Time 0.454 (0.443)	Data 0.000 (0.008)	Loss_t 0.5430 (0.5261)	Loss_x 1.7512 (2.0284)	Acc 85.94 (66.15)	Lr 0.000132	eta 3:08:04
Epoch: [13/150][80/185]	Time 0.470 (0.439)	Data 0.000 (0.006)	Loss_t 0.4920 (0.5246)	Loss_x 1.8923 (2.0017)	Acc 76.56 (67.99)	Lr 0.000132	eta 3:06:07
Epoch: [13/150][100/185]	Time 0.429 (0.436)	Data 0.000 (0.005)	Loss_t 0.5858 (0.5258)	Loss_x 1.8527 (1.9733)	Acc 71.88 (69.84)	Lr 0.000132	eta 3:04:59
Epoch: [13/150][120/185]	Time 0.421 (0.433)	Data 0.000 (0.004)	Loss_t 0.5546 (0.5253)	Loss_x 1.7833 (1.9456)	Acc 79.69 (71.58)	Lr 0.000132	eta 3:03:18
Epoch: [13/150][140/185]	Time 0.431 (0.433)	Data 0.000 (0.004)	Loss_t 0.5727 (0.5248)	Loss_x 1.6689 (1.9065)	Acc 89.06 (73.74)	Lr 0.000132	eta 3:03:09
Epoch: [13/150][160/185]	Time 0.435 (0.432)	Data 0.000 (0.003)	Loss_t 0.4702 (0.5206)	Loss_x 1.4686 (1.8627)	Acc 96.88 (75.96)	Lr 0.000132	eta 3:02:44
Epoch: [13/150][180/185]	Time 0.420 (0.432)	Data 0.000 (0.003)	Loss_t 0.4656 (0.5165)	Loss_x 1.3112 (1.8153)	Acc 100.00 (78.11)	Lr 0.000132	eta 3:02:25
Epoch: [14/150][20/185]	Time 0.455 (0.469)	Data 0.000 (0.025)	Loss_t 0.4758 (0.5122)	Loss_x 2.0065 (2.0057)	Acc 65.62 (63.67)	Lr 0.000140	eta 3:17:46
Epoch: [14/150][40/185]	Time 0.427 (0.455)	Data 0.000 (0.013)	Loss_t 0.5746 (0.4994)	Loss_x 1.8652 (1.9226)	Acc 67.19 (67.93)	Lr 0.000140	eta 3:11:43
Epoch: [14/150][60/185]	Time 0.436 (0.449)	Data 0.000 (0.009)	Loss_t 0.4584 (0.4991)	Loss_x 1.7154 (1.8799)	Acc 82.81 (70.86)	Lr 0.000140	eta 3:09:24
Epoch: [14/150][80/185]	Time 0.456 (0.446)	Data 0.000 (0.006)	Loss_t 0.4362 (0.4994)	Loss_x 1.7317 (1.8555)	Acc 73.44 (72.58)	Lr 0.000140	eta 3:07:58
Epoch: [14/150][100/185]	Time 0.442 (0.447)	Data 0.000 (0.005)	Loss_t 0.4809 (0.5012)	Loss_x 1.5190 (1.8181)	Acc 90.62 (75.11)	Lr 0.000140	eta 3:08:03
Epoch: [14/150][120/185]	Time 0.408 (0.444)	Data 0.000 (0.004)	Loss_t 0.4617 (0.4976)	Loss_x 1.6092 (1.7863)	Acc 95.31 (77.23)	Lr 0.000140	eta 3:06:34
Epoch: [14/150][140/185]	Time 0.386 (0.441)	Data 0.000 (0.004)	Loss_t 0.4418 (0.4955)	Loss_x 1.4575 (1.7522)	Acc 95.31 (79.20)	Lr 0.000140	eta 3:05:03
Epoch: [14/150][160/185]	Time 0.388 (0.434)	Data 0.000 (0.003)	Loss_t 0.4969 (0.4941)	Loss_x 1.4456 (1.7198)	Acc 98.44 (80.93)	Lr 0.000140	eta 3:02:11
Epoch: [14/150][180/185]	Time 0.428 (0.432)	Data 0.000 (0.003)	Loss_t 0.4117 (0.4909)	Loss_x 1.3465 (1.6849)	Acc 96.88 (82.47)	Lr 0.000140	eta 3:01:11
Epoch: [15/150][20/185]	Time 0.440 (0.464)	Data 0.000 (0.025)	Loss_t 0.5112 (0.4993)	Loss_x 2.0981 (1.8640)	Acc 62.50 (71.41)	Lr 0.000148	eta 3:14:20
Epoch: [15/150][40/185]	Time 0.390 (0.441)	Data 0.000 (0.012)	Loss_t 0.5442 (0.4926)	Loss_x 1.9247 (1.8111)	Acc 70.31 (73.40)	Lr 0.000148	eta 3:04:42
Epoch: [15/150][60/185]	Time 0.437 (0.440)	Data 0.000 (0.008)	Loss_t 0.3949 (0.4965)	Loss_x 1.5394 (1.7678)	Acc 90.62 (76.51)	Lr 0.000148	eta 3:03:51
Epoch: [15/150][80/185]	Time 0.436 (0.439)	Data 0.000 (0.006)	Loss_t 0.5821 (0.4927)	Loss_x 1.7884 (1.7287)	Acc 75.00 (78.67)	Lr 0.000148	eta 3:03:25
Epoch: [15/150][100/185]	Time 0.437 (0.439)	Data 0.000 (0.005)	Loss_t 0.4878 (0.4912)	Loss_x 1.7521 (1.7057)	Acc 78.12 (80.11)	Lr 0.000148	eta 3:03:12
Epoch: [15/150][120/185]	Time 0.434 (0.437)	Data 0.000 (0.004)	Loss_t 0.4676 (0.4877)	Loss_x 1.5936 (1.6787)	Acc 87.50 (81.59)	Lr 0.000148	eta 3:02:13
Epoch: [15/150][140/185]	Time 0.414 (0.435)	Data 0.000 (0.004)	Loss_t 0.4498 (0.4858)	Loss_x 1.4531 (1.6572)	Acc 89.06 (82.59)	Lr 0.000148	eta 3:01:20
Epoch: [15/150][160/185]	Time 0.422 (0.432)	Data 0.000 (0.003)	Loss_t 0.4677 (0.4861)	Loss_x 1.4526 (1.6321)	Acc 90.62 (83.96)	Lr 0.000148	eta 2:59:53
Epoch: [15/150][180/185]	Time 0.432 (0.430)	Data 0.000 (0.003)	Loss_t 0.4034 (0.4824)	Loss_x 1.2289 (1.6010)	Acc 98.44 (85.29)	Lr 0.000148	eta 2:58:57
Epoch: [16/150][20/185]	Time 0.428 (0.446)	Data 0.000 (0.025)	Loss_t 0.4414 (0.4606)	Loss_x 1.6762 (1.7106)	Acc 75.00 (75.31)	Lr 0.000156	eta 3:05:24
Epoch: [16/150][40/185]	Time 0.401 (0.431)	Data 0.000 (0.013)	Loss_t 0.5197 (0.4668)	Loss_x 1.7024 (1.6611)	Acc 82.81 (79.22)	Lr 0.000156	eta 2:58:56
Epoch: [16/150][60/185]	Time 0.404 (0.416)	Data 0.000 (0.009)	Loss_t 0.4558 (0.4641)	Loss_x 1.4611 (1.6346)	Acc 93.75 (80.86)	Lr 0.000156	eta 2:52:43
Epoch: [16/150][80/185]	Time 0.427 (0.410)	Data 0.000 (0.006)	Loss_t 0.4312 (0.4638)	Loss_x 1.4651 (1.6114)	Acc 96.88 (82.64)	Lr 0.000156	eta 2:49:58
Epoch: [16/150][100/185]	Time 0.413 (0.411)	Data 0.000 (0.005)	Loss_t 0.5208 (0.4633)	Loss_x 1.5608 (1.6013)	Acc 90.62 (83.34)	Lr 0.000156	eta 2:50:28
Epoch: [16/150][120/185]	Time 0.465 (0.412)	Data 0.000 (0.004)	Loss_t 0.5691 (0.4620)	Loss_x 1.5831 (1.5807)	Acc 89.06 (84.66)	Lr 0.000156	eta 2:50:31
Epoch: [16/150][140/185]	Time 0.423 (0.412)	Data 0.000 (0.004)	Loss_t 0.3991 (0.4638)	Loss_x 1.3736 (1.5616)	Acc 93.75 (85.85)	Lr 0.000156	eta 2:50:28
Epoch: [16/150][160/185]	Time 0.461 (0.412)	Data 0.000 (0.003)	Loss_t 0.4717 (0.4611)	Loss_x 1.3778 (1.5416)	Acc 96.88 (86.93)	Lr 0.000156	eta 2:50:18
Epoch: [16/150][180/185]	Time 0.442 (0.416)	Data 0.000 (0.003)	Loss_t 0.3999 (0.4578)	Loss_x 1.2949 (1.5152)	Acc 98.44 (88.08)	Lr 0.000156	eta 2:52:01
Epoch: [17/150][20/185]	Time 0.433 (0.449)	Data 0.000 (0.025)	Loss_t 0.4368 (0.4669)	Loss_x 1.5462 (1.6078)	Acc 92.19 (82.11)	Lr 0.000164	eta 3:05:14
Epoch: [17/150][40/185]	Time 0.400 (0.436)	Data 0.000 (0.012)	Loss_t 0.4685 (0.4589)	Loss_x 1.6071 (1.5943)	Acc 82.81 (83.12)	Lr 0.000164	eta 2:59:51
Epoch: [17/150][60/185]	Time 0.454 (0.438)	Data 0.000 (0.008)	Loss_t 0.4182 (0.4554)	Loss_x 1.4908 (1.5697)	Acc 84.38 (84.30)	Lr 0.000164	eta 3:00:20
Epoch: [17/150][80/185]	Time 0.398 (0.436)	Data 0.000 (0.006)	Loss_t 0.4393 (0.4551)	Loss_x 1.3869 (1.5523)	Acc 96.88 (85.59)	Lr 0.000164	eta 2:59:31
Epoch: [17/150][100/185]	Time 0.439 (0.433)	Data 0.000 (0.005)	Loss_t 0.4044 (0.4529)	Loss_x 1.3107 (1.5321)	Acc 100.00 (86.78)	Lr 0.000164	eta 2:58:18
Epoch: [17/150][120/185]	Time 0.358 (0.433)	Data 0.000 (0.004)	Loss_t 0.4325 (0.4522)	Loss_x 1.5038 (1.5147)	Acc 82.81 (87.93)	Lr 0.000164	eta 2:58:02
Epoch: [17/150][140/185]	Time 0.435 (0.431)	Data 0.000 (0.004)	Loss_t 0.4192 (0.4510)	Loss_x 1.2668 (1.4989)	Acc 98.44 (88.69)	Lr 0.000164	eta 2:57:05
Epoch: [17/150][160/185]	Time 0.440 (0.429)	Data 0.000 (0.003)	Loss_t 0.4465 (0.4494)	Loss_x 1.3534 (1.4780)	Acc 93.75 (89.61)	Lr 0.000164	eta 2:55:56
Epoch: [17/150][180/185]	Time 0.425 (0.428)	Data 0.000 (0.003)	Loss_t 0.4177 (0.4476)	Loss_x 1.2628 (1.4566)	Acc 98.44 (90.51)	Lr 0.000164	eta 2:55:25
Epoch: [18/150][20/185]	Time 0.444 (0.470)	Data 0.000 (0.026)	Loss_t 0.4977 (0.4534)	Loss_x 1.5959 (1.5055)	Acc 89.06 (88.12)	Lr 0.000172	eta 3:12:35
Epoch: [18/150][40/185]	Time 0.438 (0.461)	Data 0.000 (0.013)	Loss_t 0.4345 (0.4436)	Loss_x 1.3736 (1.5141)	Acc 93.75 (86.88)	Lr 0.000172	eta 3:08:45
Epoch: [18/150][60/185]	Time 0.439 (0.457)	Data 0.000 (0.009)	Loss_t 0.4015 (0.4406)	Loss_x 1.4052 (1.4912)	Acc 96.88 (88.39)	Lr 0.000172	eta 3:06:46
Epoch: [18/150][80/185]	Time 0.444 (0.452)	Data 0.000 (0.007)	Loss_t 0.4010 (0.4412)	Loss_x 1.3639 (1.4744)	Acc 95.31 (89.45)	Lr 0.000172	eta 3:04:45
Epoch: [18/150][100/185]	Time 0.461 (0.449)	Data 0.000 (0.005)	Loss_t 0.3677 (0.4399)	Loss_x 1.2902 (1.4586)	Acc 100.00 (90.28)	Lr 0.000172	eta 3:03:22
Epoch: [18/150][120/185]	Time 0.424 (0.447)	Data 0.000 (0.005)	Loss_t 0.4330 (0.4398)	Loss_x 1.4369 (1.4486)	Acc 87.50 (90.81)	Lr 0.000172	eta 3:02:27
Epoch: [18/150][140/185]	Time 0.431 (0.444)	Data 0.000 (0.004)	Loss_t 0.5012 (0.4419)	Loss_x 1.3708 (1.4367)	Acc 96.88 (91.37)	Lr 0.000172	eta 3:01:11
Epoch: [18/150][160/185]	Time 0.388 (0.440)	Data 0.000 (0.003)	Loss_t 0.3947 (0.4417)	Loss_x 1.3179 (1.4256)	Acc 95.31 (91.91)	Lr 0.000172	eta 2:59:27
Epoch: [18/150][180/185]	Time 0.405 (0.436)	Data 0.000 (0.003)	Loss_t 0.3871 (0.4401)	Loss_x 1.2316 (1.4103)	Acc 100.00 (92.56)	Lr 0.000172	eta 2:57:20
Epoch: [19/150][20/185]	Time 0.431 (0.461)	Data 0.000 (0.024)	Loss_t 0.4407 (0.4449)	Loss_x 1.6334 (1.4893)	Acc 85.94 (87.81)	Lr 0.000180	eta 3:07:16
Epoch: [19/150][40/185]	Time 0.379 (0.450)	Data 0.000 (0.012)	Loss_t 0.3945 (0.4340)	Loss_x 1.2726 (1.4557)	Acc 100.00 (89.10)	Lr 0.000180	eta 3:02:39
Epoch: [19/150][60/185]	Time 0.434 (0.447)	Data 0.000 (0.008)	Loss_t 0.4168 (0.4335)	Loss_x 1.3371 (1.4542)	Acc 95.31 (89.71)	Lr 0.000180	eta 3:01:21
Epoch: [19/150][80/185]	Time 0.422 (0.441)	Data 0.000 (0.006)	Loss_t 0.4302 (0.4283)	Loss_x 1.4891 (1.4348)	Acc 84.38 (90.94)	Lr 0.000180	eta 2:59:01
Epoch: [19/150][100/185]	Time 0.416 (0.437)	Data 0.000 (0.005)	Loss_t 0.4237 (0.4282)	Loss_x 1.4059 (1.4253)	Acc 98.44 (91.77)	Lr 0.000180	eta 2:56:56
Epoch: [19/150][120/185]	Time 0.421 (0.435)	Data 0.000 (0.004)	Loss_t 0.4343 (0.4305)	Loss_x 1.2537 (1.4179)	Acc 98.44 (92.14)	Lr 0.000180	eta 2:56:08
Epoch: [19/150][140/185]	Time 0.424 (0.432)	Data 0.000 (0.004)	Loss_t 0.4571 (0.4306)	Loss_x 1.4428 (1.4097)	Acc 95.31 (92.54)	Lr 0.000180	eta 2:54:56
Epoch: [19/150][160/185]	Time 0.436 (0.430)	Data 0.000 (0.003)	Loss_t 0.4515 (0.4312)	Loss_x 1.3428 (1.3967)	Acc 92.19 (93.05)	Lr 0.000180	eta 2:53:54
Epoch: [19/150][180/185]	Time 0.424 (0.429)	Data 0.000 (0.003)	Loss_t 0.3954 (0.4297)	Loss_x 1.2358 (1.3807)	Acc 100.00 (93.64)	Lr 0.000180	eta 2:53:16
Epoch: [20/150][20/185]	Time 0.402 (0.439)	Data 0.000 (0.025)	Loss_t 0.4637 (0.4406)	Loss_x 1.5531 (1.4515)	Acc 89.06 (91.09)	Lr 0.000188	eta 2:57:06
Epoch: [20/150][40/185]	Time 0.423 (0.433)	Data 0.000 (0.012)	Loss_t 0.4243 (0.4363)	Loss_x 1.4052 (1.4282)	Acc 92.19 (92.27)	Lr 0.000188	eta 2:54:25
Epoch: [20/150][60/185]	Time 0.421 (0.429)	Data 0.000 (0.008)	Loss_t 0.3928 (0.4292)	Loss_x 1.4035 (1.4132)	Acc 98.44 (92.60)	Lr 0.000188	eta 2:52:49
Epoch: [20/150][80/185]	Time 0.417 (0.430)	Data 0.000 (0.006)	Loss_t 0.4247 (0.4288)	Loss_x 1.3517 (1.4041)	Acc 95.31 (92.73)	Lr 0.000188	eta 2:53:14
Epoch: [20/150][100/185]	Time 0.380 (0.428)	Data 0.000 (0.005)	Loss_t 0.3892 (0.4297)	Loss_x 1.3059 (1.3994)	Acc 95.31 (92.73)	Lr 0.000188	eta 2:52:12
Epoch: [20/150][120/185]	Time 0.436 (0.428)	Data 0.000 (0.004)	Loss_t 0.4620 (0.4291)	Loss_x 1.2745 (1.3876)	Acc 96.88 (93.26)	Lr 0.000188	eta 2:52:10
Epoch: [20/150][140/185]	Time 0.392 (0.427)	Data 0.000 (0.004)	Loss_t 0.3752 (0.4286)	Loss_x 1.2757 (1.3761)	Acc 98.44 (93.77)	Lr 0.000188	eta 2:51:17
Epoch: [20/150][160/185]	Time 0.400 (0.426)	Data 0.000 (0.003)	Loss_t 0.4631 (0.4265)	Loss_x 1.2785 (1.3640)	Acc 98.44 (94.22)	Lr 0.000188	eta 2:50:50
Epoch: [20/150][180/185]	Time 0.418 (0.425)	Data 0.000 (0.003)	Loss_t 0.3749 (0.4254)	Loss_x 1.1906 (1.3519)	Acc 100.00 (94.56)	Lr 0.000188	eta 2:50:11
##### Evaluating market1501 (source) #####
Extracting features from query set ...
Done, obtained 3368-by-4608 matrix
Extracting features from gallery set ...
Done, obtained 15913-by-4608 matrix
Speed: 0.0355 sec/batch
Computing distance matrix with metric=euclidean ...
Computing CMC and mAP ...
** Results **
mAP: 72.5%
CMC curve
Rank-1  : 86.4%
Rank-5  : 95.7%
Rank-10 : 97.4%
Rank-20 : 98.3%
Checkpoint saved to "log/model.pth.tar-20"
Epoch: [21/150][20/185]	Time 0.435 (0.460)	Data 0.000 (0.030)	Loss_t 0.4101 (0.4033)	Loss_x 1.3033 (1.4009)	Acc 98.44 (92.11)	Lr 0.000197	eta 3:04:16
Epoch: [21/150][40/185]	Time 0.381 (0.437)	Data 0.000 (0.015)	Loss_t 0.3955 (0.4091)	Loss_x 1.3762 (1.3872)	Acc 90.62 (92.97)	Lr 0.000197	eta 2:54:41
Epoch: [21/150][60/185]	Time 0.416 (0.423)	Data 0.000 (0.010)	Loss_t 0.4849 (0.4132)	Loss_x 1.4400 (1.3783)	Acc 95.31 (93.62)	Lr 0.000197	eta 2:48:59
Epoch: [21/150][80/185]	Time 0.431 (0.423)	Data 0.000 (0.008)	Loss_t 0.4349 (0.4153)	Loss_x 1.3857 (1.3646)	Acc 90.62 (94.36)	Lr 0.000197	eta 2:49:07
Epoch: [21/150][100/185]	Time 0.425 (0.423)	Data 0.000 (0.006)	Loss_t 0.4386 (0.4152)	Loss_x 1.3570 (1.3555)	Acc 98.44 (94.78)	Lr 0.000197	eta 2:48:46
Epoch: [21/150][120/185]	Time 0.341 (0.410)	Data 0.000 (0.005)	Loss_t 0.4285 (0.4137)	Loss_x 1.2989 (1.3466)	Acc 98.44 (95.18)	Lr 0.000197	eta 2:43:30
Epoch: [21/150][140/185]	Time 0.436 (0.409)	Data 0.000 (0.004)	Loss_t 0.4034 (0.4137)	Loss_x 1.2351 (1.3385)	Acc 100.00 (95.50)	Lr 0.000197	eta 2:43:07
Epoch: [21/150][160/185]	Time 0.403 (0.411)	Data 0.000 (0.004)	Loss_t 0.3768 (0.4117)	Loss_x 1.2917 (1.3284)	Acc 93.75 (95.75)	Lr 0.000197	eta 2:43:29
Epoch: [21/150][180/185]	Time 0.436 (0.410)	Data 0.000 (0.003)	Loss_t 0.3853 (0.4119)	Loss_x 1.1982 (1.3192)	Acc 98.44 (95.89)	Lr 0.000197	eta 2:43:17
Epoch: [22/150][20/185]	Time 0.437 (0.444)	Data 0.000 (0.024)	Loss_t 0.4624 (0.4255)	Loss_x 1.4588 (1.3821)	Acc 89.06 (92.81)	Lr 0.000205	eta 2:56:32
Epoch: [22/150][40/185]	Time 0.434 (0.441)	Data 0.000 (0.012)	Loss_t 0.3737 (0.4147)	Loss_x 1.3089 (1.3546)	Acc 98.44 (94.26)	Lr 0.000205	eta 2:55:14
Epoch: [22/150][60/185]	Time 0.384 (0.435)	Data 0.000 (0.008)	Loss_t 0.3996 (0.4130)	Loss_x 1.2714 (1.3500)	Acc 98.44 (94.87)	Lr 0.000205	eta 2:52:31
Epoch: [22/150][80/185]	Time 0.423 (0.429)	Data 0.000 (0.006)	Loss_t 0.4438 (0.4140)	Loss_x 1.4464 (1.3499)	Acc 93.75 (94.96)	Lr 0.000205	eta 2:50:02
Epoch: [22/150][100/185]	Time 0.385 (0.425)	Data 0.000 (0.005)	Loss_t 0.4001 (0.4174)	Loss_x 1.2957 (1.3445)	Acc 96.88 (95.28)	Lr 0.000205	eta 2:48:25
Epoch: [22/150][120/185]	Time 0.405 (0.425)	Data 0.000 (0.004)	Loss_t 0.3907 (0.4164)	Loss_x 1.2831 (1.3362)	Acc 100.00 (95.43)	Lr 0.000205	eta 2:48:05
Epoch: [22/150][140/185]	Time 0.413 (0.423)	Data 0.000 (0.004)	Loss_t 0.4188 (0.4166)	Loss_x 1.2928 (1.3303)	Acc 95.31 (95.62)	Lr 0.000205	eta 2:47:12
Epoch: [22/150][160/185]	Time 0.450 (0.421)	Data 0.000 (0.003)	Loss_t 0.4214 (0.4147)	Loss_x 1.3222 (1.3225)	Acc 98.44 (95.92)	Lr 0.000205	eta 2:46:28
Epoch: [22/150][180/185]	Time 0.402 (0.422)	Data 0.000 (0.003)	Loss_t 0.3867 (0.4121)	Loss_x 1.2132 (1.3121)	Acc 98.44 (96.21)	Lr 0.000205	eta 2:46:30
Epoch: [23/150][20/185]	Time 0.436 (0.463)	Data 0.000 (0.024)	Loss_t 0.3769 (0.4028)	Loss_x 1.2614 (1.3215)	Acc 98.44 (96.02)	Lr 0.000213	eta 3:02:30
Epoch: [23/150][40/185]	Time 0.435 (0.431)	Data 0.000 (0.012)	Loss_t 0.3578 (0.4075)	Loss_x 1.3036 (1.3292)	Acc 95.31 (96.02)	Lr 0.000213	eta 2:49:59
Epoch: [23/150][60/185]	Time 0.389 (0.423)	Data 0.000 (0.008)	Loss_t 0.4186 (0.4080)	Loss_x 1.3038 (1.3272)	Acc 95.31 (95.44)	Lr 0.000213	eta 2:46:34
Epoch: [23/150][80/185]	Time 0.400 (0.415)	Data 0.000 (0.006)	Loss_t 0.4547 (0.4081)	Loss_x 1.3783 (1.3303)	Acc 93.75 (95.45)	Lr 0.000213	eta 2:43:23
Epoch: [23/150][100/185]	Time 0.413 (0.413)	Data 0.000 (0.005)	Loss_t 0.4051 (0.4092)	Loss_x 1.2555 (1.3277)	Acc 100.00 (95.77)	Lr 0.000213	eta 2:42:19
Epoch: [23/150][120/185]	Time 0.418 (0.413)	Data 0.000 (0.004)	Loss_t 0.4001 (0.4076)	Loss_x 1.2439 (1.3207)	Acc 96.88 (95.99)	Lr 0.000213	eta 2:42:01
Epoch: [23/150][140/185]	Time 0.408 (0.411)	Data 0.000 (0.004)	Loss_t 0.3832 (0.4078)	Loss_x 1.2118 (1.3134)	Acc 100.00 (96.21)	Lr 0.000213	eta 2:41:18
Epoch: [23/150][160/185]	Time 0.381 (0.410)	Data 0.000 (0.003)	Loss_t 0.3541 (0.4076)	Loss_x 1.2161 (1.3071)	Acc 93.75 (96.38)	Lr 0.000213	eta 2:40:42
Epoch: [23/150][180/185]	Time 0.398 (0.410)	Data 0.000 (0.003)	Loss_t 0.3479 (0.4049)	Loss_x 1.1558 (1.2970)	Acc 100.00 (96.61)	Lr 0.000213	eta 2:40:29
Epoch: [24/150][20/185]	Time 0.443 (0.473)	Data 0.000 (0.024)	Loss_t 0.3818 (0.4054)	Loss_x 1.2944 (1.3152)	Acc 95.31 (95.86)	Lr 0.000221	eta 3:05:04
Epoch: [24/150][40/185]	Time 0.435 (0.458)	Data 0.000 (0.012)	Loss_t 0.3622 (0.3983)	Loss_x 1.2906 (1.3117)	Acc 98.44 (96.33)	Lr 0.000221	eta 2:59:05
Epoch: [24/150][60/185]	Time 0.471 (0.456)	Data 0.000 (0.008)	Loss_t 0.3771 (0.4004)	Loss_x 1.2383 (1.3216)	Acc 100.00 (95.94)	Lr 0.000221	eta 2:58:01
Epoch: [24/150][80/185]	Time 0.453 (0.455)	Data 0.000 (0.006)	Loss_t 0.4805 (0.4002)	Loss_x 1.3465 (1.3191)	Acc 98.44 (96.11)	Lr 0.000221	eta 2:57:35
Epoch: [24/150][100/185]	Time 0.464 (0.455)	Data 0.000 (0.005)	Loss_t 0.4075 (0.3996)	Loss_x 1.3154 (1.3119)	Acc 95.31 (96.45)	Lr 0.000221	eta 2:57:13
Epoch: [24/150][120/185]	Time 0.422 (0.454)	Data 0.000 (0.004)	Loss_t 0.3948 (0.3995)	Loss_x 1.2856 (1.3076)	Acc 95.31 (96.51)	Lr 0.000221	eta 2:56:51
Epoch: [24/150][140/185]	Time 0.440 (0.449)	Data 0.000 (0.004)	Loss_t 0.5041 (0.3994)	Loss_x 1.3694 (1.3027)	Acc 95.31 (96.72)	Lr 0.000221	eta 2:54:50
Epoch: [24/150][160/185]	Time 0.433 (0.448)	Data 0.000 (0.003)	Loss_t 0.4056 (0.3988)	Loss_x 1.2724 (1.2958)	Acc 95.31 (96.88)	Lr 0.000221	eta 2:54:17
Epoch: [24/150][180/185]	Time 0.411 (0.445)	Data 0.000 (0.003)	Loss_t 0.4094 (0.3973)	Loss_x 1.2043 (1.2877)	Acc 100.00 (97.00)	Lr 0.000221	eta 2:52:47
Epoch: [25/150][20/185]	Time 0.417 (0.452)	Data 0.000 (0.025)	Loss_t 0.3956 (0.3966)	Loss_x 1.4313 (1.3181)	Acc 89.06 (95.55)	Lr 0.000229	eta 2:55:24
Epoch: [25/150][40/185]	Time 0.426 (0.445)	Data 0.000 (0.013)	Loss_t 0.3782 (0.3992)	Loss_x 1.2755 (1.3187)	Acc 100.00 (95.70)	Lr 0.000229	eta 2:52:26
Epoch: [25/150][60/185]	Time 0.438 (0.448)	Data 0.000 (0.008)	Loss_t 0.4818 (0.3958)	Loss_x 1.3417 (1.3114)	Acc 95.31 (96.22)	Lr 0.000229	eta 2:53:26
Epoch: [25/150][80/185]	Time 0.399 (0.444)	Data 0.000 (0.006)	Loss_t 0.4239 (0.3985)	Loss_x 1.2787 (1.3110)	Acc 95.31 (96.25)	Lr 0.000229	eta 2:52:00
Epoch: [25/150][100/185]	Time 0.386 (0.437)	Data 0.000 (0.005)	Loss_t 0.4621 (0.3982)	Loss_x 1.4026 (1.3077)	Acc 92.19 (96.23)	Lr 0.000229	eta 2:48:59
Epoch: [25/150][120/185]	Time 0.460 (0.434)	Data 0.000 (0.004)	Loss_t 0.4005 (0.3963)	Loss_x 1.2614 (1.3016)	Acc 98.44 (96.45)	Lr 0.000229	eta 2:47:43
Epoch: [25/150][140/185]	Time 0.444 (0.433)	Data 0.000 (0.004)	Loss_t 0.3890 (0.3946)	Loss_x 1.2596 (1.2962)	Acc 98.44 (96.65)	Lr 0.000229	eta 2:47:21
Epoch: [25/150][160/185]	Time 0.464 (0.431)	Data 0.000 (0.003)	Loss_t 0.3889 (0.3933)	Loss_x 1.2314 (1.2889)	Acc 100.00 (96.78)	Lr 0.000229	eta 2:46:14
Epoch: [25/150][180/185]	Time 0.483 (0.434)	Data 0.000 (0.003)	Loss_t 0.3629 (0.3927)	Loss_x 1.1598 (1.2799)	Acc 100.00 (96.98)	Lr 0.000229	eta 2:47:09
Epoch: [26/150][20/185]	Time 0.473 (0.475)	Data 0.000 (0.024)	Loss_t 0.3853 (0.3716)	Loss_x 1.2530 (1.2892)	Acc 98.44 (97.89)	Lr 0.000237	eta 3:03:05
Epoch: [26/150][40/185]	Time 0.463 (0.456)	Data 0.000 (0.012)	Loss_t 0.3320 (0.3799)	Loss_x 1.1909 (1.2873)	Acc 100.00 (97.19)	Lr 0.000237	eta 2:55:18
Epoch: [26/150][60/185]	Time 0.438 (0.446)	Data 0.000 (0.008)	Loss_t 0.3756 (0.3816)	Loss_x 1.2195 (1.2848)	Acc 98.44 (97.03)	Lr 0.000237	eta 2:51:36
Epoch: [26/150][80/185]	Time 0.435 (0.445)	Data 0.000 (0.006)	Loss_t 0.3891 (0.3814)	Loss_x 1.3020 (1.2818)	Acc 96.88 (97.09)	Lr 0.000237	eta 2:50:53
Epoch: [26/150][100/185]	Time 0.408 (0.439)	Data 0.000 (0.005)	Loss_t 0.3670 (0.3821)	Loss_x 1.2524 (1.2840)	Acc 96.88 (96.97)	Lr 0.000237	eta 2:48:31
Epoch: [26/150][120/185]	Time 0.434 (0.435)	Data 0.000 (0.004)	Loss_t 0.3715 (0.3819)	Loss_x 1.2322 (1.2816)	Acc 100.00 (97.11)	Lr 0.000237	eta 2:46:43
Epoch: [26/150][140/185]	Time 0.428 (0.432)	Data 0.000 (0.004)	Loss_t 0.3809 (0.3827)	Loss_x 1.2563 (1.2777)	Acc 98.44 (97.31)	Lr 0.000237	eta 2:45:35
Epoch: [26/150][160/185]	Time 0.412 (0.429)	Data 0.000 (0.003)	Loss_t 0.3780 (0.3829)	Loss_x 1.2973 (1.2723)	Acc 93.75 (97.40)	Lr 0.000237	eta 2:44:20
Epoch: [26/150][180/185]	Time 0.412 (0.427)	Data 0.000 (0.003)	Loss_t 0.3385 (0.3834)	Loss_x 1.1649 (1.2656)	Acc 98.44 (97.50)	Lr 0.000237	eta 2:43:14
Epoch: [27/150][20/185]	Time 0.412 (0.450)	Data 0.000 (0.025)	Loss_t 0.4049 (0.3909)	Loss_x 1.3512 (1.3157)	Acc 92.19 (96.56)	Lr 0.000245	eta 2:51:50
Epoch: [27/150][40/185]	Time 0.427 (0.442)	Data 0.000 (0.013)	Loss_t 0.4034 (0.3859)	Loss_x 1.3273 (1.2950)	Acc 95.31 (97.30)	Lr 0.000245	eta 2:48:34
Epoch: [27/150][60/185]	Time 0.434 (0.443)	Data 0.000 (0.009)	Loss_t 0.4220 (0.3855)	Loss_x 1.2793 (1.3003)	Acc 100.00 (96.93)	Lr 0.000245	eta 2:48:56
Epoch: [27/150][80/185]	Time 0.442 (0.442)	Data 0.000 (0.006)	Loss_t 0.4090 (0.3867)	Loss_x 1.2616 (1.2949)	Acc 98.44 (97.17)	Lr 0.000245	eta 2:48:20
Epoch: [27/150][100/185]	Time 0.428 (0.439)	Data 0.000 (0.005)	Loss_t 0.4553 (0.3880)	Loss_x 1.4091 (1.2939)	Acc 93.75 (97.14)	Lr 0.000245	eta 2:47:02
Epoch: [27/150][120/185]	Time 0.433 (0.434)	Data 0.000 (0.004)	Loss_t 0.3809 (0.3882)	Loss_x 1.2889 (1.2903)	Acc 98.44 (97.16)	Lr 0.000245	eta 2:44:58
Epoch: [27/150][140/185]	Time 0.397 (0.432)	Data 0.000 (0.004)	Loss_t 0.3851 (0.3878)	Loss_x 1.3018 (1.2855)	Acc 98.44 (97.30)	Lr 0.000245	eta 2:44:02
Epoch: [27/150][160/185]	Time 0.422 (0.430)	Data 0.000 (0.003)	Loss_t 0.3511 (0.3880)	Loss_x 1.2144 (1.2782)	Acc 98.44 (97.41)	Lr 0.000245	eta 2:43:04
Epoch: [27/150][180/185]	Time 0.386 (0.428)	Data 0.000 (0.003)	Loss_t 0.4157 (0.3879)	Loss_x 1.1833 (1.2714)	Acc 100.00 (97.51)	Lr 0.000245	eta 2:42:25
Epoch: [28/150][20/185]	Time 0.429 (0.458)	Data 0.000 (0.025)	Loss_t 0.3645 (0.3864)	Loss_x 1.2371 (1.3040)	Acc 100.00 (96.33)	Lr 0.000253	eta 2:53:26
Epoch: [28/150][40/185]	Time 0.472 (0.451)	Data 0.000 (0.013)	Loss_t 0.3355 (0.3851)	Loss_x 1.1966 (1.3020)	Acc 100.00 (96.64)	Lr 0.000253	eta 2:50:47
Epoch: [28/150][60/185]	Time 0.481 (0.448)	Data 0.000 (0.009)	Loss_t 0.4243 (0.3854)	Loss_x 1.3250 (1.2972)	Acc 95.31 (96.74)	Lr 0.000253	eta 2:49:38
Epoch: [28/150][80/185]	Time 0.420 (0.443)	Data 0.000 (0.006)	Loss_t 0.3946 (0.3843)	Loss_x 1.2830 (1.2958)	Acc 98.44 (97.01)	Lr 0.000253	eta 2:47:16
Epoch: [28/150][100/185]	Time 0.450 (0.441)	Data 0.000 (0.005)	Loss_t 0.3505 (0.3828)	Loss_x 1.2391 (1.2929)	Acc 95.31 (97.08)	Lr 0.000253	eta 2:46:36
Epoch: [28/150][120/185]	Time 0.437 (0.438)	Data 0.000 (0.004)	Loss_t 0.3879 (0.3813)	Loss_x 1.2712 (1.2874)	Acc 98.44 (97.04)	Lr 0.000253	eta 2:45:12
Epoch: [28/150][140/185]	Time 0.424 (0.434)	Data 0.000 (0.004)	Loss_t 0.3819 (0.3811)	Loss_x 1.2393 (1.2825)	Acc 100.00 (97.15)	Lr 0.000253	eta 2:43:29
Epoch: [28/150][160/185]	Time 0.428 (0.433)	Data 0.000 (0.003)	Loss_t 0.3811 (0.3794)	Loss_x 1.2344 (1.2766)	Acc 100.00 (97.28)	Lr 0.000253	eta 2:42:52
Epoch: [28/150][180/185]	Time 0.435 (0.432)	Data 0.000 (0.003)	Loss_t 0.3793 (0.3780)	Loss_x 1.1833 (1.2693)	Acc 98.44 (97.40)	Lr 0.000253	eta 2:42:41
Epoch: [29/150][20/185]	Time 0.403 (0.445)	Data 0.000 (0.029)	Loss_t 0.3723 (0.3703)	Loss_x 1.3642 (1.2881)	Acc 96.88 (96.72)	Lr 0.000261	eta 2:47:16
Epoch: [29/150][40/185]	Time 0.369 (0.443)	Data 0.000 (0.015)	Loss_t 0.4136 (0.3825)	Loss_x 1.3106 (1.2919)	Acc 100.00 (97.03)	Lr 0.000261	eta 2:46:15
Epoch: [29/150][60/185]	Time 0.452 (0.422)	Data 0.000 (0.010)	Loss_t 0.3791 (0.3834)	Loss_x 1.2459 (1.2876)	Acc 100.00 (97.34)	Lr 0.000261	eta 2:38:13
Epoch: [29/150][80/185]	Time 0.413 (0.424)	Data 0.000 (0.007)	Loss_t 0.4006 (0.3794)	Loss_x 1.2853 (1.2826)	Acc 98.44 (97.46)	Lr 0.000261	eta 2:38:56
Epoch: [29/150][100/185]	Time 0.429 (0.425)	Data 0.000 (0.006)	Loss_t 0.3566 (0.3773)	Loss_x 1.2566 (1.2800)	Acc 98.44 (97.50)	Lr 0.000261	eta 2:39:02
Epoch: [29/150][120/185]	Time 0.439 (0.426)	Data 0.000 (0.005)	Loss_t 0.4473 (0.3783)	Loss_x 1.2533 (1.2771)	Acc 100.00 (97.53)	Lr 0.000261	eta 2:39:16
Epoch: [29/150][140/185]	Time 0.372 (0.426)	Data 0.000 (0.004)	Loss_t 0.3322 (0.3764)	Loss_x 1.2225 (1.2737)	Acc 96.88 (97.62)	Lr 0.000261	eta 2:39:22
Epoch: [29/150][160/185]	Time 0.415 (0.426)	Data 0.000 (0.004)	Loss_t 0.4191 (0.3746)	Loss_x 1.2601 (1.2668)	Acc 96.88 (97.78)	Lr 0.000261	eta 2:39:14
Epoch: [29/150][180/185]	Time 0.428 (0.427)	Data 0.000 (0.003)	Loss_t 0.4290 (0.3753)	Loss_x 1.1773 (1.2607)	Acc 100.00 (97.89)	Lr 0.000261	eta 2:39:26
Epoch: [30/150][20/185]	Time 0.450 (0.466)	Data 0.000 (0.025)	Loss_t 0.3780 (0.3737)	Loss_x 1.3961 (1.2895)	Acc 95.31 (97.42)	Lr 0.000269	eta 2:53:38
Epoch: [30/150][40/185]	Time 0.445 (0.446)	Data 0.000 (0.012)	Loss_t 0.3559 (0.3758)	Loss_x 1.2659 (1.2944)	Acc 96.88 (97.11)	Lr 0.000269	eta 2:45:58
Epoch: [30/150][60/185]	Time 0.391 (0.440)	Data 0.000 (0.008)	Loss_t 0.4009 (0.3695)	Loss_x 1.2820 (1.2857)	Acc 98.44 (97.32)	Lr 0.000269	eta 2:43:44
Epoch: [30/150][80/185]	Time 0.458 (0.443)	Data 0.000 (0.006)	Loss_t 0.3522 (0.3704)	Loss_x 1.3157 (1.2856)	Acc 92.19 (97.27)	Lr 0.000269	eta 2:44:49
Epoch: [30/150][100/185]	Time 0.462 (0.446)	Data 0.000 (0.005)	Loss_t 0.3743 (0.3699)	Loss_x 1.3265 (1.2856)	Acc 98.44 (97.14)	Lr 0.000269	eta 2:45:29
Epoch: [30/150][120/185]	Time 0.410 (0.443)	Data 0.000 (0.004)	Loss_t 0.3464 (0.3712)	Loss_x 1.3158 (1.2829)	Acc 93.75 (97.27)	Lr 0.000269	eta 2:44:20
Epoch: [30/150][140/185]	Time 0.363 (0.437)	Data 0.000 (0.004)	Loss_t 0.3151 (0.3718)	Loss_x 1.1683 (1.2798)	Acc 100.00 (97.31)	Lr 0.000269	eta 2:42:10
Epoch: [30/150][160/185]	Time 0.470 (0.437)	Data 0.000 (0.003)	Loss_t 0.3791 (0.3708)	Loss_x 1.2834 (1.2750)	Acc 98.44 (97.49)	Lr 0.000269	eta 2:41:41
Epoch: [30/150][180/185]	Time 0.464 (0.437)	Data 0.000 (0.003)	Loss_t 0.3287 (0.3715)	Loss_x 1.2060 (1.2686)	Acc 100.00 (97.59)	Lr 0.000269	eta 2:41:33
##### Evaluating market1501 (source) #####
Extracting features from query set ...
Done, obtained 3368-by-4608 matrix
Extracting features from gallery set ...
Done, obtained 15913-by-4608 matrix
Speed: 0.0361 sec/batch
Computing distance matrix with metric=euclidean ...
Computing CMC and mAP ...
** Results **
mAP: 75.6%
CMC curve
Rank-1  : 88.6%
Rank-5  : 96.5%
Rank-10 : 97.7%
Rank-20 : 98.6%
Checkpoint saved to "log/model.pth.tar-30"
Epoch: [31/150][20/185]	Time 0.460 (0.460)	Data 0.000 (0.027)	Loss_t 0.4357 (0.3614)	Loss_x 1.4101 (1.3041)	Acc 93.75 (97.34)	Lr 0.000277	eta 2:50:00
Epoch: [31/150][40/185]	Time 0.423 (0.444)	Data 0.000 (0.014)	Loss_t 0.4155 (0.3675)	Loss_x 1.3498 (1.3000)	Acc 95.31 (97.07)	Lr 0.000277	eta 2:43:54
Epoch: [31/150][60/185]	Time 0.394 (0.435)	Data 0.000 (0.009)	Loss_t 0.3176 (0.3684)	Loss_x 1.2379 (1.2998)	Acc 100.00 (97.03)	Lr 0.000277	eta 2:40:29
Epoch: [31/150][80/185]	Time 0.420 (0.431)	Data 0.000 (0.007)	Loss_t 0.3594 (0.3681)	Loss_x 1.2171 (1.2913)	Acc 100.00 (97.29)	Lr 0.000277	eta 2:39:02
Epoch: [31/150][100/185]	Time 0.426 (0.428)	Data 0.000 (0.005)	Loss_t 0.3706 (0.3674)	Loss_x 1.2724 (1.2888)	Acc 98.44 (97.33)	Lr 0.000277	eta 2:37:48
Epoch: [31/150][120/185]	Time 0.426 (0.426)	Data 0.000 (0.005)	Loss_t 0.3811 (0.3663)	Loss_x 1.2740 (1.2827)	Acc 98.44 (97.42)	Lr 0.000277	eta 2:36:38
Epoch: [31/150][140/185]	Time 0.432 (0.426)	Data 0.000 (0.004)	Loss_t 0.3037 (0.3633)	Loss_x 1.1439 (1.2736)	Acc 100.00 (97.70)	Lr 0.000277	eta 2:36:35
Epoch: [31/150][160/185]	Time 0.390 (0.427)	Data 0.000 (0.003)	Loss_t 0.3355 (0.3622)	Loss_x 1.2296 (1.2674)	Acc 98.44 (97.86)	Lr 0.000277	eta 2:36:50
Epoch: [31/150][180/185]	Time 0.395 (0.425)	Data 0.000 (0.003)	Loss_t 0.4130 (0.3635)	Loss_x 1.2367 (1.2611)	Acc 98.44 (97.94)	Lr 0.000277	eta 2:35:49
Epoch: [32/150][20/185]	Time 0.403 (0.442)	Data 0.000 (0.022)	Loss_t 0.3573 (0.3726)	Loss_x 1.3127 (1.3051)	Acc 96.88 (97.81)	Lr 0.000285	eta 2:42:03
Epoch: [32/150][40/185]	Time 0.445 (0.431)	Data 0.000 (0.011)	Loss_t 0.3852 (0.3697)	Loss_x 1.3284 (1.2901)	Acc 93.75 (97.46)	Lr 0.000285	eta 2:37:42
Epoch: [32/150][60/185]	Time 0.341 (0.420)	Data 0.000 (0.007)	Loss_t 0.3396 (0.3641)	Loss_x 1.2777 (1.2858)	Acc 98.44 (97.42)	Lr 0.000285	eta 2:33:49
Epoch: [32/150][80/185]	Time 0.351 (0.401)	Data 0.000 (0.006)	Loss_t 0.3504 (0.3613)	Loss_x 1.2201 (1.2820)	Acc 100.00 (97.54)	Lr 0.000285	eta 2:26:30
Epoch: [32/150][100/185]	Time 0.408 (0.394)	Data 0.000 (0.004)	Loss_t 0.3420 (0.3614)	Loss_x 1.2763 (1.2796)	Acc 96.88 (97.59)	Lr 0.000285	eta 2:23:55
Epoch: [32/150][120/185]	Time 0.344 (0.395)	Data 0.000 (0.004)	Loss_t 0.3477 (0.3626)	Loss_x 1.2044 (1.2790)	Acc 98.44 (97.53)	Lr 0.000285	eta 2:24:06
Epoch: [32/150][140/185]	Time 0.426 (0.389)	Data 0.000 (0.003)	Loss_t 0.3688 (0.3625)	Loss_x 1.2598 (1.2754)	Acc 100.00 (97.61)	Lr 0.000285	eta 2:21:53
Epoch: [32/150][160/185]	Time 0.342 (0.385)	Data 0.000 (0.003)	Loss_t 0.3435 (0.3607)	Loss_x 1.1874 (1.2709)	Acc 100.00 (97.69)	Lr 0.000285	eta 2:20:15
Epoch: [32/150][180/185]	Time 0.417 (0.385)	Data 0.000 (0.003)	Loss_t 0.3464 (0.3602)	Loss_x 1.1889 (1.2638)	Acc 100.00 (97.78)	Lr 0.000285	eta 2:19:59
Epoch: [33/150][20/185]	Time 0.430 (0.478)	Data 0.000 (0.025)	Loss_t 0.3422 (0.3661)	Loss_x 1.2194 (1.3056)	Acc 100.00 (97.19)	Lr 0.000293	eta 2:53:47
Epoch: [33/150][40/185]	Time 0.436 (0.447)	Data 0.000 (0.012)	Loss_t 0.3915 (0.3568)	Loss_x 1.2767 (1.2926)	Acc 98.44 (97.73)	Lr 0.000293	eta 2:42:18
Epoch: [33/150][60/185]	Time 0.347 (0.431)	Data 0.000 (0.008)	Loss_t 0.3645 (0.3554)	Loss_x 1.2479 (1.2779)	Acc 98.44 (97.97)	Lr 0.000293	eta 2:36:19
Epoch: [33/150][80/185]	Time 0.400 (0.422)	Data 0.000 (0.006)	Loss_t 0.3126 (0.3521)	Loss_x 1.2937 (1.2772)	Acc 95.31 (97.83)	Lr 0.000293	eta 2:32:52
Epoch: [33/150][100/185]	Time 0.485 (0.420)	Data 0.000 (0.005)	Loss_t 0.3684 (0.3517)	Loss_x 1.3150 (1.2709)	Acc 96.88 (97.94)	Lr 0.000293	eta 2:31:59
Epoch: [33/150][120/185]	Time 0.429 (0.421)	Data 0.000 (0.004)	Loss_t 0.3691 (0.3497)	Loss_x 1.2606 (1.2660)	Acc 100.00 (98.07)	Lr 0.000293	eta 2:32:22
Epoch: [33/150][140/185]	Time 0.405 (0.421)	Data 0.000 (0.004)	Loss_t 0.3282 (0.3485)	Loss_x 1.3021 (1.2626)	Acc 93.75 (98.05)	Lr 0.000293	eta 2:32:21
Epoch: [33/150][160/185]	Time 0.345 (0.413)	Data 0.000 (0.003)	Loss_t 0.3663 (0.3471)	Loss_x 1.2442 (1.2569)	Acc 98.44 (98.12)	Lr 0.000293	eta 2:29:03
Epoch: [33/150][180/185]	Time 0.445 (0.414)	Data 0.000 (0.003)	Loss_t 0.3155 (0.3462)	Loss_x 1.1577 (1.2506)	Acc 100.00 (98.13)	Lr 0.000293	eta 2:29:24
Epoch: [34/150][20/185]	Time 0.428 (0.440)	Data 0.000 (0.024)	Loss_t 0.2956 (0.3403)	Loss_x 1.2221 (1.2940)	Acc 100.00 (98.28)	Lr 0.000302	eta 2:38:33
Epoch: [34/150][40/185]	Time 0.412 (0.435)	Data 0.000 (0.012)	Loss_t 0.3378 (0.3544)	Loss_x 1.3167 (1.3022)	Acc 93.75 (97.23)	Lr 0.000302	eta 2:36:30
Epoch: [34/150][60/185]	Time 0.415 (0.427)	Data 0.000 (0.008)	Loss_t 0.3286 (0.3512)	Loss_x 1.3969 (1.3018)	Acc 93.75 (97.16)	Lr 0.000302	eta 2:33:29
Epoch: [34/150][80/185]	Time 0.437 (0.425)	Data 0.000 (0.006)	Loss_t 0.3396 (0.3461)	Loss_x 1.2194 (1.2929)	Acc 100.00 (97.42)	Lr 0.000302	eta 2:32:54
Epoch: [34/150][100/185]	Time 0.429 (0.426)	Data 0.000 (0.005)	Loss_t 0.3464 (0.3469)	Loss_x 1.2806 (1.2882)	Acc 96.88 (97.45)	Lr 0.000302	eta 2:32:53
Epoch: [34/150][120/185]	Time 0.408 (0.429)	Data 0.000 (0.004)	Loss_t 0.3583 (0.3470)	Loss_x 1.2067 (1.2817)	Acc 100.00 (97.57)	Lr 0.000302	eta 2:33:52
Epoch: [34/150][140/185]	Time 0.438 (0.426)	Data 0.000 (0.004)	Loss_t 0.3648 (0.3465)	Loss_x 1.2125 (1.2772)	Acc 98.44 (97.60)	Lr 0.000302	eta 2:32:51
Epoch: [34/150][160/185]	Time 0.487 (0.430)	Data 0.000 (0.003)	Loss_t 0.2980 (0.3481)	Loss_x 1.2211 (1.2727)	Acc 96.88 (97.71)	Lr 0.000302	eta 2:33:53
Epoch: [34/150][180/185]	Time 0.341 (0.430)	Data 0.000 (0.003)	Loss_t 0.3272 (0.3495)	Loss_x 1.2105 (1.2664)	Acc 100.00 (97.80)	Lr 0.000302	eta 2:33:57
Epoch: [35/150][20/185]	Time 0.455 (0.480)	Data 0.000 (0.024)	Loss_t 0.3205 (0.3294)	Loss_x 1.2711 (1.3109)	Acc 96.88 (97.03)	Lr 0.000310	eta 2:51:29
Epoch: [35/150][40/185]	Time 0.368 (0.445)	Data 0.000 (0.012)	Loss_t 0.3416 (0.3298)	Loss_x 1.3409 (1.2976)	Acc 96.88 (97.23)	Lr 0.000310	eta 2:38:55
Epoch: [35/150][60/185]	Time 0.413 (0.439)	Data 0.000 (0.008)	Loss_t 0.3189 (0.3308)	Loss_x 1.2573 (1.2917)	Acc 98.44 (97.42)	Lr 0.000310	eta 2:36:24
Epoch: [35/150][80/185]	Time 0.472 (0.433)	Data 0.000 (0.006)	Loss_t 0.2628 (0.3280)	Loss_x 1.2993 (1.2910)	Acc 96.88 (97.64)	Lr 0.000310	eta 2:34:28
Epoch: [35/150][100/185]	Time 0.355 (0.426)	Data 0.000 (0.005)	Loss_t 0.3024 (0.3286)	Loss_x 1.2325 (1.2888)	Acc 96.88 (97.59)	Lr 0.000310	eta 2:31:36
Epoch: [35/150][120/185]	Time 0.444 (0.425)	Data 0.000 (0.004)	Loss_t 0.3417 (0.3304)	Loss_x 1.2345 (1.2851)	Acc 96.88 (97.63)	Lr 0.000310	eta 2:31:06
Epoch: [35/150][140/185]	Time 0.477 (0.423)	Data 0.000 (0.003)	Loss_t 0.3715 (0.3306)	Loss_x 1.2490 (1.2785)	Acc 98.44 (97.80)	Lr 0.000310	eta 2:30:19
Epoch: [35/150][160/185]	Time 0.381 (0.420)	Data 0.000 (0.003)	Loss_t 0.3537 (0.3325)	Loss_x 1.2003 (1.2714)	Acc 98.44 (97.90)	Lr 0.000310	eta 2:29:01
Epoch: [35/150][180/185]	Time 0.337 (0.416)	Data 0.000 (0.003)	Loss_t 0.3334 (0.3313)	Loss_x 1.1621 (1.2640)	Acc 100.00 (98.00)	Lr 0.000310	eta 2:27:25
Epoch: [36/150][20/185]	Time 0.435 (0.464)	Data 0.000 (0.025)	Loss_t 0.3485 (0.3268)	Loss_x 1.3368 (1.3116)	Acc 96.88 (97.89)	Lr 0.000318	eta 2:44:29
Epoch: [36/150][40/185]	Time 0.408 (0.440)	Data 0.000 (0.013)	Loss_t 0.3545 (0.3297)	Loss_x 1.2475 (1.2945)	Acc 100.00 (97.58)	Lr 0.000318	eta 2:35:53
Epoch: [36/150][60/185]	Time 0.415 (0.431)	Data 0.000 (0.008)	Loss_t 0.3948 (0.3345)	Loss_x 1.2881 (1.2955)	Acc 98.44 (97.32)	Lr 0.000318	eta 2:32:20
Epoch: [36/150][80/185]	Time 0.434 (0.425)	Data 0.000 (0.006)	Loss_t 0.4730 (0.3323)	Loss_x 1.2930 (1.2888)	Acc 95.31 (97.54)	Lr 0.000318	eta 2:30:00
Epoch: [36/150][100/185]	Time 0.441 (0.422)	Data 0.000 (0.005)	Loss_t 0.4270 (0.3330)	Loss_x 1.3591 (1.2833)	Acc 95.31 (97.80)	Lr 0.000318	eta 2:29:05
Epoch: [36/150][120/185]	Time 0.386 (0.417)	Data 0.000 (0.004)	Loss_t 0.3725 (0.3290)	Loss_x 1.2735 (1.2801)	Acc 96.88 (97.92)	Lr 0.000318	eta 2:26:54
Epoch: [36/150][140/185]	Time 0.402 (0.413)	Data 0.000 (0.004)	Loss_t 0.2526 (0.3263)	Loss_x 1.1825 (1.2746)	Acc 100.00 (97.95)	Lr 0.000318	eta 2:25:37
Epoch: [36/150][160/185]	Time 0.416 (0.414)	Data 0.000 (0.003)	Loss_t 0.3980 (0.3277)	Loss_x 1.2874 (1.2719)	Acc 96.88 (97.99)	Lr 0.000318	eta 2:25:42
Epoch: [36/150][180/185]	Time 0.361 (0.413)	Data 0.000 (0.003)	Loss_t 0.3375 (0.3271)	Loss_x 1.1734 (1.2646)	Acc 100.00 (98.04)	Lr 0.000318	eta 2:25:16
Epoch: [37/150][20/185]	Time 0.427 (0.460)	Data 0.000 (0.022)	Loss_t 0.3887 (0.3339)	Loss_x 1.2522 (1.3080)	Acc 98.44 (97.27)	Lr 0.000326	eta 2:41:40
Epoch: [37/150][40/185]	Time 0.407 (0.445)	Data 0.000 (0.011)	Loss_t 0.2726 (0.3173)	Loss_x 1.3075 (1.2958)	Acc 95.31 (97.50)	Lr 0.000326	eta 2:36:09
Epoch: [37/150][60/185]	Time 0.453 (0.436)	Data 0.000 (0.007)	Loss_t 0.3085 (0.3196)	Loss_x 1.3108 (1.2981)	Acc 95.31 (97.50)	Lr 0.000326	eta 2:32:52
Epoch: [37/150][80/185]	Time 0.458 (0.438)	Data 0.000 (0.006)	Loss_t 0.3818 (0.3239)	Loss_x 1.3342 (1.2922)	Acc 96.88 (97.68)	Lr 0.000326	eta 2:33:17
Epoch: [37/150][100/185]	Time 0.420 (0.436)	Data 0.000 (0.004)	Loss_t 0.3029 (0.3222)	Loss_x 1.2247 (1.2882)	Acc 96.88 (97.80)	Lr 0.000326	eta 2:32:29
Epoch: [37/150][120/185]	Time 0.418 (0.431)	Data 0.000 (0.004)	Loss_t 0.2838 (0.3219)	Loss_x 1.3391 (1.2863)	Acc 100.00 (97.92)	Lr 0.000326	eta 2:30:39
Epoch: [37/150][140/185]	Time 0.402 (0.428)	Data 0.000 (0.003)	Loss_t 0.3393 (0.3192)	Loss_x 1.1937 (1.2826)	Acc 96.88 (97.87)	Lr 0.000326	eta 2:29:36
Epoch: [37/150][160/185]	Time 0.387 (0.425)	Data 0.000 (0.003)	Loss_t 0.2379 (0.3155)	Loss_x 1.1657 (1.2736)	Acc 100.00 (97.93)	Lr 0.000326	eta 2:28:19
Epoch: [37/150][180/185]	Time 0.423 (0.423)	Data 0.000 (0.003)	Loss_t 0.2737 (0.3139)	Loss_x 1.1599 (1.2652)	Acc 100.00 (98.06)	Lr 0.000326	eta 2:27:34
Epoch: [38/150][20/185]	Time 0.343 (0.423)	Data 0.000 (0.024)	Loss_t 0.3612 (0.3091)	Loss_x 1.3078 (1.2952)	Acc 96.88 (96.80)	Lr 0.000334	eta 2:27:23
Epoch: [38/150][40/185]	Time 0.402 (0.413)	Data 0.000 (0.012)	Loss_t 0.3415 (0.3072)	Loss_x 1.2629 (1.2914)	Acc 98.44 (97.03)	Lr 0.000334	eta 2:23:34
Epoch: [38/150][60/185]	Time 0.406 (0.413)	Data 0.000 (0.008)	Loss_t 0.3115 (0.3036)	Loss_x 1.3305 (1.2895)	Acc 98.44 (97.37)	Lr 0.000334	eta 2:23:30
Epoch: [38/150][80/185]	Time 0.413 (0.411)	Data 0.000 (0.006)	Loss_t 0.3503 (0.3080)	Loss_x 1.3543 (1.2902)	Acc 95.31 (97.36)	Lr 0.000334	eta 2:22:46
Epoch: [38/150][100/185]	Time 0.383 (0.410)	Data 0.000 (0.005)	Loss_t 0.3780 (0.3068)	Loss_x 1.2810 (1.2884)	Acc 100.00 (97.44)	Lr 0.000334	eta 2:22:02
Epoch: [38/150][120/185]	Time 0.391 (0.409)	Data 0.000 (0.004)	Loss_t 0.2634 (0.3041)	Loss_x 1.2629 (1.2851)	Acc 98.44 (97.45)	Lr 0.000334	eta 2:21:39
Epoch: [38/150][140/185]	Time 0.447 (0.410)	Data 0.000 (0.003)	Loss_t 0.3089 (0.3022)	Loss_x 1.2484 (1.2779)	Acc 95.31 (97.57)	Lr 0.000334	eta 2:22:01
Epoch: [38/150][160/185]	Time 0.439 (0.413)	Data 0.000 (0.003)	Loss_t 0.2711 (0.3015)	Loss_x 1.2090 (1.2714)	Acc 98.44 (97.71)	Lr 0.000334	eta 2:22:53
Epoch: [38/150][180/185]	Time 0.441 (0.416)	Data 0.000 (0.003)	Loss_t 0.2260 (0.2994)	Loss_x 1.1435 (1.2637)	Acc 100.00 (97.82)	Lr 0.000334	eta 2:23:36
Epoch: [39/150][20/185]	Time 0.408 (0.436)	Data 0.000 (0.024)	Loss_t 0.3680 (0.3114)	Loss_x 1.3422 (1.3226)	Acc 92.19 (96.80)	Lr 0.000342	eta 2:30:35
Epoch: [39/150][40/185]	Time 0.411 (0.419)	Data 0.000 (0.012)	Loss_t 0.2808 (0.3049)	Loss_x 1.3382 (1.3114)	Acc 95.31 (97.07)	Lr 0.000342	eta 2:24:29
Epoch: [39/150][60/185]	Time 0.379 (0.417)	Data 0.000 (0.008)	Loss_t 0.2320 (0.2918)	Loss_x 1.3026 (1.3028)	Acc 98.44 (97.27)	Lr 0.000342	eta 2:23:40
Epoch: [39/150][80/185]	Time 0.389 (0.410)	Data 0.000 (0.006)	Loss_t 0.2539 (0.2920)	Loss_x 1.2650 (1.2997)	Acc 98.44 (97.50)	Lr 0.000342	eta 2:21:01
Epoch: [39/150][100/185]	Time 0.378 (0.407)	Data 0.000 (0.005)	Loss_t 0.2510 (0.2952)	Loss_x 1.1717 (1.2952)	Acc 100.00 (97.61)	Lr 0.000342	eta 2:19:43
Epoch: [39/150][120/185]	Time 0.373 (0.405)	Data 0.000 (0.004)	Loss_t 0.2634 (0.2949)	Loss_x 1.2403 (1.2908)	Acc 96.88 (97.70)	Lr 0.000342	eta 2:19:03
Epoch: [39/150][140/185]	Time 0.416 (0.405)	Data 0.000 (0.004)	Loss_t 0.2951 (0.2967)	Loss_x 1.2622 (1.2879)	Acc 98.44 (97.70)	Lr 0.000342	eta 2:19:04
Epoch: [39/150][160/185]	Time 0.392 (0.405)	Data 0.000 (0.003)	Loss_t 0.2666 (0.2943)	Loss_x 1.2124 (1.2823)	Acc 100.00 (97.75)	Lr 0.000342	eta 2:18:41
Epoch: [39/150][180/185]	Time 0.371 (0.404)	Data 0.000 (0.003)	Loss_t 0.2846 (0.2949)	Loss_x 1.1683 (1.2761)	Acc 100.00 (97.75)	Lr 0.000342	eta 2:18:27
Epoch: [40/150][20/185]	Time 0.481 (0.452)	Data 0.000 (0.024)	Loss_t 0.3141 (0.2988)	Loss_x 1.2680 (1.3062)	Acc 96.88 (96.64)	Lr 0.000350	eta 2:34:30
Epoch: [40/150][40/185]	Time 0.435 (0.440)	Data 0.000 (0.012)	Loss_t 0.3380 (0.2969)	Loss_x 1.2904 (1.2970)	Acc 98.44 (97.19)	Lr 0.000350	eta 2:30:24
Epoch: [40/150][60/185]	Time 0.442 (0.440)	Data 0.000 (0.008)	Loss_t 0.2995 (0.2928)	Loss_x 1.3436 (1.3014)	Acc 96.88 (97.21)	Lr 0.000350	eta 2:30:10
Epoch: [40/150][80/185]	Time 0.443 (0.440)	Data 0.000 (0.006)	Loss_t 0.3172 (0.2886)	Loss_x 1.2821 (1.2919)	Acc 98.44 (97.50)	Lr 0.000350	eta 2:29:50
Epoch: [40/150][100/185]	Time 0.446 (0.440)	Data 0.000 (0.005)	Loss_t 0.2402 (0.2866)	Loss_x 1.2492 (1.2893)	Acc 98.44 (97.50)	Lr 0.000350	eta 2:29:50
Epoch: [40/150][120/185]	Time 0.456 (0.440)	Data 0.000 (0.004)	Loss_t 0.1760 (0.2859)	Loss_x 1.2654 (1.2881)	Acc 96.88 (97.50)	Lr 0.000350	eta 2:29:37
Epoch: [40/150][140/185]	Time 0.435 (0.439)	Data 0.000 (0.004)	Loss_t 0.3146 (0.2855)	Loss_x 1.2035 (1.2846)	Acc 98.44 (97.61)	Lr 0.000350	eta 2:29:19
Epoch: [40/150][160/185]	Time 0.438 (0.439)	Data 0.000 (0.003)	Loss_t 0.3321 (0.2839)	Loss_x 1.2608 (1.2764)	Acc 98.44 (97.79)	Lr 0.000350	eta 2:29:05
Epoch: [40/150][180/185]	Time 0.485 (0.438)	Data 0.000 (0.003)	Loss_t 0.2584 (0.2817)	Loss_x 1.2875 (1.2690)	Acc 93.75 (97.83)	Lr 0.000350	eta 2:28:40
##### Evaluating market1501 (source) #####
Extracting features from query set ...
Done, obtained 3368-by-4608 matrix
Extracting features from gallery set ...
Done, obtained 15913-by-4608 matrix
Speed: 0.0330 sec/batch
Computing distance matrix with metric=euclidean ...
Computing CMC and mAP ...
** Results **
mAP: 78.0%
CMC curve
Rank-1  : 90.4%
Rank-5  : 96.8%
Rank-10 : 98.2%
Rank-20 : 98.9%
Checkpoint saved to "log/model.pth.tar-40"
Epoch: [41/150][20/185]	Time 0.447 (0.460)	Data 0.000 (0.027)	Loss_t 0.2869 (0.2787)	Loss_x 1.3047 (1.3261)	Acc 96.88 (96.64)	Lr 0.000350	eta 2:35:57
Epoch: [41/150][40/185]	Time 0.421 (0.445)	Data 0.000 (0.014)	Loss_t 0.2348 (0.2837)	Loss_x 1.2947 (1.3108)	Acc 98.44 (97.42)	Lr 0.000350	eta 2:30:31
Epoch: [41/150][60/185]	Time 0.388 (0.439)	Data 0.000 (0.009)	Loss_t 0.2467 (0.2780)	Loss_x 1.3616 (1.3026)	Acc 96.88 (97.68)	Lr 0.000350	eta 2:28:23
Epoch: [41/150][80/185]	Time 0.439 (0.441)	Data 0.000 (0.007)	Loss_t 0.2640 (0.2763)	Loss_x 1.2460 (1.2951)	Acc 98.44 (97.79)	Lr 0.000350	eta 2:29:00
Epoch: [41/150][100/185]	Time 0.433 (0.441)	Data 0.000 (0.006)	Loss_t 0.2151 (0.2778)	Loss_x 1.2309 (1.2929)	Acc 100.00 (97.89)	Lr 0.000350	eta 2:28:47
Epoch: [41/150][120/185]	Time 0.457 (0.442)	Data 0.000 (0.005)	Loss_t 0.4395 (0.2773)	Loss_x 1.2714 (1.2861)	Acc 93.75 (97.94)	Lr 0.000350	eta 2:28:54
Epoch: [41/150][140/185]	Time 0.418 (0.440)	Data 0.000 (0.004)	Loss_t 0.2834 (0.2763)	Loss_x 1.2289 (1.2804)	Acc 100.00 (97.90)	Lr 0.000350	eta 2:28:19
Epoch: [41/150][160/185]	Time 0.417 (0.438)	Data 0.000 (0.004)	Loss_t 0.2418 (0.2737)	Loss_x 1.1678 (1.2729)	Acc 100.00 (98.04)	Lr 0.000350	eta 2:27:22
Epoch: [41/150][180/185]	Time 0.438 (0.437)	Data 0.000 (0.003)	Loss_t 0.2180 (0.2711)	Loss_x 1.1812 (1.2647)	Acc 96.88 (98.08)	Lr 0.000350	eta 2:26:45
Epoch: [42/150][20/185]	Time 0.465 (0.468)	Data 0.000 (0.025)	Loss_t 0.2160 (0.2519)	Loss_x 1.3711 (1.3046)	Acc 98.44 (97.42)	Lr 0.000350	eta 2:37:01
Epoch: [42/150][40/185]	Time 0.439 (0.452)	Data 0.000 (0.013)	Loss_t 0.3964 (0.2587)	Loss_x 1.2855 (1.2932)	Acc 96.88 (97.73)	Lr 0.000350	eta 2:31:32
Epoch: [42/150][60/185]	Time 0.410 (0.451)	Data 0.000 (0.009)	Loss_t 0.2478 (0.2562)	Loss_x 1.2205 (1.2905)	Acc 98.44 (97.76)	Lr 0.000350	eta 2:31:08
Epoch: [42/150][80/185]	Time 0.426 (0.446)	Data 0.000 (0.006)	Loss_t 0.3141 (0.2587)	Loss_x 1.2994 (1.2906)	Acc 98.44 (97.70)	Lr 0.000350	eta 2:29:20
Epoch: [42/150][100/185]	Time 0.421 (0.444)	Data 0.000 (0.005)	Loss_t 0.2591 (0.2570)	Loss_x 1.2238 (1.2858)	Acc 98.44 (97.77)	Lr 0.000350	eta 2:28:19
Epoch: [42/150][120/185]	Time 0.474 (0.443)	Data 0.000 (0.004)	Loss_t 0.2136 (0.2597)	Loss_x 1.2315 (1.2839)	Acc 98.44 (97.83)	Lr 0.000350	eta 2:28:09
Epoch: [42/150][140/185]	Time 0.425 (0.442)	Data 0.000 (0.004)	Loss_t 0.2381 (0.2582)	Loss_x 1.2190 (1.2776)	Acc 100.00 (97.95)	Lr 0.000350	eta 2:27:31
Epoch: [42/150][160/185]	Time 0.431 (0.439)	Data 0.000 (0.003)	Loss_t 0.3154 (0.2593)	Loss_x 1.2999 (1.2713)	Acc 98.44 (98.05)	Lr 0.000350	eta 2:26:18
Epoch: [42/150][180/185]	Time 0.397 (0.434)	Data 0.000 (0.003)	Loss_t 0.2202 (0.2581)	Loss_x 1.1694 (1.2638)	Acc 100.00 (98.07)	Lr 0.000350	eta 2:24:26
Epoch: [43/150][20/185]	Time 0.487 (0.458)	Data 0.000 (0.024)	Loss_t 0.2990 (0.2570)	Loss_x 1.3565 (1.3114)	Acc 92.19 (96.48)	Lr 0.000350	eta 2:32:13
Epoch: [43/150][40/185]	Time 0.371 (0.444)	Data 0.000 (0.012)	Loss_t 0.2647 (0.2659)	Loss_x 1.2633 (1.2972)	Acc 98.44 (97.27)	Lr 0.000350	eta 2:27:37
Epoch: [43/150][60/185]	Time 0.451 (0.440)	Data 0.000 (0.008)	Loss_t 0.3040 (0.2665)	Loss_x 1.2389 (1.2946)	Acc 98.44 (97.63)	Lr 0.000350	eta 2:26:06
Epoch: [43/150][80/185]	Time 0.497 (0.435)	Data 0.000 (0.006)	Loss_t 0.1769 (0.2589)	Loss_x 1.3115 (1.2899)	Acc 98.44 (97.77)	Lr 0.000350	eta 2:24:25
Epoch: [43/150][100/185]	Time 0.433 (0.434)	Data 0.000 (0.005)	Loss_t 0.3956 (0.2614)	Loss_x 1.2599 (1.2841)	Acc 100.00 (97.73)	Lr 0.000350	eta 2:23:39
Epoch: [43/150][120/185]	Time 0.412 (0.431)	Data 0.000 (0.004)	Loss_t 0.1796 (0.2594)	Loss_x 1.2806 (1.2807)	Acc 100.00 (97.70)	Lr 0.000350	eta 2:22:33
Epoch: [43/150][140/185]	Time 0.440 (0.431)	Data 0.000 (0.004)	Loss_t 0.2910 (0.2564)	Loss_x 1.2483 (1.2765)	Acc 96.88 (97.82)	Lr 0.000350	eta 2:22:28
Epoch: [43/150][160/185]	Time 0.439 (0.431)	Data 0.000 (0.003)	Loss_t 0.1726 (0.2553)	Loss_x 1.1578 (1.2704)	Acc 100.00 (97.93)	Lr 0.000350	eta 2:22:32
Epoch: [43/150][180/185]	Time 0.458 (0.431)	Data 0.000 (0.003)	Loss_t 0.2841 (0.2527)	Loss_x 1.2425 (1.2615)	Acc 98.44 (98.09)	Lr 0.000350	eta 2:22:04
Epoch: [44/150][20/185]	Time 0.426 (0.455)	Data 0.000 (0.024)	Loss_t 0.1609 (0.2504)	Loss_x 1.1934 (1.2812)	Acc 100.00 (98.83)	Lr 0.000350	eta 2:29:59
Epoch: [44/150][40/185]	Time 0.436 (0.449)	Data 0.000 (0.012)	Loss_t 0.3005 (0.2451)	Loss_x 1.3327 (1.2768)	Acc 93.75 (98.48)	Lr 0.000350	eta 2:27:47
Epoch: [44/150][60/185]	Time 0.451 (0.441)	Data 0.000 (0.008)	Loss_t 0.1790 (0.2383)	Loss_x 1.2273 (1.2730)	Acc 98.44 (98.28)	Lr 0.000350	eta 2:25:11
Epoch: [44/150][80/185]	Time 0.437 (0.440)	Data 0.000 (0.006)	Loss_t 0.2754 (0.2372)	Loss_x 1.2553 (1.2668)	Acc 100.00 (98.48)	Lr 0.000350	eta 2:24:27
Epoch: [44/150][100/185]	Time 0.386 (0.436)	Data 0.000 (0.005)	Loss_t 0.1671 (0.2338)	Loss_x 1.2591 (1.2665)	Acc 98.44 (98.39)	Lr 0.000350	eta 2:23:02
Epoch: [44/150][120/185]	Time 0.399 (0.434)	Data 0.000 (0.004)	Loss_t 0.1932 (0.2349)	Loss_x 1.1634 (1.2604)	Acc 100.00 (98.50)	Lr 0.000350	eta 2:22:27
Epoch: [44/150][140/185]	Time 0.425 (0.432)	Data 0.000 (0.004)	Loss_t 0.2257 (0.2328)	Loss_x 1.2266 (1.2572)	Acc 95.31 (98.49)	Lr 0.000350	eta 2:21:28
Epoch: [44/150][160/185]	Time 0.414 (0.431)	Data 0.000 (0.003)	Loss_t 0.1902 (0.2312)	Loss_x 1.1862 (1.2534)	Acc 100.00 (98.48)	Lr 0.000350	eta 2:21:09
Epoch: [44/150][180/185]	Time 0.438 (0.430)	Data 0.000 (0.003)	Loss_t 0.2352 (0.2312)	Loss_x 1.2055 (1.2478)	Acc 98.44 (98.51)	Lr 0.000350	eta 2:20:35
Epoch: [45/150][20/185]	Time 0.429 (0.458)	Data 0.000 (0.029)	Loss_t 0.2322 (0.2316)	Loss_x 1.2326 (1.2962)	Acc 100.00 (97.27)	Lr 0.000350	eta 2:29:23
Epoch: [45/150][40/185]	Time 0.424 (0.442)	Data 0.000 (0.015)	Loss_t 0.2829 (0.2429)	Loss_x 1.3302 (1.3012)	Acc 96.88 (97.23)	Lr 0.000350	eta 2:24:04
Epoch: [45/150][60/185]	Time 0.402 (0.437)	Data 0.000 (0.010)	Loss_t 0.1871 (0.2373)	Loss_x 1.2163 (1.2948)	Acc 100.00 (97.47)	Lr 0.000350	eta 2:22:17
Epoch: [45/150][80/185]	Time 0.400 (0.431)	Data 0.000 (0.007)	Loss_t 0.2747 (0.2319)	Loss_x 1.2289 (1.2872)	Acc 98.44 (97.58)	Lr 0.000350	eta 2:20:22
Epoch: [45/150][100/185]	Time 0.422 (0.427)	Data 0.000 (0.006)	Loss_t 0.1753 (0.2308)	Loss_x 1.1889 (1.2802)	Acc 98.44 (97.78)	Lr 0.000350	eta 2:18:59
Epoch: [45/150][120/185]	Time 0.432 (0.428)	Data 0.000 (0.005)	Loss_t 0.1948 (0.2300)	Loss_x 1.2400 (1.2766)	Acc 96.88 (97.86)	Lr 0.000350	eta 2:18:57
Epoch: [45/150][140/185]	Time 0.417 (0.425)	Data 0.000 (0.004)	Loss_t 0.1560 (0.2298)	Loss_x 1.2611 (1.2734)	Acc 100.00 (97.95)	Lr 0.000350	eta 2:17:54
Epoch: [45/150][160/185]	Time 0.417 (0.424)	Data 0.000 (0.004)	Loss_t 0.3341 (0.2310)	Loss_x 1.3055 (1.2674)	Acc 95.31 (97.98)	Lr 0.000350	eta 2:17:35
Epoch: [45/150][180/185]	Time 0.424 (0.424)	Data 0.000 (0.003)	Loss_t 0.2122 (0.2327)	Loss_x 1.2140 (1.2618)	Acc 98.44 (98.07)	Lr 0.000350	eta 2:17:16
Epoch: [46/150][20/185]	Time 0.401 (0.451)	Data 0.000 (0.025)	Loss_t 0.2486 (0.2533)	Loss_x 1.3066 (1.3195)	Acc 98.44 (96.64)	Lr 0.000350	eta 2:25:55
Epoch: [46/150][40/185]	Time 0.423 (0.431)	Data 0.000 (0.013)	Loss_t 0.3334 (0.2441)	Loss_x 1.2897 (1.2986)	Acc 98.44 (97.38)	Lr 0.000350	eta 2:19:10
Epoch: [46/150][60/185]	Time 0.437 (0.425)	Data 0.000 (0.008)	Loss_t 0.3222 (0.2389)	Loss_x 1.2338 (1.2908)	Acc 98.44 (97.73)	Lr 0.000350	eta 2:17:11
Epoch: [46/150][80/185]	Time 0.415 (0.423)	Data 0.000 (0.006)	Loss_t 0.2034 (0.2331)	Loss_x 1.2578 (1.2874)	Acc 98.44 (97.77)	Lr 0.000350	eta 2:16:17
Epoch: [46/150][100/185]	Time 0.437 (0.422)	Data 0.000 (0.005)	Loss_t 0.2115 (0.2322)	Loss_x 1.1844 (1.2831)	Acc 100.00 (97.81)	Lr 0.000350	eta 2:15:57
Epoch: [46/150][120/185]	Time 0.423 (0.420)	Data 0.000 (0.004)	Loss_t 0.3040 (0.2327)	Loss_x 1.2733 (1.2785)	Acc 96.88 (97.83)	Lr 0.000350	eta 2:15:07
Epoch: [46/150][140/185]	Time 0.423 (0.420)	Data 0.000 (0.004)	Loss_t 0.1802 (0.2299)	Loss_x 1.1976 (1.2704)	Acc 98.44 (97.94)	Lr 0.000350	eta 2:14:54
Epoch: [46/150][160/185]	Time 0.435 (0.419)	Data 0.000 (0.003)	Loss_t 0.4024 (0.2311)	Loss_x 1.2291 (1.2651)	Acc 98.44 (98.01)	Lr 0.000350	eta 2:14:27
Epoch: [46/150][180/185]	Time 0.411 (0.418)	Data 0.000 (0.003)	Loss_t 0.3306 (0.2309)	Loss_x 1.1954 (1.2576)	Acc 98.44 (98.12)	Lr 0.000350	eta 2:14:01
Epoch: [47/150][20/185]	Time 0.407 (0.459)	Data 0.000 (0.025)	Loss_t 0.2758 (0.2262)	Loss_x 1.2787 (1.2933)	Acc 98.44 (98.28)	Lr 0.000350	eta 2:26:58
Epoch: [47/150][40/185]	Time 0.394 (0.442)	Data 0.000 (0.013)	Loss_t 0.2180 (0.2199)	Loss_x 1.2285 (1.2803)	Acc 100.00 (98.20)	Lr 0.000350	eta 2:21:35
Epoch: [47/150][60/185]	Time 0.440 (0.445)	Data 0.000 (0.009)	Loss_t 0.2234 (0.2257)	Loss_x 1.2680 (1.2740)	Acc 98.44 (98.28)	Lr 0.000350	eta 2:22:16
Epoch: [47/150][80/185]	Time 0.426 (0.443)	Data 0.000 (0.007)	Loss_t 0.1707 (0.2229)	Loss_x 1.2456 (1.2687)	Acc 100.00 (98.36)	Lr 0.000350	eta 2:21:25
Epoch: [47/150][100/185]	Time 0.427 (0.438)	Data 0.000 (0.005)	Loss_t 0.1883 (0.2234)	Loss_x 1.2750 (1.2649)	Acc 95.31 (98.42)	Lr 0.000350	eta 2:19:42
Epoch: [47/150][120/185]	Time 0.404 (0.433)	Data 0.000 (0.004)	Loss_t 0.2878 (0.2217)	Loss_x 1.2694 (1.2618)	Acc 95.31 (98.40)	Lr 0.000350	eta 2:18:03
Epoch: [47/150][140/185]	Time 0.399 (0.429)	Data 0.000 (0.004)	Loss_t 0.1656 (0.2187)	Loss_x 1.1677 (1.2595)	Acc 100.00 (98.36)	Lr 0.000350	eta 2:16:27
Epoch: [47/150][160/185]	Time 0.396 (0.427)	Data 0.000 (0.003)	Loss_t 0.3411 (0.2179)	Loss_x 1.2076 (1.2546)	Acc 100.00 (98.40)	Lr 0.000350	eta 2:15:41
Epoch: [47/150][180/185]	Time 0.375 (0.423)	Data 0.000 (0.003)	Loss_t 0.2008 (0.2151)	Loss_x 1.1342 (1.2472)	Acc 100.00 (98.43)	Lr 0.000350	eta 2:14:21
Epoch: [48/150][20/185]	Time 0.433 (0.445)	Data 0.000 (0.025)	Loss_t 0.2028 (0.2292)	Loss_x 1.3372 (1.3155)	Acc 98.44 (96.72)	Lr 0.000350	eta 2:21:03
Epoch: [48/150][40/185]	Time 0.405 (0.444)	Data 0.000 (0.013)	Loss_t 0.2503 (0.2234)	Loss_x 1.3608 (1.2992)	Acc 96.88 (97.38)	Lr 0.000350	eta 2:20:41
Epoch: [48/150][60/185]	Time 0.433 (0.439)	Data 0.000 (0.008)	Loss_t 0.1744 (0.2241)	Loss_x 1.2048 (1.2905)	Acc 100.00 (97.68)	Lr 0.000350	eta 2:18:57
Epoch: [48/150][80/185]	Time 0.409 (0.439)	Data 0.000 (0.006)	Loss_t 0.1832 (0.2200)	Loss_x 1.2689 (1.2825)	Acc 98.44 (97.95)	Lr 0.000350	eta 2:18:58
Epoch: [48/150][100/185]	Time 0.421 (0.437)	Data 0.000 (0.005)	Loss_t 0.2036 (0.2156)	Loss_x 1.3396 (1.2765)	Acc 93.75 (98.05)	Lr 0.000350	eta 2:17:56
Epoch: [48/150][120/185]	Time 0.413 (0.437)	Data 0.000 (0.004)	Loss_t 0.2213 (0.2147)	Loss_x 1.1942 (1.2704)	Acc 98.44 (98.15)	Lr 0.000350	eta 2:17:49
Epoch: [48/150][140/185]	Time 0.396 (0.435)	Data 0.000 (0.004)	Loss_t 0.1762 (0.2140)	Loss_x 1.2153 (1.2662)	Acc 100.00 (98.25)	Lr 0.000350	eta 2:17:00
Epoch: [48/150][160/185]	Time 0.442 (0.433)	Data 0.000 (0.003)	Loss_t 0.1825 (0.2134)	Loss_x 1.2509 (1.2619)	Acc 96.88 (98.26)	Lr 0.000350	eta 2:16:25
Epoch: [48/150][180/185]	Time 0.410 (0.432)	Data 0.000 (0.003)	Loss_t 0.4313 (0.2125)	Loss_x 1.2227 (1.2550)	Acc 98.44 (98.32)	Lr 0.000350	eta 2:15:54
Epoch: [49/150][20/185]	Time 0.441 (0.459)	Data 0.000 (0.025)	Loss_t 0.1446 (0.2246)	Loss_x 1.2818 (1.2931)	Acc 100.00 (97.58)	Lr 0.000350	eta 2:24:11
Epoch: [49/150][40/185]	Time 0.434 (0.439)	Data 0.000 (0.013)	Loss_t 0.1551 (0.2008)	Loss_x 1.2830 (1.2789)	Acc 95.31 (97.89)	Lr 0.000350	eta 2:17:39
Epoch: [49/150][60/185]	Time 0.415 (0.436)	Data 0.000 (0.008)	Loss_t 0.2243 (0.1999)	Loss_x 1.3351 (1.2725)	Acc 96.88 (98.07)	Lr 0.000350	eta 2:16:39
Epoch: [49/150][80/185]	Time 0.443 (0.435)	Data 0.000 (0.006)	Loss_t 0.1521 (0.1998)	Loss_x 1.3071 (1.2683)	Acc 98.44 (98.22)	Lr 0.000350	eta 2:16:09
Epoch: [49/150][100/185]	Time 0.423 (0.434)	Data 0.000 (0.005)	Loss_t 0.2209 (0.2015)	Loss_x 1.2530 (1.2654)	Acc 98.44 (98.27)	Lr 0.000350	eta 2:15:44
Epoch: [49/150][120/185]	Time 0.435 (0.433)	Data 0.000 (0.004)	Loss_t 0.3248 (0.2001)	Loss_x 1.2631 (1.2589)	Acc 96.88 (98.29)	Lr 0.000350	eta 2:15:22
Epoch: [49/150][140/185]	Time 0.386 (0.433)	Data 0.000 (0.004)	Loss_t 0.1896 (0.2035)	Loss_x 1.2097 (1.2542)	Acc 98.44 (98.34)	Lr 0.000350	eta 2:15:05
Epoch: [49/150][160/185]	Time 0.415 (0.432)	Data 0.000 (0.003)	Loss_t 0.2437 (0.2044)	Loss_x 1.2618 (1.2494)	Acc 100.00 (98.37)	Lr 0.000350	eta 2:14:36
Epoch: [49/150][180/185]	Time 0.426 (0.429)	Data 0.000 (0.003)	Loss_t 0.1261 (0.2028)	Loss_x 1.1797 (1.2449)	Acc 100.00 (98.38)	Lr 0.000350	eta 2:13:46
Epoch: [50/150][20/185]	Time 0.430 (0.461)	Data 0.000 (0.027)	Loss_t 0.2715 (0.2236)	Loss_x 1.2380 (1.2960)	Acc 98.44 (97.58)	Lr 0.000350	eta 2:23:26
Epoch: [50/150][40/185]	Time 0.393 (0.449)	Data 0.000 (0.014)	Loss_t 0.3201 (0.2108)	Loss_x 1.3772 (1.2866)	Acc 92.19 (97.62)	Lr 0.000350	eta 2:19:27
Epoch: [50/150][60/185]	Time 0.425 (0.441)	Data 0.000 (0.009)	Loss_t 0.1780 (0.2027)	Loss_x 1.2954 (1.2747)	Acc 96.88 (97.99)	Lr 0.000350	eta 2:16:53
Epoch: [50/150][80/185]	Time 0.439 (0.440)	Data 0.000 (0.007)	Loss_t 0.1668 (0.2044)	Loss_x 1.3080 (1.2688)	Acc 98.44 (98.14)	Lr 0.000350	eta 2:16:34
Epoch: [50/150][100/185]	Time 0.412 (0.438)	Data 0.000 (0.006)	Loss_t 0.1907 (0.2048)	Loss_x 1.2035 (1.2677)	Acc 100.00 (98.22)	Lr 0.000350	eta 2:15:47
Epoch: [50/150][120/185]	Time 0.427 (0.435)	Data 0.000 (0.005)	Loss_t 0.1581 (0.2034)	Loss_x 1.2069 (1.2622)	Acc 100.00 (98.31)	Lr 0.000350	eta 2:14:42
Epoch: [50/150][140/185]	Time 0.430 (0.435)	Data 0.000 (0.004)	Loss_t 0.1492 (0.1982)	Loss_x 1.2272 (1.2568)	Acc 100.00 (98.37)	Lr 0.000350	eta 2:14:23
Epoch: [50/150][160/185]	Time 0.424 (0.433)	Data 0.000 (0.004)	Loss_t 0.2443 (0.1979)	Loss_x 1.2478 (1.2543)	Acc 96.88 (98.29)	Lr 0.000350	eta 2:13:32
Epoch: [50/150][180/185]	Time 0.438 (0.431)	Data 0.000 (0.003)	Loss_t 0.1716 (0.1944)	Loss_x 1.2060 (1.2477)	Acc 100.00 (98.39)	Lr 0.000350	eta 2:12:57
##### Evaluating market1501 (source) #####
Extracting features from query set ...
Done, obtained 3368-by-4608 matrix
Extracting features from gallery set ...
Done, obtained 15913-by-4608 matrix
Speed: 0.0317 sec/batch
Computing distance matrix with metric=euclidean ...
Computing CMC and mAP ...
** Results **
mAP: 80.9%
CMC curve
Rank-1  : 92.3%
Rank-5  : 97.4%
Rank-10 : 98.2%
Rank-20 : 99.0%
Checkpoint saved to "log/model.pth.tar-50"
Epoch: [51/150][20/185]	Time 0.415 (0.434)	Data 0.000 (0.028)	Loss_t 0.2962 (0.2076)	Loss_x 1.2589 (1.2767)	Acc 98.44 (97.66)	Lr 0.000350	eta 2:13:48
Epoch: [51/150][40/185]	Time 0.390 (0.423)	Data 0.000 (0.014)	Loss_t 0.2674 (0.1996)	Loss_x 1.2229 (1.2674)	Acc 100.00 (97.93)	Lr 0.000350	eta 2:10:07
Epoch: [51/150][60/185]	Time 0.404 (0.418)	Data 0.000 (0.009)	Loss_t 0.1849 (0.2036)	Loss_x 1.2783 (1.2604)	Acc 98.44 (98.12)	Lr 0.000350	eta 2:08:23
Epoch: [51/150][80/185]	Time 0.374 (0.415)	Data 0.000 (0.007)	Loss_t 0.2218 (0.1962)	Loss_x 1.3589 (1.2570)	Acc 96.88 (98.24)	Lr 0.000350	eta 2:07:15
Epoch: [51/150][100/185]	Time 0.381 (0.413)	Data 0.000 (0.006)	Loss_t 0.2090 (0.1931)	Loss_x 1.3028 (1.2535)	Acc 93.75 (98.34)	Lr 0.000350	eta 2:06:32
Epoch: [51/150][120/185]	Time 0.378 (0.412)	Data 0.000 (0.005)	Loss_t 0.2071 (0.1934)	Loss_x 1.2117 (1.2540)	Acc 98.44 (98.31)	Lr 0.000350	eta 2:06:07
Epoch: [51/150][140/185]	Time 0.384 (0.411)	Data 0.000 (0.004)	Loss_t 0.1327 (0.1916)	Loss_x 1.1552 (1.2510)	Acc 100.00 (98.33)	Lr 0.000350	eta 2:05:51
Epoch: [51/150][160/185]	Time 0.393 (0.410)	Data 0.000 (0.004)	Loss_t 0.2267 (0.1885)	Loss_x 1.1929 (1.2444)	Acc 100.00 (98.40)	Lr 0.000350	eta 2:05:21
Epoch: [51/150][180/185]	Time 0.375 (0.410)	Data 0.000 (0.003)	Loss_t 0.2259 (0.1868)	Loss_x 1.1761 (1.2370)	Acc 100.00 (98.44)	Lr 0.000350	eta 2:05:11
Epoch: [52/150][20/185]	Time 0.412 (0.458)	Data 0.000 (0.024)	Loss_t 0.2696 (0.2061)	Loss_x 1.3124 (1.2971)	Acc 98.44 (98.28)	Lr 0.000350	eta 2:19:45
Epoch: [52/150][40/185]	Time 0.438 (0.448)	Data 0.000 (0.012)	Loss_t 0.2008 (0.1847)	Loss_x 1.2196 (1.2727)	Acc 100.00 (98.75)	Lr 0.000350	eta 2:16:30
Epoch: [52/150][60/185]	Time 0.429 (0.440)	Data 0.000 (0.008)	Loss_t 0.1069 (0.1811)	Loss_x 1.2083 (1.2654)	Acc 98.44 (98.54)	Lr 0.000350	eta 2:13:59
Epoch: [52/150][80/185]	Time 0.395 (0.436)	Data 0.000 (0.006)	Loss_t 0.1860 (0.1781)	Loss_x 1.3047 (1.2609)	Acc 96.88 (98.34)	Lr 0.000350	eta 2:12:27
Epoch: [52/150][100/185]	Time 0.449 (0.434)	Data 0.000 (0.005)	Loss_t 0.2128 (0.1822)	Loss_x 1.2413 (1.2579)	Acc 100.00 (98.39)	Lr 0.000350	eta 2:11:48
Epoch: [52/150][120/185]	Time 0.437 (0.433)	Data 0.000 (0.004)	Loss_t 0.1456 (0.1833)	Loss_x 1.2462 (1.2568)	Acc 98.44 (98.37)	Lr 0.000350	eta 2:11:26
Epoch: [52/150][140/185]	Time 0.435 (0.433)	Data 0.000 (0.004)	Loss_t 0.2222 (0.1861)	Loss_x 1.1854 (1.2534)	Acc 100.00 (98.48)	Lr 0.000350	eta 2:11:07
Epoch: [52/150][160/185]	Time 0.432 (0.433)	Data 0.000 (0.003)	Loss_t 0.1804 (0.1858)	Loss_x 1.2740 (1.2474)	Acc 100.00 (98.54)	Lr 0.000350	eta 2:11:08
Epoch: [52/150][180/185]	Time 0.406 (0.431)	Data 0.000 (0.003)	Loss_t 0.1872 (0.1834)	Loss_x 1.1692 (1.2393)	Acc 98.44 (98.58)	Lr 0.000350	eta 2:10:20
Epoch: [53/150][20/185]	Time 0.412 (0.459)	Data 0.000 (0.025)	Loss_t 0.2536 (0.1850)	Loss_x 1.3811 (1.3027)	Acc 96.88 (97.19)	Lr 0.000350	eta 2:18:36
Epoch: [53/150][40/185]	Time 0.394 (0.433)	Data 0.000 (0.013)	Loss_t 0.1634 (0.1881)	Loss_x 1.2891 (1.2942)	Acc 98.44 (97.54)	Lr 0.000350	eta 2:10:37
Epoch: [53/150][60/185]	Time 0.449 (0.439)	Data 0.000 (0.008)	Loss_t 0.1703 (0.1885)	Loss_x 1.2317 (1.2823)	Acc 98.44 (97.84)	Lr 0.000350	eta 2:12:20
Epoch: [53/150][80/185]	Time 0.431 (0.440)	Data 0.000 (0.006)	Loss_t 0.1675 (0.1858)	Loss_x 1.3433 (1.2788)	Acc 98.44 (97.89)	Lr 0.000350	eta 2:12:13
Epoch: [53/150][100/185]	Time 0.399 (0.437)	Data 0.000 (0.005)	Loss_t 0.1083 (0.1895)	Loss_x 1.1850 (1.2768)	Acc 100.00 (97.92)	Lr 0.000350	eta 2:11:17
Epoch: [53/150][120/185]	Time 0.420 (0.434)	Data 0.000 (0.004)	Loss_t 0.1073 (0.1869)	Loss_x 1.1828 (1.2714)	Acc 98.44 (98.07)	Lr 0.000350	eta 2:10:20
Epoch: [53/150][140/185]	Time 0.427 (0.434)	Data 0.000 (0.004)	Loss_t 0.2216 (0.1847)	Loss_x 1.2884 (1.2675)	Acc 98.44 (98.09)	Lr 0.000350	eta 2:10:00
Epoch: [53/150][160/185]	Time 0.413 (0.433)	Data 0.000 (0.003)	Loss_t 0.1419 (0.1836)	Loss_x 1.1932 (1.2599)	Acc 100.00 (98.18)	Lr 0.000350	eta 2:09:44
Epoch: [53/150][180/185]	Time 0.391 (0.432)	Data 0.000 (0.003)	Loss_t 0.1669 (0.1847)	Loss_x 1.2136 (1.2523)	Acc 96.88 (98.20)	Lr 0.000350	eta 2:09:09
Epoch: [54/150][20/185]	Time 0.440 (0.446)	Data 0.000 (0.024)	Loss_t 0.1018 (0.1692)	Loss_x 1.2105 (1.2711)	Acc 100.00 (98.52)	Lr 0.000350	eta 2:13:20
Epoch: [54/150][40/185]	Time 0.440 (0.443)	Data 0.000 (0.012)	Loss_t 0.1625 (0.1746)	Loss_x 1.3156 (1.2626)	Acc 93.75 (98.36)	Lr 0.000350	eta 2:12:05
Epoch: [54/150][60/185]	Time 0.437 (0.441)	Data 0.000 (0.008)	Loss_t 0.1748 (0.1790)	Loss_x 1.2597 (1.2613)	Acc 98.44 (98.20)	Lr 0.000350	eta 2:11:27
Epoch: [54/150][80/185]	Time 0.445 (0.438)	Data 0.000 (0.006)	Loss_t 0.1223 (0.1773)	Loss_x 1.2451 (1.2561)	Acc 100.00 (98.34)	Lr 0.000350	eta 2:10:26
Epoch: [54/150][100/185]	Time 0.431 (0.436)	Data 0.000 (0.005)	Loss_t 0.0844 (0.1707)	Loss_x 1.1722 (1.2528)	Acc 100.00 (98.44)	Lr 0.000350	eta 2:09:44
Epoch: [54/150][120/185]	Time 0.436 (0.436)	Data 0.000 (0.004)	Loss_t 0.2562 (0.1720)	Loss_x 1.2964 (1.2511)	Acc 96.88 (98.49)	Lr 0.000350	eta 2:09:37
Epoch: [54/150][140/185]	Time 0.410 (0.436)	Data 0.000 (0.004)	Loss_t 0.2131 (0.1718)	Loss_x 1.2319 (1.2477)	Acc 98.44 (98.55)	Lr 0.000350	eta 2:09:19
Epoch: [54/150][160/185]	Time 0.416 (0.434)	Data 0.000 (0.003)	Loss_t 0.2585 (0.1737)	Loss_x 1.2612 (1.2438)	Acc 98.44 (98.55)	Lr 0.000350	eta 2:08:35
Epoch: [54/150][180/185]	Time 0.434 (0.433)	Data 0.000 (0.003)	Loss_t 0.1396 (0.1731)	Loss_x 1.1492 (1.2384)	Acc 100.00 (98.61)	Lr 0.000350	eta 2:08:04
Epoch: [55/150][20/185]	Time 0.447 (0.457)	Data 0.000 (0.025)	Loss_t 0.1815 (0.1752)	Loss_x 1.1981 (1.2760)	Acc 98.44 (98.28)	Lr 0.000350	eta 2:15:13
Epoch: [55/150][40/185]	Time 0.407 (0.447)	Data 0.000 (0.012)	Loss_t 0.2625 (0.1707)	Loss_x 1.3421 (1.2588)	Acc 90.62 (98.24)	Lr 0.000350	eta 2:11:54
Epoch: [55/150][60/185]	Time 0.445 (0.445)	Data 0.000 (0.008)	Loss_t 0.0856 (0.1720)	Loss_x 1.1695 (1.2567)	Acc 100.00 (98.31)	Lr 0.000350	eta 2:11:07
Epoch: [55/150][80/185]	Time 0.440 (0.438)	Data 0.000 (0.006)	Loss_t 0.1102 (0.1696)	Loss_x 1.2464 (1.2570)	Acc 100.00 (98.38)	Lr 0.000350	eta 2:08:55
Epoch: [55/150][100/185]	Time 0.423 (0.434)	Data 0.000 (0.005)	Loss_t 0.2535 (0.1727)	Loss_x 1.2670 (1.2553)	Acc 98.44 (98.41)	Lr 0.000350	eta 2:07:39
Epoch: [55/150][120/185]	Time 0.442 (0.432)	Data 0.000 (0.004)	Loss_t 0.2470 (0.1738)	Loss_x 1.1855 (1.2542)	Acc 98.44 (98.31)	Lr 0.000350	eta 2:06:53
Epoch: [55/150][140/185]	Time 0.413 (0.429)	Data 0.000 (0.004)	Loss_t 0.1584 (0.1730)	Loss_x 1.2199 (1.2504)	Acc 100.00 (98.40)	Lr 0.000350	eta 2:05:55
Epoch: [55/150][160/185]	Time 0.449 (0.428)	Data 0.000 (0.003)	Loss_t 0.1606 (0.1750)	Loss_x 1.1470 (1.2492)	Acc 100.00 (98.31)	Lr 0.000350	eta 2:05:29
Epoch: [55/150][180/185]	Time 0.439 (0.426)	Data 0.000 (0.003)	Loss_t 0.1550 (0.1737)	Loss_x 1.1833 (1.2427)	Acc 98.44 (98.39)	Lr 0.000350	eta 2:04:53
Epoch: [56/150][20/185]	Time 0.453 (0.449)	Data 0.000 (0.027)	Loss_t 0.1811 (0.1899)	Loss_x 1.2662 (1.3005)	Acc 96.88 (97.66)	Lr 0.000350	eta 2:11:23
Epoch: [56/150][40/185]	Time 0.470 (0.446)	Data 0.000 (0.014)	Loss_t 0.1604 (0.1684)	Loss_x 1.2086 (1.2835)	Acc 98.44 (98.24)	Lr 0.000350	eta 2:10:14
Epoch: [56/150][60/185]	Time 0.431 (0.430)	Data 0.000 (0.009)	Loss_t 0.1052 (0.1658)	Loss_x 1.2595 (1.2666)	Acc 98.44 (98.46)	Lr 0.000350	eta 2:05:34
Epoch: [56/150][80/185]	Time 0.378 (0.429)	Data 0.000 (0.007)	Loss_t 0.1136 (0.1656)	Loss_x 1.1718 (1.2610)	Acc 100.00 (98.55)	Lr 0.000350	eta 2:05:11
Epoch: [56/150][100/185]	Time 0.445 (0.430)	Data 0.000 (0.006)	Loss_t 0.1955 (0.1658)	Loss_x 1.2482 (1.2553)	Acc 98.44 (98.62)	Lr 0.000350	eta 2:05:20
Epoch: [56/150][120/185]	Time 0.440 (0.431)	Data 0.000 (0.005)	Loss_t 0.1689 (0.1638)	Loss_x 1.2486 (1.2511)	Acc 98.44 (98.70)	Lr 0.000350	eta 2:05:17
Epoch: [56/150][140/185]	Time 0.425 (0.432)	Data 0.000 (0.004)	Loss_t 0.3168 (0.1628)	Loss_x 1.2813 (1.2496)	Acc 100.00 (98.65)	Lr 0.000350	eta 2:05:34
Epoch: [56/150][160/185]	Time 0.434 (0.432)	Data 0.000 (0.004)	Loss_t 0.1222 (0.1611)	Loss_x 1.1616 (1.2431)	Acc 100.00 (98.67)	Lr 0.000350	eta 2:05:15
Epoch: [56/150][180/185]	Time 0.427 (0.431)	Data 0.000 (0.003)	Loss_t 0.2293 (0.1612)	Loss_x 1.2167 (1.2365)	Acc 95.31 (98.71)	Lr 0.000350	eta 2:04:59
Epoch: [57/150][20/185]	Time 0.469 (0.469)	Data 0.000 (0.025)	Loss_t 0.1618 (0.1707)	Loss_x 1.3424 (1.2925)	Acc 96.88 (97.58)	Lr 0.000350	eta 2:15:43
Epoch: [57/150][40/185]	Time 0.445 (0.453)	Data 0.000 (0.012)	Loss_t 0.2115 (0.1747)	Loss_x 1.2244 (1.2744)	Acc 100.00 (98.12)	Lr 0.000350	eta 2:11:00
Epoch: [57/150][60/185]	Time 0.448 (0.449)	Data 0.000 (0.008)	Loss_t 0.1489 (0.1676)	Loss_x 1.2622 (1.2631)	Acc 98.44 (98.41)	Lr 0.000350	eta 2:09:49
Epoch: [57/150][80/185]	Time 0.431 (0.444)	Data 0.000 (0.006)	Loss_t 0.1269 (0.1673)	Loss_x 1.2035 (1.2590)	Acc 98.44 (98.42)	Lr 0.000350	eta 2:08:11
Epoch: [57/150][100/185]	Time 0.416 (0.441)	Data 0.000 (0.005)	Loss_t 0.2103 (0.1689)	Loss_x 1.2980 (1.2610)	Acc 95.31 (98.33)	Lr 0.000350	eta 2:06:58
Epoch: [57/150][120/185]	Time 0.423 (0.437)	Data 0.000 (0.004)	Loss_t 0.1181 (0.1647)	Loss_x 1.1706 (1.2549)	Acc 98.44 (98.33)	Lr 0.000350	eta 2:05:50
Epoch: [57/150][140/185]	Time 0.435 (0.437)	Data 0.000 (0.004)	Loss_t 0.3100 (0.1673)	Loss_x 1.2558 (1.2500)	Acc 98.44 (98.42)	Lr 0.000350	eta 2:05:34
Epoch: [57/150][160/185]	Time 0.401 (0.436)	Data 0.000 (0.003)	Loss_t 0.1632 (0.1674)	Loss_x 1.1667 (1.2446)	Acc 100.00 (98.48)	Lr 0.000350	eta 2:05:06
Epoch: [57/150][180/185]	Time 0.418 (0.433)	Data 0.000 (0.003)	Loss_t 0.3365 (0.1685)	Loss_x 1.1944 (1.2375)	Acc 98.44 (98.59)	Lr 0.000350	eta 2:04:18
Epoch: [58/150][20/185]	Time 0.441 (0.457)	Data 0.000 (0.025)	Loss_t 0.1219 (0.1560)	Loss_x 1.2461 (1.2540)	Acc 100.00 (97.97)	Lr 0.000350	eta 2:10:55
Epoch: [58/150][40/185]	Time 0.434 (0.446)	Data 0.000 (0.012)	Loss_t 0.1463 (0.1539)	Loss_x 1.1712 (1.2513)	Acc 100.00 (98.36)	Lr 0.000350	eta 2:07:33
Epoch: [58/150][60/185]	Time 0.431 (0.438)	Data 0.000 (0.008)	Loss_t 0.2862 (0.1537)	Loss_x 1.2403 (1.2506)	Acc 98.44 (98.12)	Lr 0.000350	eta 2:05:09
Epoch: [58/150][80/185]	Time 0.430 (0.434)	Data 0.000 (0.006)	Loss_t 0.1942 (0.1573)	Loss_x 1.2317 (1.2481)	Acc 98.44 (98.30)	Lr 0.000350	eta 2:03:59
Epoch: [58/150][100/185]	Time 0.451 (0.435)	Data 0.000 (0.005)	Loss_t 0.1823 (0.1607)	Loss_x 1.2263 (1.2449)	Acc 96.88 (98.41)	Lr 0.000350	eta 2:04:04
Epoch: [58/150][120/185]	Time 0.433 (0.433)	Data 0.000 (0.004)	Loss_t 0.1792 (0.1580)	Loss_x 1.2573 (1.2429)	Acc 100.00 (98.46)	Lr 0.000350	eta 2:03:09
Epoch: [58/150][140/185]	Time 0.426 (0.430)	Data 0.000 (0.004)	Loss_t 0.2042 (0.1597)	Loss_x 1.2930 (1.2397)	Acc 96.88 (98.57)	Lr 0.000350	eta 2:02:21
Epoch: [58/150][160/185]	Time 0.411 (0.428)	Data 0.000 (0.003)	Loss_t 0.1244 (0.1603)	Loss_x 1.1585 (1.2338)	Acc 100.00 (98.65)	Lr 0.000350	eta 2:01:38
Epoch: [58/150][180/185]	Time 0.419 (0.426)	Data 0.000 (0.003)	Loss_t 0.1947 (0.1590)	Loss_x 1.1853 (1.2266)	Acc 100.00 (98.69)	Lr 0.000350	eta 2:00:52
Epoch: [59/150][20/185]	Time 0.429 (0.461)	Data 0.000 (0.025)	Loss_t 0.1154 (0.1611)	Loss_x 1.2227 (1.2611)	Acc 98.44 (98.75)	Lr 0.000350	eta 2:10:41
Epoch: [59/150][40/185]	Time 0.369 (0.447)	Data 0.000 (0.013)	Loss_t 0.0950 (0.1627)	Loss_x 1.1995 (1.2598)	Acc 100.00 (98.59)	Lr 0.000350	eta 2:06:29
Epoch: [59/150][60/185]	Time 0.385 (0.435)	Data 0.000 (0.008)	Loss_t 0.2241 (0.1624)	Loss_x 1.2663 (1.2541)	Acc 95.31 (98.65)	Lr 0.000350	eta 2:02:56
Epoch: [59/150][80/185]	Time 0.354 (0.430)	Data 0.000 (0.006)	Loss_t 0.1319 (0.1607)	Loss_x 1.2036 (1.2499)	Acc 100.00 (98.69)	Lr 0.000350	eta 2:01:17
Epoch: [59/150][100/185]	Time 0.428 (0.428)	Data 0.000 (0.005)	Loss_t 0.0829 (0.1591)	Loss_x 1.1670 (1.2497)	Acc 100.00 (98.70)	Lr 0.000350	eta 2:00:45
Epoch: [59/150][120/185]	Time 0.368 (0.429)	Data 0.000 (0.004)	Loss_t 0.1488 (0.1581)	Loss_x 1.2069 (1.2468)	Acc 100.00 (98.70)	Lr 0.000350	eta 2:00:42
Epoch: [59/150][140/185]	Time 0.453 (0.424)	Data 0.000 (0.004)	Loss_t 0.2096 (0.1602)	Loss_x 1.2681 (1.2465)	Acc 98.44 (98.63)	Lr 0.000350	eta 1:59:21
Epoch: [59/150][160/185]	Time 0.420 (0.423)	Data 0.000 (0.003)	Loss_t 0.3111 (0.1611)	Loss_x 1.3208 (1.2437)	Acc 93.75 (98.57)	Lr 0.000350	eta 1:58:59
Epoch: [59/150][180/185]	Time 0.434 (0.423)	Data 0.000 (0.003)	Loss_t 0.1823 (0.1599)	Loss_x 1.1403 (1.2376)	Acc 100.00 (98.63)	Lr 0.000350	eta 1:58:37
Epoch: [60/150][20/185]	Time 0.413 (0.451)	Data 0.000 (0.025)	Loss_t 0.1579 (0.1706)	Loss_x 1.2521 (1.3013)	Acc 96.88 (97.66)	Lr 0.000350	eta 2:06:15
Epoch: [60/150][40/185]	Time 0.447 (0.441)	Data 0.000 (0.013)	Loss_t 0.1870 (0.1603)	Loss_x 1.3097 (1.2742)	Acc 98.44 (98.32)	Lr 0.000350	eta 2:03:21
Epoch: [60/150][60/185]	Time 0.383 (0.429)	Data 0.000 (0.009)	Loss_t 0.1606 (0.1604)	Loss_x 1.2210 (1.2636)	Acc 98.44 (98.46)	Lr 0.000350	eta 1:59:56
Epoch: [60/150][80/185]	Time 0.394 (0.424)	Data 0.000 (0.006)	Loss_t 0.1061 (0.1659)	Loss_x 1.2417 (1.2599)	Acc 100.00 (98.50)	Lr 0.000350	eta 1:58:30
Epoch: [60/150][100/185]	Time 0.408 (0.421)	Data 0.000 (0.005)	Loss_t 0.1338 (0.1649)	Loss_x 1.2520 (1.2583)	Acc 100.00 (98.45)	Lr 0.000350	eta 1:57:22
Epoch: [60/150][120/185]	Time 0.391 (0.418)	Data 0.000 (0.004)	Loss_t 0.0932 (0.1576)	Loss_x 1.2848 (1.2501)	Acc 98.44 (98.57)	Lr 0.000350	eta 1:56:29
Epoch: [60/150][140/185]	Time 0.418 (0.420)	Data 0.000 (0.004)	Loss_t 0.1163 (0.1559)	Loss_x 1.1739 (1.2432)	Acc 100.00 (98.64)	Lr 0.000350	eta 1:56:58
Epoch: [60/150][160/185]	Time 0.397 (0.421)	Data 0.000 (0.003)	Loss_t 0.0920 (0.1537)	Loss_x 1.1929 (1.2392)	Acc 100.00 (98.65)	Lr 0.000350	eta 1:57:02
Epoch: [60/150][180/185]	Time 0.429 (0.420)	Data 0.000 (0.003)	Loss_t 0.1708 (0.1567)	Loss_x 1.2188 (1.2322)	Acc 98.44 (98.72)	Lr 0.000350	eta 1:56:41
##### Evaluating market1501 (source) #####
Extracting features from query set ...
Done, obtained 3368-by-4608 matrix
Extracting features from gallery set ...
Done, obtained 15913-by-4608 matrix
Speed: 0.0308 sec/batch
Computing distance matrix with metric=euclidean ...
Computing CMC and mAP ...
** Results **
mAP: 81.4%
CMC curve
Rank-1  : 92.7%
Rank-5  : 97.5%
Rank-10 : 98.5%
Rank-20 : 99.1%
Checkpoint saved to "log/model.pth.tar-60"
Epoch: [61/150][20/185]	Time 0.399 (0.461)	Data 0.000 (0.028)	Loss_t 0.1395 (0.1663)	Loss_x 1.2210 (1.2946)	Acc 98.44 (98.05)	Lr 0.000350	eta 2:07:47
Epoch: [61/150][40/185]	Time 0.434 (0.441)	Data 0.000 (0.014)	Loss_t 0.1983 (0.1604)	Loss_x 1.3112 (1.2760)	Acc 98.44 (98.36)	Lr 0.000350	eta 2:02:11
Epoch: [61/150][60/185]	Time 0.396 (0.436)	Data 0.000 (0.009)	Loss_t 0.1403 (0.1528)	Loss_x 1.2221 (1.2600)	Acc 98.44 (98.62)	Lr 0.000350	eta 2:00:26
Epoch: [61/150][80/185]	Time 0.429 (0.427)	Data 0.000 (0.007)	Loss_t 0.2621 (0.1558)	Loss_x 1.3665 (1.2590)	Acc 95.31 (98.57)	Lr 0.000350	eta 1:57:57
Epoch: [61/150][100/185]	Time 0.397 (0.424)	Data 0.000 (0.006)	Loss_t 0.1689 (0.1569)	Loss_x 1.2718 (1.2553)	Acc 95.31 (98.59)	Lr 0.000350	eta 1:56:56
Epoch: [61/150][120/185]	Time 0.395 (0.420)	Data 0.000 (0.005)	Loss_t 0.1649 (0.1548)	Loss_x 1.1698 (1.2488)	Acc 100.00 (98.63)	Lr 0.000350	eta 1:55:40
Epoch: [61/150][140/185]	Time 0.415 (0.419)	Data 0.000 (0.004)	Loss_t 0.1115 (0.1550)	Loss_x 1.2141 (1.2459)	Acc 98.44 (98.68)	Lr 0.000350	eta 1:55:19
Epoch: [61/150][160/185]	Time 0.422 (0.418)	Data 0.000 (0.004)	Loss_t 0.1280 (0.1519)	Loss_x 1.1235 (1.2383)	Acc 100.00 (98.74)	Lr 0.000350	eta 1:54:52
Epoch: [61/150][180/185]	Time 0.397 (0.418)	Data 0.000 (0.003)	Loss_t 0.1205 (0.1522)	Loss_x 1.1929 (1.2315)	Acc 100.00 (98.77)	Lr 0.000350	eta 1:54:41
Epoch: [62/150][20/185]	Time 0.408 (0.448)	Data 0.000 (0.026)	Loss_t 0.0975 (0.1485)	Loss_x 1.3124 (1.2749)	Acc 96.88 (97.73)	Lr 0.000350	eta 2:02:50
Epoch: [62/150][40/185]	Time 0.426 (0.439)	Data 0.000 (0.013)	Loss_t 0.0966 (0.1363)	Loss_x 1.1974 (1.2576)	Acc 95.31 (98.32)	Lr 0.000350	eta 2:00:04
Epoch: [62/150][60/185]	Time 0.438 (0.437)	Data 0.000 (0.009)	Loss_t 0.2083 (0.1453)	Loss_x 1.2789 (1.2551)	Acc 96.88 (98.36)	Lr 0.000350	eta 1:59:21
Epoch: [62/150][80/185]	Time 0.428 (0.437)	Data 0.000 (0.007)	Loss_t 0.1371 (0.1515)	Loss_x 1.2537 (1.2545)	Acc 100.00 (98.50)	Lr 0.000350	eta 1:59:15
Epoch: [62/150][100/185]	Time 0.422 (0.435)	Data 0.000 (0.005)	Loss_t 0.1369 (0.1498)	Loss_x 1.2628 (1.2528)	Acc 100.00 (98.55)	Lr 0.000350	eta 1:58:34
Epoch: [62/150][120/185]	Time 0.428 (0.433)	Data 0.000 (0.005)	Loss_t 0.1560 (0.1538)	Loss_x 1.2526 (1.2516)	Acc 100.00 (98.57)	Lr 0.000350	eta 1:57:56
Epoch: [62/150][140/185]	Time 0.393 (0.431)	Data 0.000 (0.004)	Loss_t 0.1129 (0.1507)	Loss_x 1.1926 (1.2463)	Acc 100.00 (98.64)	Lr 0.000350	eta 1:57:09
Epoch: [62/150][160/185]	Time 0.406 (0.430)	Data 0.000 (0.003)	Loss_t 0.1285 (0.1498)	Loss_x 1.1679 (1.2392)	Acc 100.00 (98.78)	Lr 0.000350	eta 1:56:54
Epoch: [62/150][180/185]	Time 0.431 (0.428)	Data 0.000 (0.003)	Loss_t 0.1261 (0.1475)	Loss_x 1.1327 (1.2307)	Acc 100.00 (98.87)	Lr 0.000350	eta 1:56:04
Epoch: [63/150][20/185]	Time 0.459 (0.449)	Data 0.000 (0.025)	Loss_t 0.1591 (0.1650)	Loss_x 1.3010 (1.2863)	Acc 100.00 (98.36)	Lr 0.000350	eta 2:01:45
Epoch: [63/150][40/185]	Time 0.427 (0.442)	Data 0.000 (0.013)	Loss_t 0.1063 (0.1553)	Loss_x 1.2929 (1.2722)	Acc 98.44 (98.28)	Lr 0.000350	eta 1:59:30
Epoch: [63/150][60/185]	Time 0.435 (0.446)	Data 0.000 (0.008)	Loss_t 0.1830 (0.1560)	Loss_x 1.2197 (1.2647)	Acc 98.44 (98.41)	Lr 0.000350	eta 2:00:29
Epoch: [63/150][80/185]	Time 0.437 (0.441)	Data 0.000 (0.006)	Loss_t 0.1966 (0.1615)	Loss_x 1.2762 (1.2676)	Acc 96.88 (98.18)	Lr 0.000350	eta 1:59:09
Epoch: [63/150][100/185]	Time 0.438 (0.437)	Data 0.000 (0.005)	Loss_t 0.1363 (0.1551)	Loss_x 1.1758 (1.2603)	Acc 100.00 (98.33)	Lr 0.000350	eta 1:57:53
Epoch: [63/150][120/185]	Time 0.428 (0.436)	Data 0.000 (0.004)	Loss_t 0.1292 (0.1564)	Loss_x 1.1965 (1.2555)	Acc 100.00 (98.50)	Lr 0.000350	eta 1:57:23
Epoch: [63/150][140/185]	Time 0.420 (0.437)	Data 0.000 (0.004)	Loss_t 0.1362 (0.1551)	Loss_x 1.2872 (1.2498)	Acc 98.44 (98.58)	Lr 0.000350	eta 1:57:28
Epoch: [63/150][160/185]	Time 0.438 (0.437)	Data 0.000 (0.003)	Loss_t 0.1233 (0.1534)	Loss_x 1.1702 (1.2430)	Acc 98.44 (98.62)	Lr 0.000350	eta 1:57:29
Epoch: [63/150][180/185]	Time 0.435 (0.436)	Data 0.000 (0.003)	Loss_t 0.0951 (0.1549)	Loss_x 1.1260 (1.2359)	Acc 100.00 (98.72)	Lr 0.000350	eta 1:56:58
Epoch: [64/150][20/185]	Time 0.473 (0.453)	Data 0.000 (0.024)	Loss_t 0.2012 (0.1594)	Loss_x 1.3078 (1.2837)	Acc 100.00 (97.89)	Lr 0.000350	eta 2:01:25
Epoch: [64/150][40/185]	Time 0.402 (0.446)	Data 0.000 (0.012)	Loss_t 0.0957 (0.1467)	Loss_x 1.2246 (1.2608)	Acc 100.00 (98.28)	Lr 0.000350	eta 1:59:26
Epoch: [64/150][60/185]	Time 0.440 (0.440)	Data 0.000 (0.008)	Loss_t 0.1030 (0.1424)	Loss_x 1.2289 (1.2499)	Acc 98.44 (98.57)	Lr 0.000350	eta 1:57:39
Epoch: [64/150][80/185]	Time 0.441 (0.438)	Data 0.000 (0.006)	Loss_t 0.0783 (0.1400)	Loss_x 1.1848 (1.2431)	Acc 98.44 (98.63)	Lr 0.000350	eta 1:56:54
Epoch: [64/150][100/185]	Time 0.459 (0.440)	Data 0.000 (0.005)	Loss_t 0.1460 (0.1435)	Loss_x 1.1785 (1.2436)	Acc 100.00 (98.62)	Lr 0.000350	eta 1:57:17
Epoch: [64/150][120/185]	Time 0.406 (0.437)	Data 0.000 (0.004)	Loss_t 0.1169 (0.1437)	Loss_x 1.2315 (1.2394)	Acc 96.88 (98.70)	Lr 0.000350	eta 1:56:25
Epoch: [64/150][140/185]	Time 0.440 (0.436)	Data 0.000 (0.004)	Loss_t 0.0779 (0.1420)	Loss_x 1.1831 (1.2339)	Acc 100.00 (98.83)	Lr 0.000350	eta 1:55:53
Epoch: [64/150][160/185]	Time 0.422 (0.435)	Data 0.000 (0.003)	Loss_t 0.1014 (0.1386)	Loss_x 1.1601 (1.2270)	Acc 100.00 (98.92)	Lr 0.000350	eta 1:55:24
Epoch: [64/150][180/185]	Time 0.423 (0.434)	Data 0.000 (0.003)	Loss_t 0.0916 (0.1385)	Loss_x 1.1824 (1.2223)	Acc 100.00 (98.94)	Lr 0.000350	eta 1:54:59
Epoch: [65/150][20/185]	Time 0.437 (0.469)	Data 0.000 (0.028)	Loss_t 0.1952 (0.1465)	Loss_x 1.2059 (1.2774)	Acc 100.00 (97.89)	Lr 0.000350	eta 2:04:18
Epoch: [65/150][40/185]	Time 0.437 (0.454)	Data 0.000 (0.014)	Loss_t 0.1365 (0.1413)	Loss_x 1.2218 (1.2631)	Acc 98.44 (98.32)	Lr 0.000350	eta 2:00:02
Epoch: [65/150][60/185]	Time 0.398 (0.446)	Data 0.000 (0.009)	Loss_t 0.2694 (0.1384)	Loss_x 1.2666 (1.2537)	Acc 98.44 (98.59)	Lr 0.000350	eta 1:57:56
Epoch: [65/150][80/185]	Time 0.402 (0.443)	Data 0.000 (0.007)	Loss_t 0.1216 (0.1385)	Loss_x 1.1873 (1.2493)	Acc 98.44 (98.59)	Lr 0.000350	eta 1:56:51
Epoch: [65/150][100/185]	Time 0.428 (0.439)	Data 0.000 (0.006)	Loss_t 0.1257 (0.1398)	Loss_x 1.2211 (1.2468)	Acc 100.00 (98.59)	Lr 0.000350	eta 1:55:48
Epoch: [65/150][120/185]	Time 0.433 (0.438)	Data 0.000 (0.005)	Loss_t 0.1436 (0.1440)	Loss_x 1.2006 (1.2417)	Acc 100.00 (98.63)	Lr 0.000350	eta 1:55:11
Epoch: [65/150][140/185]	Time 0.416 (0.436)	Data 0.000 (0.004)	Loss_t 0.1639 (0.1440)	Loss_x 1.2111 (1.2376)	Acc 100.00 (98.69)	Lr 0.000350	eta 1:54:41
Epoch: [65/150][160/185]	Time 0.425 (0.434)	Data 0.000 (0.004)	Loss_t 0.2405 (0.1423)	Loss_x 1.2256 (1.2324)	Acc 98.44 (98.76)	Lr 0.000350	eta 1:53:59
Epoch: [65/150][180/185]	Time 0.436 (0.432)	Data 0.000 (0.003)	Loss_t 0.0644 (0.1428)	Loss_x 1.1419 (1.2269)	Acc 100.00 (98.83)	Lr 0.000350	eta 1:53:18
Epoch: [66/150][20/185]	Time 0.436 (0.475)	Data 0.000 (0.029)	Loss_t 0.1603 (0.1705)	Loss_x 1.3123 (1.2927)	Acc 98.44 (97.66)	Lr 0.000350	eta 2:04:20
Epoch: [66/150][40/185]	Time 0.410 (0.455)	Data 0.000 (0.014)	Loss_t 0.1862 (0.1499)	Loss_x 1.2994 (1.2688)	Acc 98.44 (98.12)	Lr 0.000350	eta 1:58:50
Epoch: [66/150][60/185]	Time 0.383 (0.440)	Data 0.000 (0.010)	Loss_t 0.1024 (0.1476)	Loss_x 1.1712 (1.2615)	Acc 100.00 (98.39)	Lr 0.000350	eta 1:54:49
Epoch: [66/150][80/185]	Time 0.443 (0.432)	Data 0.000 (0.007)	Loss_t 0.1493 (0.1472)	Loss_x 1.2386 (1.2586)	Acc 98.44 (98.36)	Lr 0.000350	eta 1:52:31
Epoch: [66/150][100/185]	Time 0.440 (0.430)	Data 0.000 (0.006)	Loss_t 0.1436 (0.1436)	Loss_x 1.1971 (1.2493)	Acc 98.44 (98.44)	Lr 0.000350	eta 1:51:53
Epoch: [66/150][120/185]	Time 0.439 (0.429)	Data 0.000 (0.005)	Loss_t 0.1734 (0.1448)	Loss_x 1.2906 (1.2486)	Acc 98.44 (98.44)	Lr 0.000350	eta 1:51:34
Epoch: [66/150][140/185]	Time 0.437 (0.428)	Data 0.000 (0.004)	Loss_t 0.2101 (0.1460)	Loss_x 1.2217 (1.2450)	Acc 100.00 (98.52)	Lr 0.000350	eta 1:51:12
Epoch: [66/150][160/185]	Time 0.425 (0.428)	Data 0.000 (0.004)	Loss_t 0.2854 (0.1477)	Loss_x 1.3085 (1.2400)	Acc 93.75 (98.54)	Lr 0.000350	eta 1:51:04
Epoch: [66/150][180/185]	Time 0.415 (0.427)	Data 0.000 (0.003)	Loss_t 0.1575 (0.1478)	Loss_x 1.1735 (1.2331)	Acc 100.00 (98.58)	Lr 0.000350	eta 1:50:30
Epoch: [67/150][20/185]	Time 0.431 (0.453)	Data 0.000 (0.025)	Loss_t 0.1811 (0.1508)	Loss_x 1.2614 (1.2774)	Acc 98.44 (97.50)	Lr 0.000350	eta 1:57:11
Epoch: [67/150][40/185]	Time 0.436 (0.443)	Data 0.000 (0.012)	Loss_t 0.1553 (0.1409)	Loss_x 1.3013 (1.2661)	Acc 98.44 (97.97)	Lr 0.000350	eta 1:54:21
Epoch: [67/150][60/185]	Time 0.443 (0.440)	Data 0.000 (0.008)	Loss_t 0.1569 (0.1376)	Loss_x 1.2248 (1.2570)	Acc 100.00 (98.18)	Lr 0.000350	eta 1:53:24
Epoch: [67/150][80/185]	Time 0.438 (0.434)	Data 0.000 (0.006)	Loss_t 0.0883 (0.1400)	Loss_x 1.2267 (1.2547)	Acc 96.88 (98.20)	Lr 0.000350	eta 1:51:52
Epoch: [67/150][100/185]	Time 0.452 (0.434)	Data 0.000 (0.005)	Loss_t 0.1161 (0.1376)	Loss_x 1.1955 (1.2480)	Acc 100.00 (98.39)	Lr 0.000350	eta 1:51:44
Epoch: [67/150][120/185]	Time 0.425 (0.435)	Data 0.000 (0.004)	Loss_t 0.1253 (0.1367)	Loss_x 1.1599 (1.2447)	Acc 100.00 (98.44)	Lr 0.000350	eta 1:51:42
Epoch: [67/150][140/185]	Time 0.464 (0.435)	Data 0.000 (0.004)	Loss_t 0.0725 (0.1389)	Loss_x 1.1951 (1.2398)	Acc 100.00 (98.52)	Lr 0.000350	eta 1:51:40
Epoch: [67/150][160/185]	Time 0.443 (0.436)	Data 0.000 (0.003)	Loss_t 0.1339 (0.1394)	Loss_x 1.1774 (1.2340)	Acc 100.00 (98.57)	Lr 0.000350	eta 1:51:41
Epoch: [67/150][180/185]	Time 0.417 (0.434)	Data 0.000 (0.003)	Loss_t 0.0506 (0.1390)	Loss_x 1.1540 (1.2268)	Acc 100.00 (98.65)	Lr 0.000350	eta 1:51:04
Epoch: [68/150][20/185]	Time 0.385 (0.456)	Data 0.000 (0.024)	Loss_t 0.2300 (0.1487)	Loss_x 1.2760 (1.2656)	Acc 100.00 (98.20)	Lr 0.000350	eta 1:56:32
Epoch: [68/150][40/185]	Time 0.416 (0.448)	Data 0.000 (0.012)	Loss_t 0.0950 (0.1317)	Loss_x 1.2224 (1.2510)	Acc 96.88 (98.32)	Lr 0.000350	eta 1:54:14
Epoch: [68/150][60/185]	Time 0.469 (0.450)	Data 0.000 (0.008)	Loss_t 0.1295 (0.1301)	Loss_x 1.2183 (1.2461)	Acc 95.31 (98.23)	Lr 0.000350	eta 1:54:49
Epoch: [68/150][80/185]	Time 0.438 (0.448)	Data 0.000 (0.006)	Loss_t 0.1529 (0.1351)	Loss_x 1.1957 (1.2392)	Acc 98.44 (98.46)	Lr 0.000350	eta 1:53:59
Epoch: [68/150][100/185]	Time 0.402 (0.443)	Data 0.000 (0.005)	Loss_t 0.2158 (0.1401)	Loss_x 1.2988 (1.2403)	Acc 98.44 (98.56)	Lr 0.000350	eta 1:52:43
Epoch: [68/150][120/185]	Time 0.441 (0.444)	Data 0.000 (0.004)	Loss_t 0.0967 (0.1388)	Loss_x 1.1893 (1.2388)	Acc 100.00 (98.55)	Lr 0.000350	eta 1:52:43
Epoch: [68/150][140/185]	Time 0.460 (0.444)	Data 0.000 (0.004)	Loss_t 0.0890 (0.1372)	Loss_x 1.2752 (1.2350)	Acc 98.44 (98.63)	Lr 0.000350	eta 1:52:41
Epoch: [68/150][160/185]	Time 0.443 (0.446)	Data 0.000 (0.003)	Loss_t 0.0403 (0.1351)	Loss_x 1.1257 (1.2289)	Acc 100.00 (98.66)	Lr 0.000350	eta 1:52:53
Epoch: [68/150][180/185]	Time 0.447 (0.446)	Data 0.000 (0.003)	Loss_t 0.0902 (0.1334)	Loss_x 1.1677 (1.2234)	Acc 100.00 (98.69)	Lr 0.000350	eta 1:52:52
Epoch: [69/150][20/185]	Time 0.441 (0.477)	Data 0.000 (0.026)	Loss_t 0.0982 (0.1369)	Loss_x 1.3103 (1.2717)	Acc 93.75 (97.34)	Lr 0.000350	eta 2:00:27
Epoch: [69/150][40/185]	Time 0.413 (0.453)	Data 0.000 (0.013)	Loss_t 0.1101 (0.1402)	Loss_x 1.2264 (1.2638)	Acc 100.00 (97.73)	Lr 0.000350	eta 1:54:19
Epoch: [69/150][60/185]	Time 0.441 (0.445)	Data 0.000 (0.009)	Loss_t 0.1060 (0.1385)	Loss_x 1.2579 (1.2568)	Acc 96.88 (98.02)	Lr 0.000350	eta 1:51:59
Epoch: [69/150][80/185]	Time 0.437 (0.443)	Data 0.000 (0.007)	Loss_t 0.0786 (0.1370)	Loss_x 1.2091 (1.2533)	Acc 100.00 (98.07)	Lr 0.000350	eta 1:51:29
Epoch: [69/150][100/185]	Time 0.454 (0.442)	Data 0.000 (0.005)	Loss_t 0.1385 (0.1393)	Loss_x 1.2198 (1.2503)	Acc 100.00 (98.17)	Lr 0.000350	eta 1:51:05
Epoch: [69/150][120/185]	Time 0.440 (0.443)	Data 0.000 (0.004)	Loss_t 0.1074 (0.1357)	Loss_x 1.2625 (1.2446)	Acc 92.19 (98.24)	Lr 0.000350	eta 1:51:02
Epoch: [69/150][140/185]	Time 0.445 (0.442)	Data 0.000 (0.004)	Loss_t 0.0926 (0.1352)	Loss_x 1.1833 (1.2406)	Acc 98.44 (98.38)	Lr 0.000350	eta 1:50:48
Epoch: [69/150][160/185]	Time 0.440 (0.442)	Data 0.000 (0.003)	Loss_t 0.1385 (0.1354)	Loss_x 1.1867 (1.2353)	Acc 100.00 (98.46)	Lr 0.000350	eta 1:50:31
Epoch: [69/150][180/185]	Time 0.433 (0.441)	Data 0.000 (0.003)	Loss_t 0.0621 (0.1333)	Loss_x 1.1154 (1.2266)	Acc 100.00 (98.59)	Lr 0.000350	eta 1:50:10
Epoch: [70/150][20/185]	Time 0.461 (0.466)	Data 0.000 (0.024)	Loss_t 0.1339 (0.1267)	Loss_x 1.2824 (1.2595)	Acc 100.00 (98.98)	Lr 0.000350	eta 1:56:13
Epoch: [70/150][40/185]	Time 0.437 (0.443)	Data 0.000 (0.012)	Loss_t 0.1203 (0.1223)	Loss_x 1.3125 (1.2536)	Acc 95.31 (98.71)	Lr 0.000350	eta 1:50:17
Epoch: [70/150][60/185]	Time 0.431 (0.433)	Data 0.000 (0.008)	Loss_t 0.1054 (0.1332)	Loss_x 1.2104 (1.2479)	Acc 100.00 (98.80)	Lr 0.000350	eta 1:47:39
Epoch: [70/150][80/185]	Time 0.375 (0.425)	Data 0.000 (0.006)	Loss_t 0.0769 (0.1321)	Loss_x 1.2611 (1.2444)	Acc 98.44 (98.75)	Lr 0.000350	eta 1:45:33
Epoch: [70/150][100/185]	Time 0.399 (0.422)	Data 0.000 (0.005)	Loss_t 0.1846 (0.1324)	Loss_x 1.3432 (1.2420)	Acc 95.31 (98.72)	Lr 0.000350	eta 1:44:48
Epoch: [70/150][120/185]	Time 0.408 (0.422)	Data 0.000 (0.004)	Loss_t 0.1177 (0.1313)	Loss_x 1.2177 (1.2389)	Acc 98.44 (98.68)	Lr 0.000350	eta 1:44:27
Epoch: [70/150][140/185]	Time 0.388 (0.418)	Data 0.000 (0.004)	Loss_t 0.0908 (0.1297)	Loss_x 1.1530 (1.2331)	Acc 100.00 (98.73)	Lr 0.000350	eta 1:43:31
Epoch: [70/150][160/185]	Time 0.375 (0.416)	Data 0.000 (0.003)	Loss_t 0.1723 (0.1303)	Loss_x 1.2217 (1.2291)	Acc 98.44 (98.75)	Lr 0.000350	eta 1:42:42
Epoch: [70/150][180/185]	Time 0.440 (0.414)	Data 0.000 (0.003)	Loss_t 0.0891 (0.1317)	Loss_x 1.1577 (1.2226)	Acc 100.00 (98.79)	Lr 0.000350	eta 1:42:16
##### Evaluating market1501 (source) #####
Extracting features from query set ...
Done, obtained 3368-by-4608 matrix
Extracting features from gallery set ...
Done, obtained 15913-by-4608 matrix
Speed: 0.0301 sec/batch
Computing distance matrix with metric=euclidean ...
Computing CMC and mAP ...
** Results **
mAP: 82.3%
CMC curve
Rank-1  : 93.2%
Rank-5  : 97.6%
Rank-10 : 98.5%
Rank-20 : 98.8%
Checkpoint saved to "log/model.pth.tar-70"
Epoch: [71/150][20/185]	Time 0.366 (0.434)	Data 0.000 (0.028)	Loss_t 0.2043 (0.1485)	Loss_x 1.3676 (1.2680)	Acc 98.44 (99.06)	Lr 0.000350	eta 1:46:59
Epoch: [71/150][40/185]	Time 0.360 (0.415)	Data 0.000 (0.014)	Loss_t 0.1836 (0.1401)	Loss_x 1.3190 (1.2500)	Acc 98.44 (99.06)	Lr 0.000350	eta 1:42:03
Epoch: [71/150][60/185]	Time 0.375 (0.410)	Data 0.000 (0.009)	Loss_t 0.1785 (0.1363)	Loss_x 1.2919 (1.2447)	Acc 96.88 (98.96)	Lr 0.000350	eta 1:40:47
Epoch: [71/150][80/185]	Time 0.426 (0.410)	Data 0.000 (0.007)	Loss_t 0.0952 (0.1343)	Loss_x 1.1658 (1.2389)	Acc 100.00 (98.83)	Lr 0.000350	eta 1:40:30
Epoch: [71/150][100/185]	Time 0.411 (0.412)	Data 0.000 (0.006)	Loss_t 0.1497 (0.1292)	Loss_x 1.2340 (1.2320)	Acc 98.44 (98.88)	Lr 0.000350	eta 1:40:54
Epoch: [71/150][120/185]	Time 0.429 (0.415)	Data 0.000 (0.005)	Loss_t 0.0733 (0.1298)	Loss_x 1.2025 (1.2298)	Acc 100.00 (98.83)	Lr 0.000350	eta 1:41:28
Epoch: [71/150][140/185]	Time 0.460 (0.418)	Data 0.000 (0.004)	Loss_t 0.1302 (0.1296)	Loss_x 1.1739 (1.2250)	Acc 100.00 (98.86)	Lr 0.000350	eta 1:42:05
Epoch: [71/150][160/185]	Time 0.445 (0.420)	Data 0.000 (0.004)	Loss_t 0.0340 (0.1295)	Loss_x 1.1664 (1.2206)	Acc 98.44 (98.88)	Lr 0.000350	eta 1:42:29
Epoch: [71/150][180/185]	Time 0.425 (0.420)	Data 0.000 (0.003)	Loss_t 0.0490 (0.1300)	Loss_x 1.1308 (1.2149)	Acc 100.00 (98.90)	Lr 0.000350	eta 1:42:21
Epoch: [72/150][20/185]	Time 0.414 (0.447)	Data 0.000 (0.024)	Loss_t 0.1745 (0.1507)	Loss_x 1.2562 (1.2549)	Acc 100.00 (98.59)	Lr 0.000350	eta 1:48:50
Epoch: [72/150][40/185]	Time 0.419 (0.430)	Data 0.000 (0.012)	Loss_t 0.1540 (0.1369)	Loss_x 1.2380 (1.2469)	Acc 96.88 (98.59)	Lr 0.000350	eta 1:44:32
Epoch: [72/150][60/185]	Time 0.417 (0.428)	Data 0.000 (0.008)	Loss_t 0.1215 (0.1364)	Loss_x 1.2362 (1.2463)	Acc 98.44 (98.57)	Lr 0.000350	eta 1:43:50
Epoch: [72/150][80/185]	Time 0.387 (0.424)	Data 0.000 (0.006)	Loss_t 0.0498 (0.1318)	Loss_x 1.1764 (1.2413)	Acc 100.00 (98.69)	Lr 0.000350	eta 1:42:41
Epoch: [72/150][100/185]	Time 0.452 (0.425)	Data 0.000 (0.005)	Loss_t 0.1469 (0.1299)	Loss_x 1.2303 (1.2373)	Acc 98.44 (98.77)	Lr 0.000350	eta 1:42:49
Epoch: [72/150][120/185]	Time 0.412 (0.425)	Data 0.000 (0.004)	Loss_t 0.1728 (0.1315)	Loss_x 1.1755 (1.2339)	Acc 100.00 (98.78)	Lr 0.000350	eta 1:42:36
Epoch: [72/150][140/185]	Time 0.443 (0.424)	Data 0.000 (0.004)	Loss_t 0.1445 (0.1324)	Loss_x 1.2522 (1.2304)	Acc 96.88 (98.75)	Lr 0.000350	eta 1:42:18
Epoch: [72/150][160/185]	Time 0.395 (0.423)	Data 0.000 (0.003)	Loss_t 0.0664 (0.1298)	Loss_x 1.1417 (1.2247)	Acc 100.00 (98.81)	Lr 0.000350	eta 1:41:54
Epoch: [72/150][180/185]	Time 0.409 (0.423)	Data 0.000 (0.003)	Loss_t 0.2055 (0.1295)	Loss_x 1.1524 (1.2192)	Acc 100.00 (98.82)	Lr 0.000350	eta 1:41:53
Epoch: [73/150][20/185]	Time 0.467 (0.456)	Data 0.000 (0.024)	Loss_t 0.2281 (0.1518)	Loss_x 1.2428 (1.2786)	Acc 100.00 (97.81)	Lr 0.000350	eta 1:49:35
Epoch: [73/150][40/185]	Time 0.471 (0.442)	Data 0.000 (0.012)	Loss_t 0.0959 (0.1474)	Loss_x 1.2419 (1.2650)	Acc 98.44 (98.12)	Lr 0.000350	eta 1:46:04
Epoch: [73/150][60/185]	Time 0.350 (0.435)	Data 0.000 (0.008)	Loss_t 0.0676 (0.1350)	Loss_x 1.2634 (1.2531)	Acc 98.44 (98.33)	Lr 0.000350	eta 1:44:11
Epoch: [73/150][80/185]	Time 0.397 (0.421)	Data 0.000 (0.006)	Loss_t 0.1139 (0.1285)	Loss_x 1.2411 (1.2436)	Acc 100.00 (98.52)	Lr 0.000350	eta 1:40:40
Epoch: [73/150][100/185]	Time 0.413 (0.418)	Data 0.000 (0.005)	Loss_t 0.0765 (0.1281)	Loss_x 1.2055 (1.2373)	Acc 98.44 (98.69)	Lr 0.000350	eta 1:39:50
Epoch: [73/150][120/185]	Time 0.433 (0.417)	Data 0.000 (0.004)	Loss_t 0.0772 (0.1259)	Loss_x 1.2033 (1.2324)	Acc 100.00 (98.82)	Lr 0.000350	eta 1:39:23
Epoch: [73/150][140/185]	Time 0.432 (0.417)	Data 0.000 (0.004)	Loss_t 0.1611 (0.1267)	Loss_x 1.1791 (1.2306)	Acc 100.00 (98.68)	Lr 0.000350	eta 1:39:12
Epoch: [73/150][160/185]	Time 0.422 (0.415)	Data 0.000 (0.003)	Loss_t 0.1324 (0.1249)	Loss_x 1.2047 (1.2230)	Acc 100.00 (98.81)	Lr 0.000350	eta 1:38:36
Epoch: [73/150][180/185]	Time 0.411 (0.415)	Data 0.000 (0.003)	Loss_t 0.0894 (0.1248)	Loss_x 1.1576 (1.2163)	Acc 100.00 (98.83)	Lr 0.000350	eta 1:38:34
Epoch: [74/150][20/185]	Time 0.441 (0.452)	Data 0.000 (0.025)	Loss_t 0.2402 (0.1618)	Loss_x 1.2967 (1.2813)	Acc 96.88 (97.58)	Lr 0.000350	eta 1:47:07
Epoch: [74/150][40/185]	Time 0.426 (0.448)	Data 0.000 (0.013)	Loss_t 0.0566 (0.1444)	Loss_x 1.1764 (1.2653)	Acc 100.00 (98.55)	Lr 0.000350	eta 1:46:03
Epoch: [74/150][60/185]	Time 0.445 (0.444)	Data 0.000 (0.008)	Loss_t 0.1232 (0.1384)	Loss_x 1.2478 (1.2550)	Acc 98.44 (98.65)	Lr 0.000350	eta 1:44:58
Epoch: [74/150][80/185]	Time 0.432 (0.442)	Data 0.000 (0.006)	Loss_t 0.1941 (0.1324)	Loss_x 1.2163 (1.2462)	Acc 98.44 (98.73)	Lr 0.000350	eta 1:44:16
Epoch: [74/150][100/185]	Time 0.423 (0.436)	Data 0.000 (0.005)	Loss_t 0.0993 (0.1309)	Loss_x 1.2371 (1.2406)	Acc 100.00 (98.80)	Lr 0.000350	eta 1:42:43
Epoch: [74/150][120/185]	Time 0.420 (0.432)	Data 0.000 (0.004)	Loss_t 0.1341 (0.1305)	Loss_x 1.2052 (1.2347)	Acc 98.44 (98.87)	Lr 0.000350	eta 1:41:42
Epoch: [74/150][140/185]	Time 0.397 (0.429)	Data 0.000 (0.004)	Loss_t 0.1259 (0.1268)	Loss_x 1.1683 (1.2285)	Acc 98.44 (98.93)	Lr 0.000350	eta 1:40:47
Epoch: [74/150][160/185]	Time 0.400 (0.426)	Data 0.000 (0.003)	Loss_t 0.0839 (0.1252)	Loss_x 1.1955 (1.2231)	Acc 98.44 (98.97)	Lr 0.000350	eta 1:40:06
Epoch: [74/150][180/185]	Time 0.417 (0.427)	Data 0.000 (0.003)	Loss_t 0.1155 (0.1245)	Loss_x 1.1331 (1.2175)	Acc 100.00 (98.97)	Lr 0.000350	eta 1:40:02
Epoch: [75/150][20/185]	Time 0.438 (0.454)	Data 0.000 (0.024)	Loss_t 0.0907 (0.1332)	Loss_x 1.2145 (1.2727)	Acc 100.00 (98.52)	Lr 0.000350	eta 1:46:12
Epoch: [75/150][40/185]	Time 0.424 (0.438)	Data 0.000 (0.012)	Loss_t 0.1128 (0.1310)	Loss_x 1.2166 (1.2599)	Acc 98.44 (98.95)	Lr 0.000350	eta 1:42:18
Epoch: [75/150][60/185]	Time 0.435 (0.437)	Data 0.000 (0.008)	Loss_t 0.0852 (0.1197)	Loss_x 1.1783 (1.2418)	Acc 98.44 (99.06)	Lr 0.000350	eta 1:42:02
Epoch: [75/150][80/185]	Time 0.407 (0.433)	Data 0.000 (0.006)	Loss_t 0.0554 (0.1190)	Loss_x 1.1428 (1.2352)	Acc 100.00 (98.93)	Lr 0.000350	eta 1:40:49
Epoch: [75/150][100/185]	Time 0.430 (0.429)	Data 0.000 (0.005)	Loss_t 0.0567 (0.1189)	Loss_x 1.1939 (1.2308)	Acc 100.00 (98.95)	Lr 0.000350	eta 1:39:48
Epoch: [75/150][120/185]	Time 0.396 (0.427)	Data 0.000 (0.004)	Loss_t 0.1140 (0.1199)	Loss_x 1.2210 (1.2260)	Acc 100.00 (99.02)	Lr 0.000350	eta 1:39:06
Epoch: [75/150][140/185]	Time 0.423 (0.426)	Data 0.000 (0.004)	Loss_t 0.0963 (0.1214)	Loss_x 1.2162 (1.2241)	Acc 100.00 (98.97)	Lr 0.000350	eta 1:38:51
Epoch: [75/150][160/185]	Time 0.413 (0.425)	Data 0.000 (0.003)	Loss_t 0.1240 (0.1206)	Loss_x 1.2162 (1.2207)	Acc 100.00 (98.99)	Lr 0.000350	eta 1:38:32
Epoch: [75/150][180/185]	Time 0.420 (0.424)	Data 0.000 (0.003)	Loss_t 0.0903 (0.1219)	Loss_x 1.1385 (1.2154)	Acc 98.44 (98.96)	Lr 0.000350	eta 1:38:08
Epoch: [76/150][20/185]	Time 0.418 (0.455)	Data 0.000 (0.025)	Loss_t 0.1453 (0.1332)	Loss_x 1.2748 (1.2796)	Acc 98.44 (97.66)	Lr 0.000350	eta 1:45:09
Epoch: [76/150][40/185]	Time 0.429 (0.436)	Data 0.000 (0.013)	Loss_t 0.0772 (0.1312)	Loss_x 1.2475 (1.2602)	Acc 100.00 (98.01)	Lr 0.000350	eta 1:40:28
Epoch: [76/150][60/185]	Time 0.431 (0.430)	Data 0.000 (0.009)	Loss_t 0.1327 (0.1282)	Loss_x 1.2099 (1.2538)	Acc 98.44 (98.20)	Lr 0.000350	eta 1:38:58
Epoch: [76/150][80/185]	Time 0.440 (0.431)	Data 0.000 (0.006)	Loss_t 0.0680 (0.1269)	Loss_x 1.1764 (1.2471)	Acc 98.44 (98.38)	Lr 0.000350	eta 1:39:01
Epoch: [76/150][100/185]	Time 0.437 (0.432)	Data 0.000 (0.005)	Loss_t 0.1533 (0.1237)	Loss_x 1.1887 (1.2394)	Acc 100.00 (98.53)	Lr 0.000350	eta 1:39:13
Epoch: [76/150][120/185]	Time 0.447 (0.432)	Data 0.000 (0.004)	Loss_t 0.0829 (0.1206)	Loss_x 1.1547 (1.2335)	Acc 100.00 (98.59)	Lr 0.000350	eta 1:39:08
Epoch: [76/150][140/185]	Time 0.416 (0.432)	Data 0.000 (0.004)	Loss_t 0.0910 (0.1194)	Loss_x 1.1972 (1.2283)	Acc 98.44 (98.72)	Lr 0.000350	eta 1:38:50
Epoch: [76/150][160/185]	Time 0.440 (0.432)	Data 0.000 (0.003)	Loss_t 0.1668 (0.1204)	Loss_x 1.2569 (1.2226)	Acc 96.88 (98.78)	Lr 0.000350	eta 1:38:50
Epoch: [76/150][180/185]	Time 0.431 (0.431)	Data 0.000 (0.003)	Loss_t 0.1019 (0.1189)	Loss_x 1.1265 (1.2164)	Acc 100.00 (98.80)	Lr 0.000350	eta 1:38:22
Epoch: [77/150][20/185]	Time 0.404 (0.460)	Data 0.000 (0.023)	Loss_t 0.1079 (0.1419)	Loss_x 1.2504 (1.2790)	Acc 98.44 (98.28)	Lr 0.000350	eta 1:44:51
Epoch: [77/150][40/185]	Time 0.445 (0.453)	Data 0.000 (0.012)	Loss_t 0.1991 (0.1321)	Loss_x 1.1694 (1.2481)	Acc 100.00 (98.83)	Lr 0.000350	eta 1:43:09
Epoch: [77/150][60/185]	Time 0.421 (0.450)	Data 0.000 (0.008)	Loss_t 0.2041 (0.1277)	Loss_x 1.2361 (1.2416)	Acc 100.00 (98.91)	Lr 0.000350	eta 1:42:07
Epoch: [77/150][80/185]	Time 0.479 (0.449)	Data 0.000 (0.006)	Loss_t 0.0941 (0.1275)	Loss_x 1.2398 (1.2387)	Acc 100.00 (99.02)	Lr 0.000350	eta 1:41:49
Epoch: [77/150][100/185]	Time 0.400 (0.447)	Data 0.000 (0.005)	Loss_t 0.1153 (0.1251)	Loss_x 1.1545 (1.2352)	Acc 100.00 (98.97)	Lr 0.000350	eta 1:41:12
Epoch: [77/150][120/185]	Time 0.403 (0.444)	Data 0.000 (0.004)	Loss_t 0.0641 (0.1219)	Loss_x 1.2232 (1.2296)	Acc 98.44 (98.98)	Lr 0.000350	eta 1:40:31
Epoch: [77/150][140/185]	Time 0.419 (0.442)	Data 0.000 (0.003)	Loss_t 0.1449 (0.1237)	Loss_x 1.2478 (1.2252)	Acc 98.44 (99.02)	Lr 0.000350	eta 1:39:52
Epoch: [77/150][160/185]	Time 0.432 (0.442)	Data 0.000 (0.003)	Loss_t 0.0696 (0.1210)	Loss_x 1.1466 (1.2204)	Acc 100.00 (99.03)	Lr 0.000350	eta 1:39:37
Epoch: [77/150][180/185]	Time 0.442 (0.441)	Data 0.000 (0.003)	Loss_t 0.1955 (0.1224)	Loss_x 1.1857 (1.2154)	Acc 100.00 (99.05)	Lr 0.000350	eta 1:39:14
Epoch: [78/150][20/185]	Time 0.456 (0.457)	Data 0.000 (0.025)	Loss_t 0.2259 (0.1084)	Loss_x 1.3056 (1.2429)	Acc 96.88 (98.59)	Lr 0.000350	eta 1:42:40
Epoch: [78/150][40/185]	Time 0.412 (0.441)	Data 0.000 (0.013)	Loss_t 0.1069 (0.1149)	Loss_x 1.1660 (1.2423)	Acc 100.00 (98.40)	Lr 0.000350	eta 1:38:55
Epoch: [78/150][60/185]	Time 0.443 (0.432)	Data 0.000 (0.008)	Loss_t 0.0952 (0.1118)	Loss_x 1.1747 (1.2315)	Acc 100.00 (98.70)	Lr 0.000350	eta 1:36:49
Epoch: [78/150][80/185]	Time 0.434 (0.431)	Data 0.000 (0.006)	Loss_t 0.1142 (0.1124)	Loss_x 1.2129 (1.2288)	Acc 100.00 (98.75)	Lr 0.000350	eta 1:36:29
Epoch: [78/150][100/185]	Time 0.436 (0.427)	Data 0.000 (0.005)	Loss_t 0.2356 (0.1216)	Loss_x 1.2337 (1.2280)	Acc 98.44 (98.73)	Lr 0.000350	eta 1:35:29
Epoch: [78/150][120/185]	Time 0.416 (0.427)	Data 0.000 (0.004)	Loss_t 0.1483 (0.1241)	Loss_x 1.2132 (1.2289)	Acc 98.44 (98.71)	Lr 0.000350	eta 1:35:18
Epoch: [78/150][140/185]	Time 0.489 (0.427)	Data 0.000 (0.004)	Loss_t 0.1313 (0.1239)	Loss_x 1.2190 (1.2260)	Acc 98.44 (98.68)	Lr 0.000350	eta 1:35:03
Epoch: [78/150][160/185]	Time 0.422 (0.426)	Data 0.000 (0.003)	Loss_t 0.1431 (0.1248)	Loss_x 1.2070 (1.2222)	Acc 95.31 (98.74)	Lr 0.000350	eta 1:34:45
Epoch: [78/150][180/185]	Time 0.427 (0.425)	Data 0.000 (0.003)	Loss_t 0.0518 (0.1217)	Loss_x 1.1425 (1.2158)	Acc 100.00 (98.79)	Lr 0.000350	eta 1:34:27
Epoch: [79/150][20/185]	Time 0.451 (0.450)	Data 0.000 (0.025)	Loss_t 0.0724 (0.1304)	Loss_x 1.1977 (1.2592)	Acc 100.00 (98.52)	Lr 0.000350	eta 1:39:44
Epoch: [79/150][40/185]	Time 0.474 (0.449)	Data 0.000 (0.012)	Loss_t 0.1781 (0.1316)	Loss_x 1.3424 (1.2529)	Acc 93.75 (98.44)	Lr 0.000350	eta 1:39:18
Epoch: [79/150][60/185]	Time 0.449 (0.445)	Data 0.000 (0.008)	Loss_t 0.0874 (0.1251)	Loss_x 1.2379 (1.2495)	Acc 100.00 (98.41)	Lr 0.000350	eta 1:38:14
Epoch: [79/150][80/185]	Time 0.411 (0.441)	Data 0.000 (0.006)	Loss_t 0.0872 (0.1234)	Loss_x 1.2168 (1.2420)	Acc 100.00 (98.61)	Lr 0.000350	eta 1:37:14
Epoch: [79/150][100/185]	Time 0.436 (0.435)	Data 0.000 (0.005)	Loss_t 0.1073 (0.1217)	Loss_x 1.3262 (1.2405)	Acc 93.75 (98.59)	Lr 0.000350	eta 1:35:56
Epoch: [79/150][120/185]	Time 0.416 (0.432)	Data 0.000 (0.004)	Loss_t 0.1031 (0.1219)	Loss_x 1.1681 (1.2372)	Acc 100.00 (98.63)	Lr 0.000350	eta 1:35:06
Epoch: [79/150][140/185]	Time 0.438 (0.429)	Data 0.000 (0.004)	Loss_t 0.0990 (0.1192)	Loss_x 1.1986 (1.2299)	Acc 98.44 (98.73)	Lr 0.000350	eta 1:34:12
Epoch: [79/150][160/185]	Time 0.483 (0.428)	Data 0.000 (0.003)	Loss_t 0.1018 (0.1194)	Loss_x 1.1277 (1.2244)	Acc 100.00 (98.80)	Lr 0.000350	eta 1:33:47
Epoch: [79/150][180/185]	Time 0.418 (0.427)	Data 0.000 (0.003)	Loss_t 0.0476 (0.1191)	Loss_x 1.1188 (1.2173)	Acc 100.00 (98.86)	Lr 0.000350	eta 1:33:32
Epoch: [80/150][20/185]	Time 0.384 (0.437)	Data 0.000 (0.024)	Loss_t 0.2098 (0.1288)	Loss_x 1.3669 (1.2564)	Acc 92.19 (98.28)	Lr 0.000350	eta 1:35:37
Epoch: [80/150][40/185]	Time 0.446 (0.431)	Data 0.000 (0.012)	Loss_t 0.0962 (0.1219)	Loss_x 1.2218 (1.2378)	Acc 98.44 (98.71)	Lr 0.000350	eta 1:34:01
Epoch: [80/150][60/185]	Time 0.439 (0.432)	Data 0.000 (0.008)	Loss_t 0.0792 (0.1179)	Loss_x 1.1791 (1.2311)	Acc 100.00 (98.70)	Lr 0.000350	eta 1:34:02
Epoch: [80/150][80/185]	Time 0.393 (0.428)	Data 0.000 (0.006)	Loss_t 0.1051 (0.1143)	Loss_x 1.2334 (1.2287)	Acc 96.88 (98.79)	Lr 0.000350	eta 1:33:07
Epoch: [80/150][100/185]	Time 0.432 (0.427)	Data 0.000 (0.005)	Loss_t 0.1317 (0.1138)	Loss_x 1.1893 (1.2235)	Acc 100.00 (98.86)	Lr 0.000350	eta 1:32:39
Epoch: [80/150][120/185]	Time 0.402 (0.426)	Data 0.000 (0.004)	Loss_t 0.1796 (0.1157)	Loss_x 1.3002 (1.2216)	Acc 95.31 (98.89)	Lr 0.000350	eta 1:32:18
Epoch: [80/150][140/185]	Time 0.412 (0.426)	Data 0.000 (0.004)	Loss_t 0.0838 (0.1151)	Loss_x 1.2096 (1.2162)	Acc 100.00 (98.98)	Lr 0.000350	eta 1:32:13
Epoch: [80/150][160/185]	Time 0.397 (0.423)	Data 0.000 (0.003)	Loss_t 0.0812 (0.1134)	Loss_x 1.1760 (1.2131)	Acc 98.44 (98.94)	Lr 0.000350	eta 1:31:22
Epoch: [80/150][180/185]	Time 0.397 (0.421)	Data 0.000 (0.003)	Loss_t 0.0782 (0.1144)	Loss_x 1.1136 (1.2069)	Acc 100.00 (98.98)	Lr 0.000350	eta 1:30:54
##### Evaluating market1501 (source) #####
Extracting features from query set ...
Done, obtained 3368-by-4608 matrix
Extracting features from gallery set ...
Done, obtained 15913-by-4608 matrix
Speed: 0.0354 sec/batch
Computing distance matrix with metric=euclidean ...
Computing CMC and mAP ...
** Results **
mAP: 80.1%
CMC curve
Rank-1  : 91.7%
Rank-5  : 97.1%
Rank-10 : 98.2%
Rank-20 : 98.9%
Checkpoint saved to "log/model.pth.tar-80"
Epoch: [81/150][20/185]	Time 0.370 (0.408)	Data 0.000 (0.027)	Loss_t 0.1528 (0.1274)	Loss_x 1.1978 (1.2691)	Acc 98.44 (98.12)	Lr 0.000350	eta 1:28:00
Epoch: [81/150][40/185]	Time 0.379 (0.416)	Data 0.000 (0.014)	Loss_t 0.2288 (0.1333)	Loss_x 1.2804 (1.2568)	Acc 100.00 (98.52)	Lr 0.000350	eta 1:29:32
Epoch: [81/150][60/185]	Time 0.440 (0.420)	Data 0.000 (0.009)	Loss_t 0.2852 (0.1319)	Loss_x 1.3592 (1.2535)	Acc 95.31 (98.46)	Lr 0.000350	eta 1:30:15
Epoch: [81/150][80/185]	Time 0.434 (0.423)	Data 0.000 (0.007)	Loss_t 0.1146 (0.1318)	Loss_x 1.2763 (1.2507)	Acc 98.44 (98.50)	Lr 0.000350	eta 1:30:41
Epoch: [81/150][100/185]	Time 0.431 (0.427)	Data 0.000 (0.006)	Loss_t 0.1917 (0.1282)	Loss_x 1.2444 (1.2435)	Acc 100.00 (98.59)	Lr 0.000350	eta 1:31:26
Epoch: [81/150][120/185]	Time 0.434 (0.427)	Data 0.000 (0.005)	Loss_t 0.0668 (0.1257)	Loss_x 1.2500 (1.2412)	Acc 93.75 (98.57)	Lr 0.000350	eta 1:31:15
Epoch: [81/150][140/185]	Time 0.427 (0.422)	Data 0.000 (0.004)	Loss_t 0.2161 (0.1290)	Loss_x 1.1867 (1.2368)	Acc 100.00 (98.66)	Lr 0.000350	eta 1:30:10
Epoch: [81/150][160/185]	Time 0.434 (0.419)	Data 0.000 (0.004)	Loss_t 0.1537 (0.1300)	Loss_x 1.1494 (1.2318)	Acc 100.00 (98.68)	Lr 0.000350	eta 1:29:23
Epoch: [81/150][180/185]	Time 0.417 (0.419)	Data 0.000 (0.003)	Loss_t 0.1778 (0.1282)	Loss_x 1.1683 (1.2252)	Acc 100.00 (98.78)	Lr 0.000350	eta 1:29:07
Epoch: [82/150][20/185]	Time 0.439 (0.456)	Data 0.000 (0.024)	Loss_t 0.1960 (0.1288)	Loss_x 1.3823 (1.2713)	Acc 98.44 (98.59)	Lr 0.000350	eta 1:36:50
Epoch: [82/150][40/185]	Time 0.394 (0.448)	Data 0.000 (0.012)	Loss_t 0.1421 (0.1195)	Loss_x 1.3114 (1.2593)	Acc 90.62 (98.09)	Lr 0.000350	eta 1:35:03
Epoch: [82/150][60/185]	Time 0.455 (0.444)	Data 0.000 (0.008)	Loss_t 0.0644 (0.1188)	Loss_x 1.1959 (1.2493)	Acc 98.44 (98.18)	Lr 0.000350	eta 1:34:04
Epoch: [82/150][80/185]	Time 0.450 (0.439)	Data 0.000 (0.006)	Loss_t 0.1050 (0.1178)	Loss_x 1.1633 (1.2375)	Acc 100.00 (98.48)	Lr 0.000350	eta 1:32:53
Epoch: [82/150][100/185]	Time 0.405 (0.436)	Data 0.000 (0.005)	Loss_t 0.0762 (0.1194)	Loss_x 1.2295 (1.2331)	Acc 98.44 (98.56)	Lr 0.000350	eta 1:32:06
