Show configuration
adam:
  beta1: 0.9
  beta2: 0.999
cuhk03:
  classic_split: False
  labeled_images: True
  use_metric_cuhk03: False
data:
  combineall: False
  height: 256
  norm_mean: [0.485, 0.456, 0.406]
  norm_std: [0.229, 0.224, 0.225]
  root: /home/s2019020843/changeeeee/data/
  save_dir: log
  sources: ['market1501']
  split_id: 0
  targets: ['market1501']
  transforms: ['random_flip', 'random_erase']
  type: image
  width: 128
  workers: 4
loss:
  name: triplet
  softmax:
    label_smooth: True
  triplet:
    margin: 0.0
    weight_t: 1.0
    weight_x: 1.0
market1501:
  use_500k_distractors: False
model:
  load_weights: 
  name: plr_osnet
  pretrained: True
  resume: 
rmsprop:
  alpha: 0.99
sampler:
  num_instances: 4
  train_sampler: RandomIdentitySampler
sgd:
  dampening: 0.0
  momentum: 0.9
  nesterov: False
test:
  batch_size: 300
  dist_metric: euclidean
  eval_freq: 10
  evaluate: False
  normalize_feature: False
  ranks: [1, 5, 10, 20]
  rerank: False
  start_eval: 0
  visactmap: False
  visrank: False
  visrank_topk: 10
train:
  base_lr_mult: 0.1
  batch_size: 64
  fixbase_epoch: 0
  gamma: 0.1
  lr: 3.5e-05
  lr_scheduler: warmup
  max_epoch: 150
  multiplier: 10
  new_layers: ['classifier']
  open_layers: ['classifier']
  optim: adam
  print_freq: 20
  seed: 1
  staged_lr: False
  start_epoch: 0
  stepsize: [60, 90]
  total_epoch: 39
  weight_decay: 0.0005
use_gpu: True
video:
  pooling_method: avg
  sample_method: evenly
  seq_len: 15

Collecting env info ...
** System info **
PyTorch version: 1.5.1
Is debug build: No
CUDA used to build PyTorch: 10.2

OS: CentOS Linux 7 (Core)
GCC version: (GCC) 4.8.5 20150623 (Red Hat 4.8.5-39)
CMake version: Could not collect

Python version: 3.7
Is CUDA available: Yes
CUDA runtime version: 10.2.89
GPU models and configuration: 
GPU 0: GeForce RTX 2080 Ti
GPU 1: GeForce RTX 2080 Ti
GPU 2: GeForce RTX 2080 Ti
GPU 3: GeForce RTX 2080 Ti

Nvidia driver version: 440.82
cuDNN version: Could not collect

Versions of relevant libraries:
[pip3] numpy==1.19.0
[pip3] torch==1.5.1
[pip3] torchvision==0.6.1
[conda] torch                     1.5.1                     <pip>
[conda] torchvision               0.6.1                     <pip>
        Pillow (7.1.2)

Building train transforms ...
+ resize to 256x128
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
+ random erase
Building test transforms ...
+ resize to 256x128
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
=> Loading train (source) dataset
=> Loaded Market1501
  ----------------------------------------
  subset   | # ids | # images | # cameras
  ----------------------------------------
  train    |   751 |    12936 |         6
  query    |   750 |     3368 |         6
  gallery  |   751 |    15913 |         6
  ----------------------------------------
=> Loading test (target) dataset
=> Loaded Market1501
  ----------------------------------------
  subset   | # ids | # images | # cameras
  ----------------------------------------
  train    |   751 |    12936 |         6
  query    |   750 |     3368 |         6
  gallery  |   751 |    15913 |         6
  ----------------------------------------


  **************** Summary ****************
  train            : ['market1501']
  # train datasets : 1
  # train ids      : 751
  # train images   : 12936
  # train cameras  : 6
  test             : ['market1501']
  *****************************************


Building model: plr_osnet
Successfully loaded imagenet pretrained weights from "/home/s2019020843/.cache/torch/checkpoints/osnet_x1_0_imagenet.pth"
Model complexity: params=2,529,488 flops=1,071,153,040
Model structure: PLR_OSNet(
  (layer0): Sequential(
    (0): ConvLayer(
      (conv): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (1): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
  )
  (layer10): Sequential(
    (0): OSBlock(
      (conv1): Conv1x1(
        (conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (conv2a): LightConv3x3(
        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)
        (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (conv2b): Sequential(
        (0): LightConv3x3(
          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)
          (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (1): LightConv3x3(
          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)
          (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
      )
      (conv2c): Sequential(
        (0): LightConv3x3(
          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)
          (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (1): LightConv3x3(
          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)
          (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (2): LightConv3x3(
          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)
          (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
      )
      (conv2d): Sequential(
        (0): LightConv3x3(
          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)
          (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (1): LightConv3x3(
          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)
          (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (2): LightConv3x3(
          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)
          (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (3): LightConv3x3(
          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)
          (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
      )
      (gate): ChannelGate(
        (global_avgpool): AdaptiveAvgPool2d(output_size=1)
        (fc1): Conv2d(64, 4, kernel_size=(1, 1), stride=(1, 1))
        (relu): ReLU(inplace=True)
        (fc2): Conv2d(4, 64, kernel_size=(1, 1), stride=(1, 1))
        (gate_activation): Sigmoid()
      )
      (conv3): Conv1x1Linear(
        (conv): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (downsample): Conv1x1Linear(
        (conv): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): OSBlock(
      (conv1): Conv1x1(
        (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (conv2a): LightConv3x3(
        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)
        (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (conv2b): Sequential(
        (0): LightConv3x3(
          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)
          (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (1): LightConv3x3(
          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)
          (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
      )
      (conv2c): Sequential(
        (0): LightConv3x3(
          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)
          (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (1): LightConv3x3(
          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)
          (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (2): LightConv3x3(
          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)
          (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
      )
      (conv2d): Sequential(
        (0): LightConv3x3(
          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)
          (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (1): LightConv3x3(
          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)
          (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (2): LightConv3x3(
          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)
          (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (3): LightConv3x3(
          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)
          (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
      )
      (gate): ChannelGate(
        (global_avgpool): AdaptiveAvgPool2d(output_size=1)
        (fc1): Conv2d(64, 4, kernel_size=(1, 1), stride=(1, 1))
        (relu): ReLU(inplace=True)
        (fc2): Conv2d(4, 64, kernel_size=(1, 1), stride=(1, 1))
        (gate_activation): Sigmoid()
      )
      (conv3): Conv1x1Linear(
        (conv): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (2): Sequential(
      (0): Conv1x1(
        (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (1): AvgPool2d(kernel_size=2, stride=2, padding=0)
    )
  )
  (layer11): Sequential(
    (0): Sequential(
      (0): OSBlock(
        (conv1): Conv1x1(
          (conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (conv2a): LightConv3x3(
          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)
          (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (conv2b): Sequential(
          (0): LightConv3x3(
            (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)
            (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (1): LightConv3x3(
            (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)
            (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
        )
        (conv2c): Sequential(
          (0): LightConv3x3(
            (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)
            (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (1): LightConv3x3(
            (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)
            (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (2): LightConv3x3(
            (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)
            (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
        )
        (conv2d): Sequential(
          (0): LightConv3x3(
            (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)
            (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (1): LightConv3x3(
            (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)
            (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (2): LightConv3x3(
            (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)
            (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (3): LightConv3x3(
            (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)
            (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
        )
        (gate): ChannelGate(
          (global_avgpool): AdaptiveAvgPool2d(output_size=1)
          (fc1): Conv2d(64, 4, kernel_size=(1, 1), stride=(1, 1))
          (relu): ReLU(inplace=True)
          (fc2): Conv2d(4, 64, kernel_size=(1, 1), stride=(1, 1))
          (gate_activation): Sigmoid()
        )
        (conv3): Conv1x1Linear(
          (conv): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (downsample): Conv1x1Linear(
          (conv): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): OSBlock(
        (conv1): Conv1x1(
          (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (conv2a): LightConv3x3(
          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)
          (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (conv2b): Sequential(
          (0): LightConv3x3(
            (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)
            (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (1): LightConv3x3(
            (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)
            (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
        )
        (conv2c): Sequential(
          (0): LightConv3x3(
            (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)
            (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (1): LightConv3x3(
            (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)
            (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (2): LightConv3x3(
            (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)
            (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
        )
        (conv2d): Sequential(
          (0): LightConv3x3(
            (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)
            (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (1): LightConv3x3(
            (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)
            (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (2): LightConv3x3(
            (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)
            (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (3): LightConv3x3(
            (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)
            (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
        )
        (gate): ChannelGate(
          (global_avgpool): AdaptiveAvgPool2d(output_size=1)
          (fc1): Conv2d(64, 4, kernel_size=(1, 1), stride=(1, 1))
          (relu): ReLU(inplace=True)
          (fc2): Conv2d(4, 64, kernel_size=(1, 1), stride=(1, 1))
          (gate_activation): Sigmoid()
        )
        (conv3): Conv1x1Linear(
          (conv): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (2): Sequential(
        (0): Conv1x1(
          (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (1): AvgPool2d(kernel_size=2, stride=2, padding=0)
      )
    )
  )
  (layer20): Sequential(
    (0): OSBlock(
      (conv1): Conv1x1(
        (conv): Conv2d(256, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (conv2a): LightConv3x3(
        (conv1): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv2): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
        (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (conv2b): Sequential(
        (0): LightConv3x3(
          (conv1): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (conv2): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
          (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (1): LightConv3x3(
          (conv1): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (conv2): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
          (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
      )
      (conv2c): Sequential(
        (0): LightConv3x3(
          (conv1): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (conv2): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
          (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (1): LightConv3x3(
          (conv1): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (conv2): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
          (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (2): LightConv3x3(
          (conv1): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (conv2): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
          (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
      )
      (conv2d): Sequential(
        (0): LightConv3x3(
          (conv1): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (conv2): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
          (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (1): LightConv3x3(
          (conv1): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (conv2): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
          (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (2): LightConv3x3(
          (conv1): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (conv2): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
          (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (3): LightConv3x3(
          (conv1): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (conv2): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
          (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
      )
      (gate): ChannelGate(
        (global_avgpool): AdaptiveAvgPool2d(output_size=1)
        (fc1): Conv2d(96, 6, kernel_size=(1, 1), stride=(1, 1))
        (relu): ReLU(inplace=True)
        (fc2): Conv2d(6, 96, kernel_size=(1, 1), stride=(1, 1))
        (gate_activation): Sigmoid()
      )
      (conv3): Conv1x1Linear(
        (conv): Conv2d(96, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (downsample): Conv1x1Linear(
        (conv): Conv2d(256, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): OSBlock(
      (conv1): Conv1x1(
        (conv): Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (conv2a): LightConv3x3(
        (conv1): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv2): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
        (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (conv2b): Sequential(
        (0): LightConv3x3(
          (conv1): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (conv2): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
          (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (1): LightConv3x3(
          (conv1): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (conv2): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
          (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
      )
      (conv2c): Sequential(
        (0): LightConv3x3(
          (conv1): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (conv2): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
          (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (1): LightConv3x3(
          (conv1): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (conv2): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
          (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (2): LightConv3x3(
          (conv1): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (conv2): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
          (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
      )
      (conv2d): Sequential(
        (0): LightConv3x3(
          (conv1): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (conv2): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
          (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (1): LightConv3x3(
          (conv1): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (conv2): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
          (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (2): LightConv3x3(
          (conv1): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (conv2): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
          (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (3): LightConv3x3(
          (conv1): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (conv2): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
          (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
      )
      (gate): ChannelGate(
        (global_avgpool): AdaptiveAvgPool2d(output_size=1)
        (fc1): Conv2d(96, 6, kernel_size=(1, 1), stride=(1, 1))
        (relu): ReLU(inplace=True)
        (fc2): Conv2d(6, 96, kernel_size=(1, 1), stride=(1, 1))
        (gate_activation): Sigmoid()
      )
      (conv3): Conv1x1Linear(
        (conv): Conv2d(96, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (2): Sequential(
      (0): Conv1x1(
        (conv): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (1): AvgPool2d(kernel_size=2, stride=2, padding=0)
    )
  )
  (layer21): Sequential(
    (0): Sequential(
      (0): OSBlock(
        (conv1): Conv1x1(
          (conv): Conv2d(256, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (conv2a): LightConv3x3(
          (conv1): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (conv2): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
          (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (conv2b): Sequential(
          (0): LightConv3x3(
            (conv1): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (conv2): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (1): LightConv3x3(
            (conv1): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (conv2): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
        )
        (conv2c): Sequential(
          (0): LightConv3x3(
            (conv1): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (conv2): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (1): LightConv3x3(
            (conv1): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (conv2): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (2): LightConv3x3(
            (conv1): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (conv2): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
        )
        (conv2d): Sequential(
          (0): LightConv3x3(
            (conv1): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (conv2): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (1): LightConv3x3(
            (conv1): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (conv2): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (2): LightConv3x3(
            (conv1): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (conv2): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (3): LightConv3x3(
            (conv1): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (conv2): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
        )
        (gate): ChannelGate(
          (global_avgpool): AdaptiveAvgPool2d(output_size=1)
          (fc1): Conv2d(96, 6, kernel_size=(1, 1), stride=(1, 1))
          (relu): ReLU(inplace=True)
          (fc2): Conv2d(6, 96, kernel_size=(1, 1), stride=(1, 1))
          (gate_activation): Sigmoid()
        )
        (conv3): Conv1x1Linear(
          (conv): Conv2d(96, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (downsample): Conv1x1Linear(
          (conv): Conv2d(256, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): OSBlock(
        (conv1): Conv1x1(
          (conv): Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (conv2a): LightConv3x3(
          (conv1): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (conv2): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
          (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (conv2b): Sequential(
          (0): LightConv3x3(
            (conv1): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (conv2): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (1): LightConv3x3(
            (conv1): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (conv2): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
        )
        (conv2c): Sequential(
          (0): LightConv3x3(
            (conv1): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (conv2): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (1): LightConv3x3(
            (conv1): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (conv2): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (2): LightConv3x3(
            (conv1): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (conv2): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
        )
        (conv2d): Sequential(
          (0): LightConv3x3(
            (conv1): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (conv2): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (1): LightConv3x3(
            (conv1): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (conv2): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (2): LightConv3x3(
            (conv1): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (conv2): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (3): LightConv3x3(
            (conv1): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (conv2): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
        )
        (gate): ChannelGate(
          (global_avgpool): AdaptiveAvgPool2d(output_size=1)
          (fc1): Conv2d(96, 6, kernel_size=(1, 1), stride=(1, 1))
          (relu): ReLU(inplace=True)
          (fc2): Conv2d(6, 96, kernel_size=(1, 1), stride=(1, 1))
          (gate_activation): Sigmoid()
        )
        (conv3): Conv1x1Linear(
          (conv): Conv2d(96, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (2): Sequential(
        (0): Conv1x1(
          (conv): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (1): AvgPool2d(kernel_size=2, stride=2, padding=0)
      )
    )
  )
  (layer30): Sequential(
    (0): OSBlock(
      (conv1): Conv1x1(
        (conv): Conv2d(384, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (conv2a): LightConv3x3(
        (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)
        (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (conv2b): Sequential(
        (0): LightConv3x3(
          (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)
          (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (1): LightConv3x3(
          (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)
          (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
      )
      (conv2c): Sequential(
        (0): LightConv3x3(
          (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)
          (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (1): LightConv3x3(
          (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)
          (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (2): LightConv3x3(
          (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)
          (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
      )
      (conv2d): Sequential(
        (0): LightConv3x3(
          (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)
          (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (1): LightConv3x3(
          (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)
          (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (2): LightConv3x3(
          (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)
          (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (3): LightConv3x3(
          (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)
          (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
      )
      (gate): ChannelGate(
        (global_avgpool): AdaptiveAvgPool2d(output_size=1)
        (fc1): Conv2d(128, 8, kernel_size=(1, 1), stride=(1, 1))
        (relu): ReLU(inplace=True)
        (fc2): Conv2d(8, 128, kernel_size=(1, 1), stride=(1, 1))
        (gate_activation): Sigmoid()
      )
      (conv3): Conv1x1Linear(
        (conv): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (downsample): Conv1x1Linear(
        (conv): Conv2d(384, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): OSBlock(
      (conv1): Conv1x1(
        (conv): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (conv2a): LightConv3x3(
        (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)
        (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (conv2b): Sequential(
        (0): LightConv3x3(
          (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)
          (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (1): LightConv3x3(
          (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)
          (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
      )
      (conv2c): Sequential(
        (0): LightConv3x3(
          (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)
          (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (1): LightConv3x3(
          (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)
          (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (2): LightConv3x3(
          (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)
          (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
      )
      (conv2d): Sequential(
        (0): LightConv3x3(
          (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)
          (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (1): LightConv3x3(
          (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)
          (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (2): LightConv3x3(
          (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)
          (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (3): LightConv3x3(
          (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)
          (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
      )
      (gate): ChannelGate(
        (global_avgpool): AdaptiveAvgPool2d(output_size=1)
        (fc1): Conv2d(128, 8, kernel_size=(1, 1), stride=(1, 1))
        (relu): ReLU(inplace=True)
        (fc2): Conv2d(8, 128, kernel_size=(1, 1), stride=(1, 1))
        (gate_activation): Sigmoid()
      )
      (conv3): Conv1x1Linear(
        (conv): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (layer31): Sequential(
    (0): Sequential(
      (0): OSBlock(
        (conv1): Conv1x1(
          (conv): Conv2d(384, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (conv2a): LightConv3x3(
          (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)
          (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (conv2b): Sequential(
          (0): LightConv3x3(
            (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)
            (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (1): LightConv3x3(
            (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)
            (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
        )
        (conv2c): Sequential(
          (0): LightConv3x3(
            (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)
            (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (1): LightConv3x3(
            (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)
            (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (2): LightConv3x3(
            (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)
            (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
        )
        (conv2d): Sequential(
          (0): LightConv3x3(
            (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)
            (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (1): LightConv3x3(
            (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)
            (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (2): LightConv3x3(
            (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)
            (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (3): LightConv3x3(
            (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)
            (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
        )
        (gate): ChannelGate(
          (global_avgpool): AdaptiveAvgPool2d(output_size=1)
          (fc1): Conv2d(128, 8, kernel_size=(1, 1), stride=(1, 1))
          (relu): ReLU(inplace=True)
          (fc2): Conv2d(8, 128, kernel_size=(1, 1), stride=(1, 1))
          (gate_activation): Sigmoid()
        )
        (conv3): Conv1x1Linear(
          (conv): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (downsample): Conv1x1Linear(
          (conv): Conv2d(384, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): OSBlock(
        (conv1): Conv1x1(
          (conv): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (conv2a): LightConv3x3(
          (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)
          (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (conv2b): Sequential(
          (0): LightConv3x3(
            (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)
            (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (1): LightConv3x3(
            (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)
            (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
        )
        (conv2c): Sequential(
          (0): LightConv3x3(
            (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)
            (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (1): LightConv3x3(
            (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)
            (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (2): LightConv3x3(
            (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)
            (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
        )
        (conv2d): Sequential(
          (0): LightConv3x3(
            (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)
            (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (1): LightConv3x3(
            (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)
            (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (2): LightConv3x3(
            (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)
            (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
          (3): LightConv3x3(
            (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)
            (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
        )
        (gate): ChannelGate(
          (global_avgpool): AdaptiveAvgPool2d(output_size=1)
          (fc1): Conv2d(128, 8, kernel_size=(1, 1), stride=(1, 1))
          (relu): ReLU(inplace=True)
          (fc2): Conv2d(8, 128, kernel_size=(1, 1), stride=(1, 1))
          (gate_activation): Sigmoid()
        )
        (conv3): Conv1x1Linear(
          (conv): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
    )
  )
  (layer40): Conv1x1(
    (conv): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
  )
  (layer41): Sequential(
    (0): Conv1x1(
      (conv): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (conv10): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (conv20): Conv2d(384, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (global_avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
  (global_maxpool): AdaptiveMaxPool2d(output_size=(1, 1))
  (fc1): Linear(in_features=512, out_features=512, bias=True)
  (bn1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn2): BatchNorm1d(4096, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (classifier1): Linear(in_features=512, out_features=751, bias=True)
  (classifier2): Linear(in_features=4096, out_features=751, bias=True)
)
Building triplet-engine for image-reid
=> Start training
Epoch: [1/150][20/185]	Time 0.428 (0.504)	Data 0.000 (0.024)	Loss_t 0.9625 (1.0084)	Loss_x 6.7359 (6.7130)	Acc 0.00 (0.08)	Lr 0.000035	eta 3:53:08
Epoch: [1/150][40/185]	Time 0.419 (0.447)	Data 0.000 (0.012)	Loss_t 0.9142 (0.9724)	Loss_x 6.6561 (6.6955)	Acc 0.00 (0.04)	Lr 0.000035	eta 3:26:18
Epoch: [1/150][60/185]	Time 0.493 (0.442)	Data 0.000 (0.008)	Loss_t 0.8935 (0.9483)	Loss_x 6.6347 (6.6755)	Acc 0.00 (0.08)	Lr 0.000035	eta 3:24:09
Epoch: [1/150][80/185]	Time 0.426 (0.438)	Data 0.000 (0.006)	Loss_t 0.8792 (0.9335)	Loss_x 6.5334 (6.6536)	Acc 0.00 (0.06)	Lr 0.000035	eta 3:22:12
Epoch: [1/150][100/185]	Time 0.416 (0.435)	Data 0.000 (0.005)	Loss_t 0.8554 (0.9208)	Loss_x 6.5227 (6.6348)	Acc 0.00 (0.06)	Lr 0.000035	eta 3:20:37
Epoch: [1/150][120/185]	Time 0.427 (0.432)	Data 0.000 (0.004)	Loss_t 0.8679 (0.9108)	Loss_x 6.5053 (6.6138)	Acc 0.00 (0.09)	Lr 0.000035	eta 3:19:02
Epoch: [1/150][140/185]	Time 0.444 (0.432)	Data 0.000 (0.004)	Loss_t 0.8558 (0.9027)	Loss_x 6.4494 (6.5941)	Acc 0.00 (0.17)	Lr 0.000035	eta 3:18:53
Epoch: [1/150][160/185]	Time 0.460 (0.432)	Data 0.000 (0.003)	Loss_t 0.8524 (0.8965)	Loss_x 6.3814 (6.5710)	Acc 1.56 (0.18)	Lr 0.000035	eta 3:18:25
Epoch: [1/150][180/185]	Time 0.406 (0.430)	Data 0.000 (0.003)	Loss_t 0.8344 (0.8907)	Loss_x 6.1628 (6.5375)	Acc 0.00 (0.20)	Lr 0.000035	eta 3:17:47
Epoch: [2/150][20/185]	Time 0.455 (0.454)	Data 0.000 (0.019)	Loss_t 0.7984 (0.8264)	Loss_x 6.2186 (6.2487)	Acc 0.00 (0.62)	Lr 0.000043	eta 3:28:17
Epoch: [2/150][40/185]	Time 0.441 (0.429)	Data 0.000 (0.009)	Loss_t 0.8063 (0.8222)	Loss_x 6.2275 (6.2189)	Acc 0.00 (0.62)	Lr 0.000043	eta 3:16:34
Epoch: [2/150][60/185]	Time 0.362 (0.413)	Data 0.000 (0.006)	Loss_t 0.8054 (0.8208)	Loss_x 6.0834 (6.2008)	Acc 1.56 (0.68)	Lr 0.000043	eta 3:09:11
Epoch: [2/150][80/185]	Time 0.430 (0.412)	Data 0.000 (0.005)	Loss_t 0.8272 (0.8194)	Loss_x 6.0225 (6.1681)	Acc 1.56 (0.86)	Lr 0.000043	eta 3:08:40
Epoch: [2/150][100/185]	Time 0.427 (0.413)	Data 0.000 (0.004)	Loss_t 0.8015 (0.8183)	Loss_x 5.9933 (6.1392)	Acc 3.12 (1.06)	Lr 0.000043	eta 3:09:13
Epoch: [2/150][120/185]	Time 0.430 (0.416)	Data 0.000 (0.003)	Loss_t 0.8071 (0.8171)	Loss_x 6.0119 (6.1100)	Acc 4.69 (1.28)	Lr 0.000043	eta 3:10:20
Epoch: [2/150][140/185]	Time 0.427 (0.418)	Data 0.000 (0.003)	Loss_t 0.8157 (0.8162)	Loss_x 5.7083 (6.0720)	Acc 4.69 (1.63)	Lr 0.000043	eta 3:11:16
Epoch: [2/150][160/185]	Time 0.389 (0.417)	Data 0.000 (0.002)	Loss_t 0.8088 (0.8154)	Loss_x 5.5692 (6.0249)	Acc 6.25 (1.93)	Lr 0.000043	eta 3:10:38
Epoch: [2/150][180/185]	Time 0.404 (0.417)	Data 0.000 (0.002)	Loss_t 0.7893 (0.8136)	Loss_x 5.0390 (5.9530)	Acc 20.31 (2.98)	Lr 0.000043	eta 3:10:29
Epoch: [3/150][20/185]	Time 0.432 (0.456)	Data 0.000 (0.026)	Loss_t 0.7912 (0.7987)	Loss_x 5.7677 (5.6482)	Acc 4.69 (7.03)	Lr 0.000051	eta 3:27:42
Epoch: [3/150][40/185]	Time 0.418 (0.446)	Data 0.000 (0.013)	Loss_t 0.8051 (0.7990)	Loss_x 5.4737 (5.6308)	Acc 6.25 (6.25)	Lr 0.000051	eta 3:23:19
Epoch: [3/150][60/185]	Time 0.453 (0.437)	Data 0.000 (0.009)	Loss_t 0.7949 (0.7967)	Loss_x 5.8192 (5.6139)	Acc 4.69 (6.02)	Lr 0.000051	eta 3:19:00
Epoch: [3/150][80/185]	Time 0.447 (0.433)	Data 0.000 (0.007)	Loss_t 0.7991 (0.7961)	Loss_x 5.4436 (5.5880)	Acc 7.81 (6.04)	Lr 0.000051	eta 3:16:53
Epoch: [3/150][100/185]	Time 0.349 (0.421)	Data 0.000 (0.005)	Loss_t 0.7685 (0.7956)	Loss_x 5.0417 (5.5378)	Acc 12.50 (6.53)	Lr 0.000051	eta 3:11:30
Epoch: [3/150][120/185]	Time 0.395 (0.417)	Data 0.000 (0.004)	Loss_t 0.7970 (0.7925)	Loss_x 5.1740 (5.4794)	Acc 10.94 (7.32)	Lr 0.000051	eta 3:09:34
Epoch: [3/150][140/185]	Time 0.463 (0.418)	Data 0.000 (0.004)	Loss_t 0.7887 (0.7922)	Loss_x 5.1747 (5.4322)	Acc 10.94 (7.72)	Lr 0.000051	eta 3:09:56
Epoch: [3/150][160/185]	Time 0.414 (0.416)	Data 0.000 (0.003)	Loss_t 0.7673 (0.7906)	Loss_x 4.7011 (5.3645)	Acc 29.69 (8.93)	Lr 0.000051	eta 3:08:56
Epoch: [3/150][180/185]	Time 0.448 (0.415)	Data 0.000 (0.003)	Loss_t 0.8034 (0.7900)	Loss_x 4.5695 (5.2857)	Acc 39.06 (10.49)	Lr 0.000051	eta 3:08:08
Epoch: [4/150][20/185]	Time 0.472 (0.456)	Data 0.000 (0.018)	Loss_t 0.8042 (0.7579)	Loss_x 4.9199 (5.0618)	Acc 6.25 (8.52)	Lr 0.000059	eta 3:26:26
Epoch: [4/150][40/185]	Time 0.378 (0.449)	Data 0.000 (0.009)	Loss_t 0.8222 (0.7643)	Loss_x 5.3324 (5.0762)	Acc 9.38 (8.12)	Lr 0.000059	eta 3:23:18
Epoch: [4/150][60/185]	Time 0.466 (0.438)	Data 0.000 (0.006)	Loss_t 0.7757 (0.7652)	Loss_x 5.1656 (5.0216)	Acc 7.81 (8.83)	Lr 0.000059	eta 3:18:04
Epoch: [4/150][80/185]	Time 0.421 (0.432)	Data 0.000 (0.005)	Loss_t 0.7273 (0.7651)	Loss_x 4.6243 (4.9721)	Acc 10.94 (9.51)	Lr 0.000059	eta 3:15:13
Epoch: [4/150][100/185]	Time 0.440 (0.433)	Data 0.000 (0.004)	Loss_t 0.7613 (0.7637)	Loss_x 4.5123 (4.9116)	Acc 17.19 (10.17)	Lr 0.000059	eta 3:15:23
Epoch: [4/150][120/185]	Time 0.444 (0.432)	Data 0.000 (0.003)	Loss_t 0.7503 (0.7637)	Loss_x 4.5161 (4.8659)	Acc 21.88 (11.05)	Lr 0.000059	eta 3:14:57
Epoch: [4/150][140/185]	Time 0.382 (0.426)	Data 0.000 (0.003)	Loss_t 0.7399 (0.7635)	Loss_x 4.4298 (4.8054)	Acc 23.44 (12.37)	Lr 0.000059	eta 3:12:00
Epoch: [4/150][160/185]	Time 0.395 (0.426)	Data 0.000 (0.002)	Loss_t 0.7475 (0.7631)	Loss_x 4.0977 (4.7418)	Acc 20.31 (13.33)	Lr 0.000059	eta 3:11:56
Epoch: [4/150][180/185]	Time 0.447 (0.427)	Data 0.000 (0.002)	Loss_t 0.8029 (0.7626)	Loss_x 3.9510 (4.6560)	Acc 35.94 (15.79)	Lr 0.000059	eta 3:12:13
Epoch: [5/150][20/185]	Time 0.487 (0.465)	Data 0.000 (0.020)	Loss_t 0.7708 (0.7510)	Loss_x 4.5353 (4.5902)	Acc 10.94 (9.14)	Lr 0.000067	eta 3:29:02
Epoch: [5/150][40/185]	Time 0.446 (0.449)	Data 0.000 (0.010)	Loss_t 0.7790 (0.7432)	Loss_x 4.4708 (4.5230)	Acc 23.44 (9.96)	Lr 0.000067	eta 3:21:52
Epoch: [5/150][60/185]	Time 0.419 (0.444)	Data 0.000 (0.007)	Loss_t 0.7101 (0.7440)	Loss_x 4.3901 (4.4854)	Acc 17.19 (10.83)	Lr 0.000067	eta 3:19:17
Epoch: [5/150][80/185]	Time 0.417 (0.439)	Data 0.000 (0.005)	Loss_t 0.7083 (0.7390)	Loss_x 4.1317 (4.4229)	Acc 17.19 (11.80)	Lr 0.000067	eta 3:17:07
Epoch: [5/150][100/185]	Time 0.398 (0.435)	Data 0.000 (0.004)	Loss_t 0.7806 (0.7404)	Loss_x 4.5056 (4.3793)	Acc 6.25 (12.70)	Lr 0.000067	eta 3:15:08
Epoch: [5/150][120/185]	Time 0.401 (0.429)	Data 0.000 (0.003)	Loss_t 0.7380 (0.7398)	Loss_x 3.9756 (4.3313)	Acc 25.00 (13.85)	Lr 0.000067	eta 3:12:07
Epoch: [5/150][140/185]	Time 0.378 (0.422)	Data 0.000 (0.003)	Loss_t 0.7534 (0.7372)	Loss_x 3.9818 (4.2681)	Acc 21.88 (15.81)	Lr 0.000067	eta 3:09:09
Epoch: [5/150][160/185]	Time 0.374 (0.419)	Data 0.000 (0.003)	Loss_t 0.7479 (0.7365)	Loss_x 3.6164 (4.2029)	Acc 32.81 (18.09)	Lr 0.000067	eta 3:07:24
Epoch: [5/150][180/185]	Time 0.399 (0.416)	Data 0.000 (0.002)	Loss_t 0.7319 (0.7355)	Loss_x 3.2625 (4.1138)	Acc 42.19 (21.41)	Lr 0.000067	eta 3:05:58
Epoch: [6/150][20/185]	Time 0.510 (0.466)	Data 0.000 (0.024)	Loss_t 0.7190 (0.7213)	Loss_x 4.3471 (4.1325)	Acc 6.25 (13.28)	Lr 0.000075	eta 3:28:09
Epoch: [6/150][40/185]	Time 0.425 (0.455)	Data 0.000 (0.012)	Loss_t 0.6808 (0.7129)	Loss_x 3.7530 (4.0401)	Acc 20.31 (13.98)	Lr 0.000075	eta 3:23:11
Epoch: [6/150][60/185]	Time 0.404 (0.454)	Data 0.000 (0.008)	Loss_t 0.6886 (0.7142)	Loss_x 3.4334 (3.9905)	Acc 32.81 (14.69)	Lr 0.000075	eta 3:22:40
Epoch: [6/150][80/185]	Time 0.393 (0.446)	Data 0.000 (0.006)	Loss_t 0.6820 (0.7117)	Loss_x 4.0115 (3.9514)	Acc 4.69 (15.27)	Lr 0.000075	eta 3:18:59
Epoch: [6/150][100/185]	Time 0.429 (0.442)	Data 0.000 (0.005)	Loss_t 0.7145 (0.7131)	Loss_x 3.6459 (3.8967)	Acc 20.31 (16.91)	Lr 0.000075	eta 3:16:39
Epoch: [6/150][120/185]	Time 0.377 (0.432)	Data 0.000 (0.004)	Loss_t 0.7333 (0.7125)	Loss_x 3.8695 (3.8541)	Acc 21.88 (18.16)	Lr 0.000075	eta 3:12:03
Epoch: [6/150][140/185]	Time 0.449 (0.429)	Data 0.000 (0.004)	Loss_t 0.6671 (0.7088)	Loss_x 3.3320 (3.7895)	Acc 29.69 (20.15)	Lr 0.000075	eta 3:10:55
Epoch: [6/150][160/185]	Time 0.405 (0.427)	Data 0.000 (0.003)	Loss_t 0.6888 (0.7048)	Loss_x 3.1398 (3.7168)	Acc 50.00 (23.36)	Lr 0.000075	eta 3:09:34
Epoch: [6/150][180/185]	Time 0.411 (0.427)	Data 0.000 (0.003)	Loss_t 0.7063 (0.7038)	Loss_x 2.7364 (3.6394)	Acc 73.44 (27.13)	Lr 0.000075	eta 3:09:30
Epoch: [7/150][20/185]	Time 0.465 (0.470)	Data 0.000 (0.022)	Loss_t 0.6358 (0.6882)	Loss_x 3.3663 (3.6525)	Acc 23.44 (20.16)	Lr 0.000083	eta 3:28:27
Epoch: [7/150][40/185]	Time 0.484 (0.452)	Data 0.000 (0.011)	Loss_t 0.7238 (0.6849)	Loss_x 3.2824 (3.6002)	Acc 42.19 (19.38)	Lr 0.000083	eta 3:20:31
Epoch: [7/150][60/185]	Time 0.460 (0.447)	Data 0.000 (0.007)	Loss_t 0.6981 (0.6866)	Loss_x 3.7086 (3.5785)	Acc 17.19 (19.71)	Lr 0.000083	eta 3:17:50
Epoch: [7/150][80/185]	Time 0.406 (0.446)	Data 0.000 (0.005)	Loss_t 0.7029 (0.6856)	Loss_x 3.1815 (3.5317)	Acc 35.94 (20.90)	Lr 0.000083	eta 3:17:35
Epoch: [7/150][100/185]	Time 0.454 (0.441)	Data 0.000 (0.004)	Loss_t 0.7394 (0.6876)	Loss_x 3.4540 (3.4879)	Acc 31.25 (23.08)	Lr 0.000083	eta 3:15:08
Epoch: [7/150][120/185]	Time 0.423 (0.436)	Data 0.000 (0.004)	Loss_t 0.6019 (0.6859)	Loss_x 2.9145 (3.4400)	Acc 60.94 (25.43)	Lr 0.000083	eta 3:12:55
Epoch: [7/150][140/185]	Time 0.424 (0.432)	Data 0.000 (0.003)	Loss_t 0.6754 (0.6840)	Loss_x 2.7973 (3.3857)	Acc 59.38 (27.53)	Lr 0.000083	eta 3:10:56
Epoch: [7/150][160/185]	Time 0.382 (0.428)	Data 0.000 (0.003)	Loss_t 0.6303 (0.6802)	Loss_x 2.8233 (3.3188)	Acc 46.88 (30.89)	Lr 0.000083	eta 3:08:46
Epoch: [7/150][180/185]	Time 0.343 (0.420)	Data 0.000 (0.003)	Loss_t 0.6298 (0.6788)	Loss_x 2.2912 (3.2388)	Acc 81.25 (34.97)	Lr 0.000083	eta 3:05:06
Epoch: [8/150][20/185]	Time 0.428 (0.429)	Data 0.000 (0.024)	Loss_t 0.6694 (0.6606)	Loss_x 3.1787 (3.2903)	Acc 21.88 (22.66)	Lr 0.000092	eta 3:08:51
Epoch: [8/150][40/185]	Time 0.369 (0.424)	Data 0.000 (0.012)	Loss_t 0.7076 (0.6562)	Loss_x 3.3567 (3.2320)	Acc 9.38 (24.38)	Lr 0.000092	eta 3:06:29
Epoch: [8/150][60/185]	Time 0.484 (0.430)	Data 0.000 (0.008)	Loss_t 0.6136 (0.6574)	Loss_x 3.0842 (3.1795)	Acc 21.88 (27.58)	Lr 0.000092	eta 3:09:03
Epoch: [8/150][80/185]	Time 0.434 (0.424)	Data 0.000 (0.006)	Loss_t 0.6886 (0.6588)	Loss_x 3.2438 (3.1456)	Acc 26.56 (28.95)	Lr 0.000092	eta 3:06:29
Epoch: [8/150][100/185]	Time 0.368 (0.421)	Data 0.000 (0.005)	Loss_t 0.6049 (0.6577)	Loss_x 2.7348 (3.1139)	Acc 37.50 (30.73)	Lr 0.000092	eta 3:04:48
Epoch: [8/150][120/185]	Time 0.383 (0.424)	Data 0.000 (0.004)	Loss_t 0.6270 (0.6560)	Loss_x 2.6783 (3.0763)	Acc 57.81 (32.58)	Lr 0.000092	eta 3:05:53
Epoch: [8/150][140/185]	Time 0.436 (0.423)	Data 0.000 (0.004)	Loss_t 0.6665 (0.6530)	Loss_x 2.7932 (3.0162)	Acc 46.88 (35.62)	Lr 0.000092	eta 3:05:29
Epoch: [8/150][160/185]	Time 0.405 (0.421)	Data 0.000 (0.003)	Loss_t 0.6584 (0.6482)	Loss_x 2.5779 (2.9582)	Acc 62.50 (38.35)	Lr 0.000092	eta 3:04:31
Epoch: [8/150][180/185]	Time 0.425 (0.422)	Data 0.000 (0.003)	Loss_t 0.6051 (0.6445)	Loss_x 2.2586 (2.8810)	Acc 85.94 (42.27)	Lr 0.000092	eta 3:04:43
Epoch: [9/150][20/185]	Time 0.373 (0.441)	Data 0.000 (0.020)	Loss_t 0.5881 (0.6137)	Loss_x 2.5503 (2.8745)	Acc 48.44 (34.45)	Lr 0.000100	eta 3:13:09
Epoch: [9/150][40/185]	Time 0.429 (0.439)	Data 0.000 (0.010)	Loss_t 0.6281 (0.6259)	Loss_x 2.8680 (2.8829)	Acc 29.69 (34.14)	Lr 0.000100	eta 3:11:44
Epoch: [9/150][60/185]	Time 0.421 (0.435)	Data 0.000 (0.007)	Loss_t 0.5542 (0.6258)	Loss_x 2.9632 (2.8740)	Acc 17.19 (34.90)	Lr 0.000100	eta 3:09:56
Epoch: [9/150][80/185]	Time 0.359 (0.430)	Data 0.000 (0.005)	Loss_t 0.6854 (0.6280)	Loss_x 2.6702 (2.8437)	Acc 51.56 (36.91)	Lr 0.000100	eta 3:07:44
Epoch: [9/150][100/185]	Time 0.377 (0.426)	Data 0.000 (0.004)	Loss_t 0.5493 (0.6238)	Loss_x 2.6486 (2.8017)	Acc 54.69 (39.45)	Lr 0.000100	eta 3:05:52
Epoch: [9/150][120/185]	Time 0.394 (0.422)	Data 0.000 (0.003)	Loss_t 0.6266 (0.6237)	Loss_x 2.4504 (2.7641)	Acc 65.62 (41.69)	Lr 0.000100	eta 3:03:45
Epoch: [9/150][140/185]	Time 0.498 (0.423)	Data 0.000 (0.003)	Loss_t 0.6753 (0.6244)	Loss_x 2.2230 (2.7160)	Acc 81.25 (44.62)	Lr 0.000100	eta 3:04:15
Epoch: [9/150][160/185]	Time 0.457 (0.424)	Data 0.000 (0.003)	Loss_t 0.5520 (0.6216)	Loss_x 2.1322 (2.6619)	Acc 73.44 (47.70)	Lr 0.000100	eta 3:04:21
Epoch: [9/150][180/185]	Time 0.403 (0.422)	Data 0.000 (0.002)	Loss_t 0.5384 (0.6193)	Loss_x 1.7243 (2.5908)	Acc 92.19 (51.42)	Lr 0.000100	eta 3:03:33
Epoch: [10/150][20/185]	Time 0.442 (0.458)	Data 0.000 (0.024)	Loss_t 0.5939 (0.6006)	Loss_x 2.4605 (2.7654)	Acc 51.56 (32.81)	Lr 0.000108	eta 3:19:03
Epoch: [10/150][40/185]	Time 0.452 (0.433)	Data 0.000 (0.012)	Loss_t 0.5367 (0.6004)	Loss_x 2.2537 (2.6955)	Acc 64.06 (38.20)	Lr 0.000108	eta 3:08:01
Epoch: [10/150][60/185]	Time 0.431 (0.429)	Data 0.000 (0.008)	Loss_t 0.5812 (0.6013)	Loss_x 2.3768 (2.6435)	Acc 50.00 (41.20)	Lr 0.000108	eta 3:06:12
Epoch: [10/150][80/185]	Time 0.454 (0.425)	Data 0.000 (0.006)	Loss_t 0.5820 (0.5972)	Loss_x 2.2606 (2.5860)	Acc 60.94 (44.47)	Lr 0.000108	eta 3:04:01
Epoch: [10/150][100/185]	Time 0.472 (0.426)	Data 0.000 (0.005)	Loss_t 0.6687 (0.5948)	Loss_x 2.5613 (2.5364)	Acc 48.44 (47.31)	Lr 0.000108	eta 3:04:35
Epoch: [10/150][120/185]	Time 0.458 (0.429)	Data 0.000 (0.004)	Loss_t 0.6002 (0.5923)	Loss_x 2.2147 (2.4950)	Acc 60.94 (50.17)	Lr 0.000108	eta 3:05:26
Epoch: [10/150][140/185]	Time 0.442 (0.429)	Data 0.000 (0.004)	Loss_t 0.5587 (0.5905)	Loss_x 2.0582 (2.4484)	Acc 71.88 (53.09)	Lr 0.000108	eta 3:05:30
Epoch: [10/150][160/185]	Time 0.459 (0.431)	Data 0.000 (0.003)	Loss_t 0.5919 (0.5894)	Loss_x 1.8401 (2.3948)	Acc 90.62 (56.17)	Lr 0.000108	eta 3:06:07
Epoch: [10/150][180/185]	Time 0.446 (0.430)	Data 0.000 (0.003)	Loss_t 0.5808 (0.5860)	Loss_x 1.9021 (2.3338)	Acc 76.56 (59.33)	Lr 0.000108	eta 3:05:45
##### Evaluating market1501 (source) #####
Extracting features from query set ...
Done, obtained 3368-by-4608 matrix
Extracting features from gallery set ...
Done, obtained 15913-by-4608 matrix
Speed: 0.0744 sec/batch
Computing distance matrix with metric=euclidean ...
Computing CMC and mAP ...
** Results **
mAP: 59.0%
CMC curve
Rank-1  : 76.4%
Rank-5  : 91.2%
Rank-10 : 94.7%
Rank-20 : 97.1%
Checkpoint saved to "log/model.pth.tar-10"
Epoch: [11/150][20/185]	Time 0.457 (0.458)	Data 0.000 (0.027)	Loss_t 0.5663 (0.5729)	Loss_x 2.4745 (2.5023)	Acc 53.12 (43.59)	Lr 0.000116	eta 3:17:45
Epoch: [11/150][40/185]	Time 0.400 (0.434)	Data 0.000 (0.014)	Loss_t 0.6430 (0.5779)	Loss_x 2.2575 (2.4300)	Acc 60.94 (47.54)	Lr 0.000116	eta 3:07:03
Epoch: [11/150][60/185]	Time 0.427 (0.423)	Data 0.000 (0.009)	Loss_t 0.6234 (0.5779)	Loss_x 2.3662 (2.4012)	Acc 67.19 (50.18)	Lr 0.000116	eta 3:02:05
Epoch: [11/150][80/185]	Time 0.408 (0.418)	Data 0.000 (0.007)	Loss_t 0.5437 (0.5782)	Loss_x 2.0590 (2.3735)	Acc 71.88 (52.52)	Lr 0.000116	eta 2:59:41
Epoch: [11/150][100/185]	Time 0.395 (0.418)	Data 0.000 (0.005)	Loss_t 0.6628 (0.5739)	Loss_x 2.3942 (2.3275)	Acc 59.38 (55.77)	Lr 0.000116	eta 2:59:46
Epoch: [11/150][120/185]	Time 0.433 (0.418)	Data 0.000 (0.005)	Loss_t 0.4569 (0.5692)	Loss_x 1.6817 (2.2768)	Acc 93.75 (58.70)	Lr 0.000116	eta 2:59:33
Epoch: [11/150][140/185]	Time 0.423 (0.418)	Data 0.000 (0.004)	Loss_t 0.6016 (0.5675)	Loss_x 1.7995 (2.2260)	Acc 89.06 (61.69)	Lr 0.000116	eta 2:59:30
Epoch: [11/150][160/185]	Time 0.456 (0.422)	Data 0.000 (0.003)	Loss_t 0.5080 (0.5651)	Loss_x 1.7090 (2.1813)	Acc 85.94 (64.15)	Lr 0.000116	eta 3:00:50
Epoch: [11/150][180/185]	Time 0.414 (0.424)	Data 0.000 (0.003)	Loss_t 0.6239 (0.5627)	Loss_x 1.6317 (2.1285)	Acc 95.31 (66.88)	Lr 0.000116	eta 3:01:44
Epoch: [12/150][20/185]	Time 0.483 (0.426)	Data 0.000 (0.021)	Loss_t 0.5963 (0.5537)	Loss_x 2.5224 (2.3508)	Acc 43.75 (48.20)	Lr 0.000124	eta 3:02:33
Epoch: [12/150][40/185]	Time 0.369 (0.426)	Data 0.000 (0.011)	Loss_t 0.5482 (0.5609)	Loss_x 2.2036 (2.2952)	Acc 56.25 (52.81)	Lr 0.000124	eta 3:02:17
Epoch: [12/150][60/185]	Time 0.382 (0.424)	Data 0.000 (0.007)	Loss_t 0.5301 (0.5555)	Loss_x 1.9234 (2.2361)	Acc 76.56 (57.45)	Lr 0.000124	eta 3:01:12
Epoch: [12/150][80/185]	Time 0.427 (0.420)	Data 0.000 (0.005)	Loss_t 0.5318 (0.5519)	Loss_x 1.9022 (2.1840)	Acc 81.25 (60.33)	Lr 0.000124	eta 2:59:21
Epoch: [12/150][100/185]	Time 0.395 (0.418)	Data 0.000 (0.004)	Loss_t 0.4891 (0.5495)	Loss_x 1.6918 (2.1312)	Acc 85.94 (63.50)	Lr 0.000124	eta 2:58:28
Epoch: [12/150][120/185]	Time 0.419 (0.420)	Data 0.000 (0.004)	Loss_t 0.5197 (0.5467)	Loss_x 1.8697 (2.0923)	Acc 76.56 (65.95)	Lr 0.000124	eta 2:59:20
Epoch: [12/150][140/185]	Time 0.368 (0.419)	Data 0.000 (0.003)	Loss_t 0.5216 (0.5460)	Loss_x 1.7183 (2.0529)	Acc 78.12 (68.37)	Lr 0.000124	eta 2:58:45
Epoch: [12/150][160/185]	Time 0.404 (0.419)	Data 0.000 (0.003)	Loss_t 0.5241 (0.5406)	Loss_x 1.5416 (2.0042)	Acc 92.19 (70.69)	Lr 0.000124	eta 2:58:36
Epoch: [12/150][180/185]	Time 0.392 (0.418)	Data 0.000 (0.002)	Loss_t 0.4566 (0.5375)	Loss_x 1.4516 (1.9561)	Acc 93.75 (73.23)	Lr 0.000124	eta 2:57:54
Epoch: [13/150][20/185]	Time 0.395 (0.463)	Data 0.000 (0.025)	Loss_t 0.5377 (0.5235)	Loss_x 2.2782 (2.1075)	Acc 50.00 (59.45)	Lr 0.000132	eta 3:17:02
Epoch: [13/150][40/185]	Time 0.413 (0.442)	Data 0.000 (0.013)	Loss_t 0.5437 (0.5268)	Loss_x 1.9880 (2.0950)	Acc 59.38 (61.09)	Lr 0.000132	eta 3:07:51
Epoch: [13/150][60/185]	Time 0.430 (0.434)	Data 0.000 (0.008)	Loss_t 0.5378 (0.5262)	Loss_x 1.7583 (2.0292)	Acc 84.38 (65.49)	Lr 0.000132	eta 3:04:26
Epoch: [13/150][80/185]	Time 0.408 (0.429)	Data 0.000 (0.006)	Loss_t 0.4808 (0.5246)	Loss_x 1.8880 (2.0020)	Acc 75.00 (67.58)	Lr 0.000132	eta 3:01:50
Epoch: [13/150][100/185]	Time 0.446 (0.428)	Data 0.000 (0.005)	Loss_t 0.5878 (0.5257)	Loss_x 1.8503 (1.9733)	Acc 75.00 (69.48)	Lr 0.000132	eta 3:01:33
Epoch: [13/150][120/185]	Time 0.424 (0.430)	Data 0.000 (0.004)	Loss_t 0.5611 (0.5259)	Loss_x 1.7931 (1.9459)	Acc 78.12 (71.17)	Lr 0.000132	eta 3:01:58
Epoch: [13/150][140/185]	Time 0.453 (0.429)	Data 0.000 (0.004)	Loss_t 0.5772 (0.5255)	Loss_x 1.6605 (1.9069)	Acc 89.06 (73.35)	Lr 0.000132	eta 3:01:39
Epoch: [13/150][160/185]	Time 0.372 (0.429)	Data 0.000 (0.003)	Loss_t 0.4673 (0.5212)	Loss_x 1.4704 (1.8634)	Acc 98.44 (75.52)	Lr 0.000132	eta 3:01:14
Epoch: [13/150][180/185]	Time 0.467 (0.430)	Data 0.000 (0.003)	Loss_t 0.4656 (0.5171)	Loss_x 1.3043 (1.8159)	Acc 100.00 (77.68)	Lr 0.000132	eta 3:01:40
Epoch: [14/150][20/185]	Time 0.438 (0.474)	Data 0.000 (0.025)	Loss_t 0.4722 (0.5135)	Loss_x 2.0082 (2.0084)	Acc 62.50 (62.89)	Lr 0.000140	eta 3:20:00
Epoch: [14/150][40/185]	Time 0.489 (0.459)	Data 0.000 (0.013)	Loss_t 0.5578 (0.4989)	Loss_x 1.8613 (1.9251)	Acc 65.62 (67.30)	Lr 0.000140	eta 3:13:44
Epoch: [14/150][60/185]	Time 0.428 (0.449)	Data 0.000 (0.008)	Loss_t 0.4632 (0.4986)	Loss_x 1.7133 (1.8825)	Acc 81.25 (70.42)	Lr 0.000140	eta 3:09:21
Epoch: [14/150][80/185]	Time 0.381 (0.441)	Data 0.000 (0.006)	Loss_t 0.4407 (0.4996)	Loss_x 1.7416 (1.8577)	Acc 73.44 (72.03)	Lr 0.000140	eta 3:05:38
Epoch: [14/150][100/185]	Time 0.414 (0.440)	Data 0.000 (0.005)	Loss_t 0.4920 (0.5019)	Loss_x 1.5105 (1.8203)	Acc 93.75 (74.56)	Lr 0.000140	eta 3:05:06
Epoch: [14/150][120/185]	Time 0.431 (0.436)	Data 0.000 (0.004)	Loss_t 0.4635 (0.4985)	Loss_x 1.6264 (1.7886)	Acc 90.62 (76.68)	Lr 0.000140	eta 3:03:11
Epoch: [14/150][140/185]	Time 0.389 (0.433)	Data 0.000 (0.004)	Loss_t 0.4476 (0.4961)	Loss_x 1.4684 (1.7540)	Acc 92.19 (78.71)	Lr 0.000140	eta 3:01:42
Epoch: [14/150][160/185]	Time 0.377 (0.430)	Data 0.000 (0.003)	Loss_t 0.4949 (0.4947)	Loss_x 1.4417 (1.7215)	Acc 98.44 (80.47)	Lr 0.000140	eta 3:00:28
Epoch: [14/150][180/185]	Time 0.407 (0.425)	Data 0.000 (0.003)	Loss_t 0.4062 (0.4913)	Loss_x 1.3509 (1.6866)	Acc 96.88 (82.07)	Lr 0.000140	eta 2:58:05
Epoch: [15/150][20/185]	Time 0.427 (0.462)	Data 0.000 (0.020)	Loss_t 0.5077 (0.5009)	Loss_x 2.1022 (1.8669)	Acc 64.06 (71.17)	Lr 0.000148	eta 3:13:28
Epoch: [15/150][40/185]	Time 0.442 (0.461)	Data 0.000 (0.010)	Loss_t 0.5713 (0.4955)	Loss_x 1.9316 (1.8141)	Acc 70.31 (73.09)	Lr 0.000148	eta 3:13:01
Epoch: [15/150][60/185]	Time 0.459 (0.458)	Data 0.000 (0.007)	Loss_t 0.3991 (0.4979)	Loss_x 1.5416 (1.7701)	Acc 90.62 (76.46)	Lr 0.000148	eta 3:11:33
Epoch: [15/150][80/185]	Time 0.434 (0.449)	Data 0.000 (0.005)	Loss_t 0.5804 (0.4937)	Loss_x 1.7988 (1.7307)	Acc 70.31 (78.52)	Lr 0.000148	eta 3:07:51
Epoch: [15/150][100/185]	Time 0.398 (0.444)	Data 0.000 (0.004)	Loss_t 0.4997 (0.4919)	Loss_x 1.7683 (1.7079)	Acc 76.56 (80.14)	Lr 0.000148	eta 3:05:38
Epoch: [15/150][120/185]	Time 0.399 (0.442)	Data 0.000 (0.003)	Loss_t 0.4751 (0.4879)	Loss_x 1.6033 (1.6804)	Acc 89.06 (81.61)	Lr 0.000148	eta 3:04:19
Epoch: [15/150][140/185]	Time 0.422 (0.439)	Data 0.000 (0.003)	Loss_t 0.4501 (0.4862)	Loss_x 1.4562 (1.6590)	Acc 89.06 (82.62)	Lr 0.000148	eta 3:02:55
Epoch: [15/150][160/185]	Time 0.432 (0.438)	Data 0.000 (0.003)	Loss_t 0.4685 (0.4865)	Loss_x 1.4521 (1.6338)	Acc 95.31 (84.00)	Lr 0.000148	eta 3:02:21
Epoch: [15/150][180/185]	Time 0.425 (0.436)	Data 0.000 (0.002)	Loss_t 0.4046 (0.4826)	Loss_x 1.2343 (1.6026)	Acc 98.44 (85.23)	Lr 0.000148	eta 3:01:26
Epoch: [16/150][20/185]	Time 0.419 (0.448)	Data 0.000 (0.020)	Loss_t 0.4348 (0.4611)	Loss_x 1.6774 (1.7129)	Acc 73.44 (75.23)	Lr 0.000156	eta 3:06:27
Epoch: [16/150][40/185]	Time 0.461 (0.450)	Data 0.000 (0.010)	Loss_t 0.5055 (0.4659)	Loss_x 1.6976 (1.6636)	Acc 84.38 (79.57)	Lr 0.000156	eta 3:06:50
Epoch: [16/150][60/185]	Time 0.441 (0.445)	Data 0.000 (0.007)	Loss_t 0.4612 (0.4633)	Loss_x 1.4602 (1.6366)	Acc 92.19 (81.30)	Lr 0.000156	eta 3:04:54
Epoch: [16/150][80/185]	Time 0.442 (0.444)	Data 0.000 (0.005)	Loss_t 0.4304 (0.4636)	Loss_x 1.4713 (1.6135)	Acc 95.31 (82.97)	Lr 0.000156	eta 3:04:08
Epoch: [16/150][100/185]	Time 0.426 (0.441)	Data 0.000 (0.004)	Loss_t 0.5145 (0.4634)	Loss_x 1.5571 (1.6032)	Acc 90.62 (83.56)	Lr 0.000156	eta 3:03:01
Epoch: [16/150][120/185]	Time 0.396 (0.437)	Data 0.000 (0.003)	Loss_t 0.5509 (0.4620)	Loss_x 1.5796 (1.5826)	Acc 89.06 (84.67)	Lr 0.000156	eta 3:01:13
Epoch: [16/150][140/185]	Time 0.411 (0.435)	Data 0.000 (0.003)	Loss_t 0.3990 (0.4637)	Loss_x 1.3763 (1.5631)	Acc 93.75 (85.86)	Lr 0.000156	eta 3:00:00
Epoch: [16/150][160/185]	Time 0.380 (0.432)	Data 0.000 (0.003)	Loss_t 0.4641 (0.4611)	Loss_x 1.3868 (1.5432)	Acc 95.31 (86.93)	Lr 0.000156	eta 2:58:47
Epoch: [16/150][180/185]	Time 0.413 (0.431)	Data 0.000 (0.002)	Loss_t 0.3956 (0.4578)	Loss_x 1.2911 (1.5169)	Acc 96.88 (88.07)	Lr 0.000156	eta 2:57:54
Epoch: [17/150][20/185]	Time 0.379 (0.441)	Data 0.000 (0.021)	Loss_t 0.4378 (0.4652)	Loss_x 1.5598 (1.6082)	Acc 92.19 (82.11)	Lr 0.000164	eta 3:02:05
Epoch: [17/150][40/185]	Time 0.465 (0.434)	Data 0.000 (0.010)	Loss_t 0.4801 (0.4603)	Loss_x 1.6127 (1.5953)	Acc 82.81 (83.20)	Lr 0.000164	eta 2:59:04
Epoch: [17/150][60/185]	Time 0.396 (0.431)	Data 0.000 (0.007)	Loss_t 0.4084 (0.4576)	Loss_x 1.4890 (1.5713)	Acc 85.94 (84.40)	Lr 0.000164	eta 2:57:39
Epoch: [17/150][80/185]	Time 0.453 (0.427)	Data 0.000 (0.005)	Loss_t 0.4424 (0.4567)	Loss_x 1.3947 (1.5541)	Acc 95.31 (85.62)	Lr 0.000164	eta 2:55:46
Epoch: [17/150][100/185]	Time 0.456 (0.430)	Data 0.000 (0.004)	Loss_t 0.4040 (0.4542)	Loss_x 1.3078 (1.5331)	Acc 100.00 (86.83)	Lr 0.000164	eta 2:57:05
Epoch: [17/150][120/185]	Time 0.434 (0.431)	Data 0.000 (0.004)	Loss_t 0.4246 (0.4533)	Loss_x 1.5058 (1.5156)	Acc 82.81 (87.86)	Lr 0.000164	eta 2:57:19
Epoch: [17/150][140/185]	Time 0.430 (0.431)	Data 0.000 (0.003)	Loss_t 0.4299 (0.4520)	Loss_x 1.2661 (1.4999)	Acc 98.44 (88.71)	Lr 0.000164	eta 2:57:00
Epoch: [17/150][160/185]	Time 0.452 (0.431)	Data 0.000 (0.003)	Loss_t 0.4518 (0.4503)	Loss_x 1.3746 (1.4792)	Acc 93.75 (89.59)	Lr 0.000164	eta 2:56:58
Epoch: [17/150][180/185]	Time 0.427 (0.429)	Data 0.000 (0.002)	Loss_t 0.4264 (0.4484)	Loss_x 1.2565 (1.4581)	Acc 100.00 (90.50)	Lr 0.000164	eta 2:56:04
Epoch: [18/150][20/185]	Time 0.424 (0.441)	Data 0.000 (0.024)	Loss_t 0.4800 (0.4525)	Loss_x 1.5838 (1.5113)	Acc 90.62 (87.34)	Lr 0.000172	eta 3:00:38
Epoch: [18/150][40/185]	Time 0.341 (0.433)	Data 0.000 (0.012)	Loss_t 0.4435 (0.4435)	Loss_x 1.3661 (1.5151)	Acc 93.75 (86.68)	Lr 0.000172	eta 2:57:08
Epoch: [18/150][60/185]	Time 0.404 (0.428)	Data 0.000 (0.008)	Loss_t 0.3962 (0.4414)	Loss_x 1.4019 (1.4924)	Acc 96.88 (88.07)	Lr 0.000172	eta 2:55:04
Epoch: [18/150][80/185]	Time 0.430 (0.427)	Data 0.000 (0.006)	Loss_t 0.4046 (0.4418)	Loss_x 1.3635 (1.4757)	Acc 95.31 (89.28)	Lr 0.000172	eta 2:54:32
Epoch: [18/150][100/185]	Time 0.406 (0.425)	Data 0.000 (0.005)	Loss_t 0.3677 (0.4406)	Loss_x 1.2909 (1.4597)	Acc 100.00 (90.27)	Lr 0.000172	eta 2:53:24
Epoch: [18/150][120/185]	Time 0.433 (0.422)	Data 0.000 (0.004)	Loss_t 0.4468 (0.4412)	Loss_x 1.4420 (1.4502)	Acc 89.06 (90.68)	Lr 0.000172	eta 2:52:19
Epoch: [18/150][140/185]	Time 0.492 (0.422)	Data 0.000 (0.004)	Loss_t 0.5234 (0.4432)	Loss_x 1.3694 (1.4382)	Acc 95.31 (91.26)	Lr 0.000172	eta 2:52:00
Epoch: [18/150][160/185]	Time 0.394 (0.422)	Data 0.000 (0.003)	Loss_t 0.3969 (0.4430)	Loss_x 1.3062 (1.4268)	Acc 95.31 (91.84)	Lr 0.000172	eta 2:52:07
Epoch: [18/150][180/185]	Time 0.427 (0.422)	Data 0.000 (0.003)	Loss_t 0.3920 (0.4416)	Loss_x 1.2446 (1.4119)	Acc 100.00 (92.40)	Lr 0.000172	eta 2:51:49
Epoch: [19/150][20/185]	Time 0.442 (0.454)	Data 0.000 (0.021)	Loss_t 0.4498 (0.4426)	Loss_x 1.6479 (1.4895)	Acc 79.69 (87.89)	Lr 0.000180	eta 3:04:45
Epoch: [19/150][40/185]	Time 0.414 (0.448)	Data 0.000 (0.010)	Loss_t 0.3916 (0.4323)	Loss_x 1.2759 (1.4561)	Acc 96.88 (88.91)	Lr 0.000180	eta 3:02:00
Epoch: [19/150][60/185]	Time 0.463 (0.438)	Data 0.000 (0.007)	Loss_t 0.4060 (0.4323)	Loss_x 1.3282 (1.4545)	Acc 96.88 (89.58)	Lr 0.000180	eta 2:57:57
Epoch: [19/150][80/185]	Time 0.398 (0.434)	Data 0.000 (0.005)	Loss_t 0.4263 (0.4278)	Loss_x 1.4856 (1.4350)	Acc 81.25 (90.84)	Lr 0.000180	eta 2:56:05
Epoch: [19/150][100/185]	Time 0.422 (0.427)	Data 0.000 (0.004)	Loss_t 0.4223 (0.4276)	Loss_x 1.4112 (1.4259)	Acc 100.00 (91.53)	Lr 0.000180	eta 2:53:05
Epoch: [19/150][120/185]	Time 0.375 (0.421)	Data 0.000 (0.004)	Loss_t 0.4267 (0.4299)	Loss_x 1.2487 (1.4185)	Acc 98.44 (91.90)	Lr 0.000180	eta 2:50:32
Epoch: [19/150][140/185]	Time 0.435 (0.418)	Data 0.000 (0.003)	Loss_t 0.4793 (0.4302)	Loss_x 1.4364 (1.4100)	Acc 93.75 (92.32)	Lr 0.000180	eta 2:49:15
Epoch: [19/150][160/185]	Time 0.389 (0.418)	Data 0.000 (0.003)	Loss_t 0.4596 (0.4309)	Loss_x 1.3436 (1.3965)	Acc 93.75 (92.88)	Lr 0.000180	eta 2:49:03
Epoch: [19/150][180/185]	Time 0.445 (0.417)	Data 0.000 (0.002)	Loss_t 0.3960 (0.4298)	Loss_x 1.2455 (1.3808)	Acc 100.00 (93.48)	Lr 0.000180	eta 2:48:17
Epoch: [20/150][20/185]	Time 0.382 (0.447)	Data 0.000 (0.022)	Loss_t 0.4634 (0.4413)	Loss_x 1.5727 (1.4562)	Acc 87.50 (90.94)	Lr 0.000188	eta 3:00:12
Epoch: [20/150][40/185]	Time 0.394 (0.433)	Data 0.000 (0.011)	Loss_t 0.4169 (0.4365)	Loss_x 1.4007 (1.4296)	Acc 92.19 (92.23)	Lr 0.000188	eta 2:54:43
Epoch: [20/150][60/185]	Time 0.372 (0.417)	Data 0.000 (0.007)	Loss_t 0.3940 (0.4295)	Loss_x 1.4150 (1.4138)	Acc 95.31 (92.58)	Lr 0.000188	eta 2:47:58
Epoch: [20/150][80/185]	Time 0.461 (0.416)	Data 0.000 (0.006)	Loss_t 0.4253 (0.4286)	Loss_x 1.3359 (1.4041)	Acc 96.88 (92.77)	Lr 0.000188	eta 2:47:37
Epoch: [20/150][100/185]	Time 0.411 (0.420)	Data 0.000 (0.004)	Loss_t 0.3886 (0.4290)	Loss_x 1.2952 (1.3986)	Acc 96.88 (92.89)	Lr 0.000188	eta 2:49:00
Epoch: [20/150][120/185]	Time 0.452 (0.422)	Data 0.000 (0.004)	Loss_t 0.4457 (0.4284)	Loss_x 1.2866 (1.3875)	Acc 93.75 (93.33)	Lr 0.000188	eta 2:49:26
Epoch: [20/150][140/185]	Time 0.487 (0.422)	Data 0.000 (0.003)	Loss_t 0.3736 (0.4281)	Loss_x 1.2760 (1.3762)	Acc 96.88 (93.76)	Lr 0.000188	eta 2:49:25
Epoch: [20/150][160/185]	Time 0.369 (0.421)	Data 0.000 (0.003)	Loss_t 0.4528 (0.4261)	Loss_x 1.2823 (1.3640)	Acc 98.44 (94.19)	Lr 0.000188	eta 2:48:44
Epoch: [20/150][180/185]	Time 0.430 (0.420)	Data 0.000 (0.003)	Loss_t 0.3815 (0.4249)	Loss_x 1.1948 (1.3519)	Acc 100.00 (94.55)	Lr 0.000188	eta 2:48:14
##### Evaluating market1501 (source) #####
Extracting features from query set ...
Done, obtained 3368-by-4608 matrix
Extracting features from gallery set ...
Done, obtained 15913-by-4608 matrix
Speed: 0.0366 sec/batch
Computing distance matrix with metric=euclidean ...
Computing CMC and mAP ...
** Results **
mAP: 72.2%
CMC curve
Rank-1  : 86.7%
Rank-5  : 95.7%
Rank-10 : 97.4%
Rank-20 : 98.5%
Checkpoint saved to "log/model.pth.tar-20"
Epoch: [21/150][20/185]	Time 0.361 (0.446)	Data 0.000 (0.023)	Loss_t 0.4069 (0.4004)	Loss_x 1.3065 (1.3965)	Acc 93.75 (92.58)	Lr 0.000197	eta 2:58:34
Epoch: [21/150][40/185]	Time 0.475 (0.436)	Data 0.000 (0.012)	Loss_t 0.3913 (0.4077)	Loss_x 1.3525 (1.3840)	Acc 90.62 (93.01)	Lr 0.000197	eta 2:54:19
Epoch: [21/150][60/185]	Time 0.401 (0.435)	Data 0.000 (0.008)	Loss_t 0.4967 (0.4113)	Loss_x 1.4427 (1.3762)	Acc 95.31 (93.65)	Lr 0.000197	eta 2:54:07
Epoch: [21/150][80/185]	Time 0.408 (0.427)	Data 0.000 (0.006)	Loss_t 0.4342 (0.4140)	Loss_x 1.3932 (1.3633)	Acc 90.62 (94.41)	Lr 0.000197	eta 2:50:44
Epoch: [21/150][100/185]	Time 0.430 (0.429)	Data 0.000 (0.005)	Loss_t 0.4444 (0.4142)	Loss_x 1.3570 (1.3542)	Acc 96.88 (94.83)	Lr 0.000197	eta 2:51:03
Epoch: [21/150][120/185]	Time 0.430 (0.424)	Data 0.000 (0.004)	Loss_t 0.4311 (0.4135)	Loss_x 1.2927 (1.3455)	Acc 98.44 (95.21)	Lr 0.000197	eta 2:49:06
Epoch: [21/150][140/185]	Time 0.430 (0.423)	Data 0.000 (0.003)	Loss_t 0.3996 (0.4139)	Loss_x 1.2305 (1.3377)	Acc 98.44 (95.49)	Lr 0.000197	eta 2:48:37
Epoch: [21/150][160/185]	Time 0.438 (0.424)	Data 0.000 (0.003)	Loss_t 0.4009 (0.4118)	Loss_x 1.3041 (1.3279)	Acc 93.75 (95.77)	Lr 0.000197	eta 2:48:58
Epoch: [21/150][180/185]	Time 0.419 (0.425)	Data 0.000 (0.003)	Loss_t 0.3835 (0.4121)	Loss_x 1.1979 (1.3190)	Acc 98.44 (95.94)	Lr 0.000197	eta 2:48:57
Epoch: [22/150][20/185]	Time 0.421 (0.462)	Data 0.000 (0.020)	Loss_t 0.4740 (0.4235)	Loss_x 1.4468 (1.3806)	Acc 90.62 (92.89)	Lr 0.000205	eta 3:03:36
Epoch: [22/150][40/185]	Time 0.428 (0.447)	Data 0.000 (0.010)	Loss_t 0.3762 (0.4142)	Loss_x 1.3082 (1.3543)	Acc 96.88 (94.30)	Lr 0.000205	eta 2:57:34
Epoch: [22/150][60/185]	Time 0.386 (0.433)	Data 0.000 (0.007)	Loss_t 0.3859 (0.4124)	Loss_x 1.2708 (1.3517)	Acc 98.44 (94.74)	Lr 0.000205	eta 2:51:40
Epoch: [22/150][80/185]	Time 0.345 (0.421)	Data 0.000 (0.005)	Loss_t 0.4439 (0.4129)	Loss_x 1.4441 (1.3511)	Acc 93.75 (94.88)	Lr 0.000205	eta 2:46:55
Epoch: [22/150][100/185]	Time 0.355 (0.409)	Data 0.000 (0.004)	Loss_t 0.4005 (0.4161)	Loss_x 1.2923 (1.3447)	Acc 96.88 (95.16)	Lr 0.000205	eta 2:42:07
Epoch: [22/150][120/185]	Time 0.404 (0.409)	Data 0.000 (0.003)	Loss_t 0.3902 (0.4152)	Loss_x 1.2857 (1.3367)	Acc 100.00 (95.31)	Lr 0.000205	eta 2:41:49
Epoch: [22/150][140/185]	Time 0.387 (0.409)	Data 0.000 (0.003)	Loss_t 0.4239 (0.4154)	Loss_x 1.2779 (1.3307)	Acc 98.44 (95.49)	Lr 0.000205	eta 2:41:45
Epoch: [22/150][160/185]	Time 0.451 (0.409)	Data 0.000 (0.003)	Loss_t 0.4242 (0.4138)	Loss_x 1.3340 (1.3225)	Acc 95.31 (95.81)	Lr 0.000205	eta 2:41:31
Epoch: [22/150][180/185]	Time 0.420 (0.410)	Data 0.000 (0.002)	Loss_t 0.3912 (0.4112)	Loss_x 1.2100 (1.3119)	Acc 96.88 (96.15)	Lr 0.000205	eta 2:41:39
Epoch: [23/150][20/185]	Time 0.509 (0.470)	Data 0.000 (0.023)	Loss_t 0.3806 (0.4039)	Loss_x 1.2647 (1.3212)	Acc 98.44 (95.62)	Lr 0.000213	eta 3:05:26
Epoch: [23/150][40/185]	Time 0.423 (0.457)	Data 0.000 (0.011)	Loss_t 0.3645 (0.4092)	Loss_x 1.3008 (1.3271)	Acc 96.88 (95.94)	Lr 0.000213	eta 2:59:55
Epoch: [23/150][60/185]	Time 0.429 (0.440)	Data 0.000 (0.008)	Loss_t 0.4018 (0.4089)	Loss_x 1.2890 (1.3248)	Acc 100.00 (95.86)	Lr 0.000213	eta 2:53:21
Epoch: [23/150][80/185]	Time 0.428 (0.431)	Data 0.000 (0.006)	Loss_t 0.4587 (0.4087)	Loss_x 1.3796 (1.3285)	Acc 93.75 (95.70)	Lr 0.000213	eta 2:49:40
Epoch: [23/150][100/185]	Time 0.465 (0.434)	Data 0.000 (0.005)	Loss_t 0.3966 (0.4096)	Loss_x 1.2533 (1.3264)	Acc 98.44 (95.91)	Lr 0.000213	eta 2:50:44
Epoch: [23/150][120/185]	Time 0.371 (0.433)	Data 0.000 (0.004)	Loss_t 0.3835 (0.4073)	Loss_x 1.2337 (1.3194)	Acc 98.44 (96.11)	Lr 0.000213	eta 2:50:07
Epoch: [23/150][140/185]	Time 0.434 (0.433)	Data 0.000 (0.003)	Loss_t 0.3863 (0.4074)	Loss_x 1.2108 (1.3120)	Acc 100.00 (96.28)	Lr 0.000213	eta 2:49:46
Epoch: [23/150][160/185]	Time 0.403 (0.432)	Data 0.000 (0.003)	Loss_t 0.3657 (0.4069)	Loss_x 1.2204 (1.3062)	Acc 93.75 (96.40)	Lr 0.000213	eta 2:49:24
Epoch: [23/150][180/185]	Time 0.434 (0.431)	Data 0.000 (0.003)	Loss_t 0.3446 (0.4043)	Loss_x 1.1618 (1.2963)	Acc 100.00 (96.64)	Lr 0.000213	eta 2:48:49
Epoch: [24/150][20/185]	Time 0.384 (0.447)	Data 0.000 (0.021)	Loss_t 0.3820 (0.4024)	Loss_x 1.2750 (1.3110)	Acc 98.44 (96.33)	Lr 0.000221	eta 2:54:57
Epoch: [24/150][40/185]	Time 0.468 (0.440)	Data 0.000 (0.010)	Loss_t 0.3661 (0.3964)	Loss_x 1.2752 (1.3065)	Acc 98.44 (96.68)	Lr 0.000221	eta 2:51:53
Epoch: [24/150][60/185]	Time 0.353 (0.424)	Data 0.000 (0.007)	Loss_t 0.3805 (0.3986)	Loss_x 1.2518 (1.3176)	Acc 100.00 (96.12)	Lr 0.000221	eta 2:45:43
Epoch: [24/150][80/185]	Time 0.350 (0.404)	Data 0.000 (0.005)	Loss_t 0.4566 (0.3975)	Loss_x 1.3455 (1.3156)	Acc 96.88 (96.21)	Lr 0.000221	eta 2:37:48
Epoch: [24/150][100/185]	Time 0.344 (0.392)	Data 0.000 (0.004)	Loss_t 0.3990 (0.3978)	Loss_x 1.3158 (1.3095)	Acc 93.75 (96.39)	Lr 0.000221	eta 2:32:50
Epoch: [24/150][120/185]	Time 0.338 (0.384)	Data 0.000 (0.004)	Loss_t 0.3964 (0.3976)	Loss_x 1.3007 (1.3055)	Acc 96.88 (96.51)	Lr 0.000221	eta 2:29:34
Epoch: [24/150][140/185]	Time 0.368 (0.380)	Data 0.000 (0.003)	Loss_t 0.4752 (0.3971)	Loss_x 1.3306 (1.3004)	Acc 95.31 (96.76)	Lr 0.000221	eta 2:28:04
Epoch: [24/150][160/185]	Time 0.414 (0.383)	Data 0.000 (0.003)	Loss_t 0.3952 (0.3966)	Loss_x 1.2602 (1.2940)	Acc 95.31 (96.88)	Lr 0.000221	eta 2:28:53
Epoch: [24/150][180/185]	Time 0.426 (0.385)	Data 0.000 (0.002)	Loss_t 0.3897 (0.3948)	Loss_x 1.2052 (1.2859)	Acc 100.00 (96.97)	Lr 0.000221	eta 2:29:28
Epoch: [25/150][20/185]	Time 0.357 (0.414)	Data 0.000 (0.024)	Loss_t 0.3895 (0.3918)	Loss_x 1.4228 (1.3127)	Acc 89.06 (95.70)	Lr 0.000229	eta 2:40:49
Epoch: [25/150][40/185]	Time 0.436 (0.416)	Data 0.000 (0.012)	Loss_t 0.3730 (0.3975)	Loss_x 1.2807 (1.3165)	Acc 98.44 (95.43)	Lr 0.000229	eta 2:41:19
Epoch: [25/150][60/185]	Time 0.429 (0.417)	Data 0.000 (0.008)	Loss_t 0.4646 (0.3940)	Loss_x 1.3398 (1.3098)	Acc 93.75 (96.04)	Lr 0.000229	eta 2:41:36
Epoch: [25/150][80/185]	Time 0.438 (0.419)	Data 0.000 (0.006)	Loss_t 0.4213 (0.3968)	Loss_x 1.2681 (1.3094)	Acc 98.44 (96.11)	Lr 0.000229	eta 2:42:17
Epoch: [25/150][100/185]	Time 0.416 (0.421)	Data 0.000 (0.005)	Loss_t 0.4648 (0.3971)	Loss_x 1.3985 (1.3070)	Acc 93.75 (96.23)	Lr 0.000229	eta 2:42:45
Epoch: [25/150][120/185]	Time 0.422 (0.422)	Data 0.000 (0.004)	Loss_t 0.4229 (0.3954)	Loss_x 1.2554 (1.3012)	Acc 98.44 (96.43)	Lr 0.000229	eta 2:43:14
Epoch: [25/150][140/185]	Time 0.427 (0.424)	Data 0.000 (0.004)	Loss_t 0.3935 (0.3935)	Loss_x 1.2564 (1.2957)	Acc 100.00 (96.64)	Lr 0.000229	eta 2:43:42
Epoch: [25/150][160/185]	Time 0.434 (0.425)	Data 0.000 (0.003)	Loss_t 0.3735 (0.3922)	Loss_x 1.2305 (1.2885)	Acc 100.00 (96.77)	Lr 0.000229	eta 2:43:49
Epoch: [25/150][180/185]	Time 0.428 (0.425)	Data 0.000 (0.003)	Loss_t 0.3555 (0.3917)	Loss_x 1.1578 (1.2796)	Acc 100.00 (96.98)	Lr 0.000229	eta 2:43:49
Epoch: [26/150][20/185]	Time 0.385 (0.446)	Data 0.000 (0.023)	Loss_t 0.3780 (0.3717)	Loss_x 1.2431 (1.2909)	Acc 98.44 (97.58)	Lr 0.000237	eta 2:51:34
Epoch: [26/150][40/185]	Time 0.411 (0.415)	Data 0.000 (0.012)	Loss_t 0.3306 (0.3795)	Loss_x 1.1942 (1.2864)	Acc 100.00 (96.95)	Lr 0.000237	eta 2:39:38
Epoch: [26/150][60/185]	Time 0.351 (0.417)	Data 0.000 (0.008)	Loss_t 0.3807 (0.3824)	Loss_x 1.2273 (1.2852)	Acc 98.44 (97.06)	Lr 0.000237	eta 2:40:21
Epoch: [26/150][80/185]	Time 0.422 (0.416)	Data 0.000 (0.006)	Loss_t 0.3899 (0.3826)	Loss_x 1.3160 (1.2822)	Acc 93.75 (97.21)	Lr 0.000237	eta 2:39:50
Epoch: [26/150][100/185]	Time 0.428 (0.419)	Data 0.000 (0.005)	Loss_t 0.3581 (0.3829)	Loss_x 1.2490 (1.2846)	Acc 96.88 (96.94)	Lr 0.000237	eta 2:40:58
Epoch: [26/150][120/185]	Time 0.431 (0.421)	Data 0.000 (0.004)	Loss_t 0.3776 (0.3828)	Loss_x 1.2319 (1.2812)	Acc 100.00 (97.10)	Lr 0.000237	eta 2:41:24
Epoch: [26/150][140/185]	Time 0.430 (0.422)	Data 0.000 (0.003)	Loss_t 0.3893 (0.3836)	Loss_x 1.2526 (1.2778)	Acc 95.31 (97.23)	Lr 0.000237	eta 2:41:49
Epoch: [26/150][160/185]	Time 0.419 (0.422)	Data 0.000 (0.003)	Loss_t 0.3750 (0.3835)	Loss_x 1.2927 (1.2723)	Acc 93.75 (97.35)	Lr 0.000237	eta 2:41:23
Epoch: [26/150][180/185]	Time 0.350 (0.417)	Data 0.000 (0.003)	Loss_t 0.3361 (0.3841)	Loss_x 1.1618 (1.2656)	Acc 100.00 (97.47)	Lr 0.000237	eta 2:39:21
Epoch: [27/150][20/185]	Time 0.348 (0.375)	Data 0.000 (0.022)	Loss_t 0.4221 (0.3917)	Loss_x 1.3259 (1.3126)	Acc 92.19 (96.41)	Lr 0.000245	eta 2:23:24
Epoch: [27/150][40/185]	Time 0.389 (0.391)	Data 0.000 (0.011)	Loss_t 0.3952 (0.3868)	Loss_x 1.3102 (1.2920)	Acc 96.88 (97.30)	Lr 0.000245	eta 2:29:24
Epoch: [27/150][60/185]	Time 0.453 (0.395)	Data 0.000 (0.007)	Loss_t 0.4168 (0.3867)	Loss_x 1.2900 (1.2979)	Acc 100.00 (96.85)	Lr 0.000245	eta 2:30:33
Epoch: [27/150][80/185]	Time 0.367 (0.402)	Data 0.000 (0.006)	Loss_t 0.4292 (0.3866)	Loss_x 1.2727 (1.2945)	Acc 98.44 (96.88)	Lr 0.000245	eta 2:33:06
Epoch: [27/150][100/185]	Time 0.388 (0.397)	Data 0.000 (0.004)	Loss_t 0.4441 (0.3868)	Loss_x 1.4234 (1.2931)	Acc 90.62 (96.98)	Lr 0.000245	eta 2:31:14
Epoch: [27/150][120/185]	Time 0.450 (0.400)	Data 0.000 (0.004)	Loss_t 0.3859 (0.3869)	Loss_x 1.3013 (1.2894)	Acc 95.31 (97.07)	Lr 0.000245	eta 2:32:15
Epoch: [27/150][140/185]	Time 0.468 (0.401)	Data 0.000 (0.003)	Loss_t 0.3706 (0.3863)	Loss_x 1.2808 (1.2844)	Acc 98.44 (97.22)	Lr 0.000245	eta 2:32:26
Epoch: [27/150][160/185]	Time 0.366 (0.402)	Data 0.000 (0.003)	Loss_t 0.3538 (0.3866)	Loss_x 1.2315 (1.2773)	Acc 98.44 (97.36)	Lr 0.000245	eta 2:32:36
Epoch: [27/150][180/185]	Time 0.342 (0.404)	Data 0.000 (0.003)	Loss_t 0.4011 (0.3861)	Loss_x 1.1827 (1.2704)	Acc 100.00 (97.47)	Lr 0.000245	eta 2:33:05
Epoch: [28/150][20/185]	Time 0.436 (0.439)	Data 0.000 (0.025)	Loss_t 0.3474 (0.3810)	Loss_x 1.2122 (1.3061)	Acc 100.00 (96.17)	Lr 0.000253	eta 2:46:20
Epoch: [28/150][40/185]	Time 0.473 (0.423)	Data 0.001 (0.012)	Loss_t 0.3359 (0.3806)	Loss_x 1.2002 (1.3025)	Acc 100.00 (96.72)	Lr 0.000253	eta 2:40:12
Epoch: [28/150][60/185]	Time 0.429 (0.410)	Data 0.000 (0.008)	Loss_t 0.4222 (0.3827)	Loss_x 1.3393 (1.2975)	Acc 93.75 (96.77)	Lr 0.000253	eta 2:35:01
Epoch: [28/150][80/185]	Time 0.442 (0.415)	Data 0.000 (0.006)	Loss_t 0.3945 (0.3821)	Loss_x 1.2789 (1.2958)	Acc 96.88 (96.97)	Lr 0.000253	eta 2:36:39
Epoch: [28/150][100/185]	Time 0.406 (0.419)	Data 0.000 (0.005)	Loss_t 0.3537 (0.3813)	Loss_x 1.2235 (1.2929)	Acc 96.88 (96.97)	Lr 0.000253	eta 2:38:13
Epoch: [28/150][120/185]	Time 0.424 (0.421)	Data 0.000 (0.004)	Loss_t 0.3578 (0.3798)	Loss_x 1.2469 (1.2880)	Acc 100.00 (96.98)	Lr 0.000253	eta 2:38:46
Epoch: [28/150][140/185]	Time 0.394 (0.416)	Data 0.000 (0.004)	Loss_t 0.3866 (0.3796)	Loss_x 1.2299 (1.2826)	Acc 100.00 (97.14)	Lr 0.000253	eta 2:36:50
Epoch: [28/150][160/185]	Time 0.344 (0.410)	Data 0.000 (0.003)	Loss_t 0.3833 (0.3785)	Loss_x 1.2282 (1.2766)	Acc 100.00 (97.27)	Lr 0.000253	eta 2:34:14
Epoch: [28/150][180/185]	Time 0.350 (0.403)	Data 0.000 (0.003)	Loss_t 0.3778 (0.3768)	Loss_x 1.1818 (1.2688)	Acc 100.00 (97.39)	Lr 0.000253	eta 2:31:31
Epoch: [29/150][20/185]	Time 0.426 (0.438)	Data 0.000 (0.027)	Loss_t 0.3792 (0.3677)	Loss_x 1.3639 (1.2866)	Acc 98.44 (96.41)	Lr 0.000261	eta 2:44:26
Epoch: [29/150][40/185]	Time 0.362 (0.421)	Data 0.000 (0.013)	Loss_t 0.4092 (0.3799)	Loss_x 1.2934 (1.2903)	Acc 100.00 (97.07)	Lr 0.000261	eta 2:38:09
Epoch: [29/150][60/185]	Time 0.354 (0.407)	Data 0.000 (0.009)	Loss_t 0.3643 (0.3817)	Loss_x 1.2332 (1.2866)	Acc 100.00 (97.27)	Lr 0.000261	eta 2:32:35
Epoch: [29/150][80/185]	Time 0.377 (0.401)	Data 0.000 (0.007)	Loss_t 0.3906 (0.3782)	Loss_x 1.2807 (1.2817)	Acc 98.44 (97.38)	Lr 0.000261	eta 2:30:16
Epoch: [29/150][100/185]	Time 0.419 (0.403)	Data 0.000 (0.005)	Loss_t 0.3428 (0.3762)	Loss_x 1.2466 (1.2798)	Acc 98.44 (97.45)	Lr 0.000261	eta 2:30:44
Epoch: [29/150][120/185]	Time 0.445 (0.411)	Data 0.000 (0.005)	Loss_t 0.4450 (0.3781)	Loss_x 1.2741 (1.2765)	Acc 98.44 (97.57)	Lr 0.000261	eta 2:33:50
Epoch: [29/150][140/185]	Time 0.372 (0.410)	Data 0.000 (0.004)	Loss_t 0.3284 (0.3756)	Loss_x 1.2128 (1.2723)	Acc 96.88 (97.67)	Lr 0.000261	eta 2:33:16
Epoch: [29/150][160/185]	Time 0.352 (0.407)	Data 0.000 (0.003)	Loss_t 0.4122 (0.3738)	Loss_x 1.2539 (1.2656)	Acc 96.88 (97.83)	Lr 0.000261	eta 2:32:04
Epoch: [29/150][180/185]	Time 0.434 (0.407)	Data 0.000 (0.003)	Loss_t 0.4676 (0.3747)	Loss_x 1.1970 (1.2598)	Acc 98.44 (97.90)	Lr 0.000261	eta 2:31:51
Epoch: [30/150][20/185]	Time 0.357 (0.439)	Data 0.000 (0.023)	Loss_t 0.3837 (0.3770)	Loss_x 1.3619 (1.2983)	Acc 96.88 (97.03)	Lr 0.000269	eta 2:43:32
Epoch: [30/150][40/185]	Time 0.379 (0.422)	Data 0.000 (0.011)	Loss_t 0.3510 (0.3787)	Loss_x 1.2766 (1.3006)	Acc 96.88 (96.68)	Lr 0.000269	eta 2:37:06
Epoch: [30/150][60/185]	Time 0.399 (0.426)	Data 0.000 (0.008)	Loss_t 0.3736 (0.3716)	Loss_x 1.2658 (1.2907)	Acc 98.44 (97.06)	Lr 0.000269	eta 2:38:36
Epoch: [30/150][80/185]	Time 0.464 (0.416)	Data 0.000 (0.006)	Loss_t 0.3527 (0.3720)	Loss_x 1.3117 (1.2896)	Acc 93.75 (97.09)	Lr 0.000269	eta 2:34:36
Epoch: [30/150][100/185]	Time 0.413 (0.419)	Data 0.000 (0.005)	Loss_t 0.3625 (0.3713)	Loss_x 1.3162 (1.2881)	Acc 96.88 (97.05)	Lr 0.000269	eta 2:35:48
Epoch: [30/150][120/185]	Time 0.423 (0.422)	Data 0.000 (0.004)	Loss_t 0.3366 (0.3718)	Loss_x 1.3126 (1.2841)	Acc 95.31 (97.23)	Lr 0.000269	eta 2:36:26
Epoch: [30/150][140/185]	Time 0.453 (0.421)	Data 0.000 (0.003)	Loss_t 0.3071 (0.3721)	Loss_x 1.1758 (1.2802)	Acc 98.44 (97.33)	Lr 0.000269	eta 2:36:02
Epoch: [30/150][160/185]	Time 0.389 (0.420)	Data 0.000 (0.003)	Loss_t 0.3704 (0.3711)	Loss_x 1.2711 (1.2755)	Acc 98.44 (97.45)	Lr 0.000269	eta 2:35:41
Epoch: [30/150][180/185]	Time 0.476 (0.422)	Data 0.000 (0.003)	Loss_t 0.3265 (0.3719)	Loss_x 1.2042 (1.2698)	Acc 98.44 (97.51)	Lr 0.000269	eta 2:36:05
##### Evaluating market1501 (source) #####
Extracting features from query set ...
Done, obtained 3368-by-4608 matrix
Extracting features from gallery set ...
Done, obtained 15913-by-4608 matrix
Speed: 0.0384 sec/batch
Computing distance matrix with metric=euclidean ...
Computing CMC and mAP ...
** Results **
mAP: 75.1%
CMC curve
Rank-1  : 88.3%
Rank-5  : 96.1%
Rank-10 : 97.5%
Rank-20 : 98.5%
Checkpoint saved to "log/model.pth.tar-30"
Epoch: [31/150][20/185]	Time 0.474 (0.455)	Data 0.000 (0.024)	Loss_t 0.4632 (0.3652)	Loss_x 1.4058 (1.3035)	Acc 92.19 (96.95)	Lr 0.000277	eta 2:48:03
Epoch: [31/150][40/185]	Time 0.425 (0.455)	Data 0.000 (0.012)	Loss_t 0.3941 (0.3687)	Loss_x 1.3254 (1.2978)	Acc 95.31 (97.03)	Lr 0.000277	eta 2:48:07
Epoch: [31/150][60/185]	Time 0.451 (0.451)	Data 0.000 (0.008)	Loss_t 0.3226 (0.3707)	Loss_x 1.2499 (1.2995)	Acc 100.00 (97.01)	Lr 0.000277	eta 2:46:24
Epoch: [31/150][80/185]	Time 0.415 (0.447)	Data 0.000 (0.006)	Loss_t 0.3489 (0.3696)	Loss_x 1.2105 (1.2907)	Acc 100.00 (97.30)	Lr 0.000277	eta 2:44:37
Epoch: [31/150][100/185]	Time 0.399 (0.441)	Data 0.000 (0.005)	Loss_t 0.3709 (0.3687)	Loss_x 1.2619 (1.2879)	Acc 98.44 (97.34)	Lr 0.000277	eta 2:42:21
Epoch: [31/150][120/185]	Time 0.411 (0.437)	Data 0.000 (0.004)	Loss_t 0.4007 (0.3669)	Loss_x 1.2764 (1.2818)	Acc 98.44 (97.38)	Lr 0.000277	eta 2:40:56
Epoch: [31/150][140/185]	Time 0.490 (0.431)	Data 0.000 (0.004)	Loss_t 0.2953 (0.3639)	Loss_x 1.1443 (1.2726)	Acc 100.00 (97.62)	Lr 0.000277	eta 2:38:17
Epoch: [31/150][160/185]	Time 0.437 (0.430)	Data 0.000 (0.003)	Loss_t 0.3255 (0.3626)	Loss_x 1.2138 (1.2666)	Acc 98.44 (97.72)	Lr 0.000277	eta 2:37:56
Epoch: [31/150][180/185]	Time 0.416 (0.429)	Data 0.000 (0.003)	Loss_t 0.4005 (0.3643)	Loss_x 1.2000 (1.2602)	Acc 100.00 (97.87)	Lr 0.000277	eta 2:37:18
Epoch: [32/150][20/185]	Time 0.550 (0.463)	Data 0.000 (0.020)	Loss_t 0.3642 (0.3682)	Loss_x 1.3101 (1.3080)	Acc 98.44 (97.81)	Lr 0.000285	eta 2:49:40
Epoch: [32/150][40/185]	Time 0.372 (0.437)	Data 0.000 (0.010)	Loss_t 0.3746 (0.3651)	Loss_x 1.3244 (1.2893)	Acc 96.88 (97.77)	Lr 0.000285	eta 2:40:06
Epoch: [32/150][60/185]	Time 0.433 (0.431)	Data 0.000 (0.007)	Loss_t 0.3408 (0.3609)	Loss_x 1.2546 (1.2840)	Acc 100.00 (97.76)	Lr 0.000285	eta 2:37:47
Epoch: [32/150][80/185]	Time 0.456 (0.431)	Data 0.000 (0.005)	Loss_t 0.3467 (0.3589)	Loss_x 1.2381 (1.2805)	Acc 98.44 (97.77)	Lr 0.000285	eta 2:37:43
Epoch: [32/150][100/185]	Time 0.435 (0.427)	Data 0.000 (0.004)	Loss_t 0.3325 (0.3597)	Loss_x 1.2761 (1.2776)	Acc 93.75 (97.75)	Lr 0.000285	eta 2:36:05
Epoch: [32/150][120/185]	Time 0.432 (0.424)	Data 0.000 (0.004)	Loss_t 0.3497 (0.3619)	Loss_x 1.1979 (1.2779)	Acc 100.00 (97.67)	Lr 0.000285	eta 2:34:34
Epoch: [32/150][140/185]	Time 0.440 (0.419)	Data 0.000 (0.003)	Loss_t 0.3562 (0.3617)	Loss_x 1.2560 (1.2743)	Acc 100.00 (97.77)	Lr 0.000285	eta 2:32:42
Epoch: [32/150][160/185]	Time 0.452 (0.421)	Data 0.000 (0.003)	Loss_t 0.3127 (0.3598)	Loss_x 1.1683 (1.2695)	Acc 100.00 (97.84)	Lr 0.000285	eta 2:33:13
Epoch: [32/150][180/185]	Time 0.453 (0.425)	Data 0.000 (0.002)	Loss_t 0.3284 (0.3588)	Loss_x 1.1863 (1.2625)	Acc 100.00 (97.93)	Lr 0.000285	eta 2:34:31
Epoch: [33/150][20/185]	Time 0.442 (0.459)	Data 0.000 (0.019)	Loss_t 0.3356 (0.3587)	Loss_x 1.2304 (1.2996)	Acc 98.44 (97.50)	Lr 0.000293	eta 2:46:51
Epoch: [33/150][40/185]	Time 0.432 (0.444)	Data 0.000 (0.010)	Loss_t 0.3785 (0.3515)	Loss_x 1.2751 (1.2886)	Acc 98.44 (97.62)	Lr 0.000293	eta 2:41:18
Epoch: [33/150][60/185]	Time 0.395 (0.438)	Data 0.000 (0.007)	Loss_t 0.3458 (0.3517)	Loss_x 1.2435 (1.2734)	Acc 98.44 (97.81)	Lr 0.000293	eta 2:38:50
Epoch: [33/150][80/185]	Time 0.443 (0.433)	Data 0.000 (0.005)	Loss_t 0.3171 (0.3493)	Loss_x 1.3055 (1.2748)	Acc 95.31 (97.70)	Lr 0.000293	eta 2:36:48
Epoch: [33/150][100/185]	Time 0.388 (0.431)	Data 0.000 (0.004)	Loss_t 0.3751 (0.3493)	Loss_x 1.3071 (1.2691)	Acc 98.44 (97.91)	Lr 0.000293	eta 2:35:55
Epoch: [33/150][120/185]	Time 0.406 (0.429)	Data 0.000 (0.003)	Loss_t 0.3751 (0.3474)	Loss_x 1.2802 (1.2643)	Acc 100.00 (98.05)	Lr 0.000293	eta 2:35:18
Epoch: [33/150][140/185]	Time 0.474 (0.428)	Data 0.000 (0.003)	Loss_t 0.3259 (0.3459)	Loss_x 1.3028 (1.2606)	Acc 93.75 (98.01)	Lr 0.000293	eta 2:34:42
Epoch: [33/150][160/185]	Time 0.405 (0.430)	Data 0.000 (0.003)	Loss_t 0.3745 (0.3446)	Loss_x 1.2545 (1.2547)	Acc 96.88 (98.12)	Lr 0.000293	eta 2:35:08
Epoch: [33/150][180/185]	Time 0.464 (0.431)	Data 0.000 (0.002)	Loss_t 0.3191 (0.3435)	Loss_x 1.1534 (1.2485)	Acc 100.00 (98.16)	Lr 0.000293	eta 2:35:24
Epoch: [34/150][20/185]	Time 0.412 (0.450)	Data 0.000 (0.021)	Loss_t 0.2980 (0.3400)	Loss_x 1.2176 (1.2944)	Acc 100.00 (98.05)	Lr 0.000302	eta 2:42:18
Epoch: [34/150][40/185]	Time 0.412 (0.441)	Data 0.000 (0.011)	Loss_t 0.3324 (0.3519)	Loss_x 1.2896 (1.3006)	Acc 96.88 (97.54)	Lr 0.000302	eta 2:38:39
Epoch: [34/150][60/185]	Time 0.401 (0.434)	Data 0.000 (0.007)	Loss_t 0.3259 (0.3501)	Loss_x 1.4197 (1.2990)	Acc 90.62 (97.24)	Lr 0.000302	eta 2:36:17
Epoch: [34/150][80/185]	Time 0.431 (0.429)	Data 0.000 (0.005)	Loss_t 0.3796 (0.3473)	Loss_x 1.2362 (1.2924)	Acc 100.00 (97.42)	Lr 0.000302	eta 2:34:20
Epoch: [34/150][100/185]	Time 0.373 (0.423)	Data 0.000 (0.004)	Loss_t 0.3551 (0.3486)	Loss_x 1.3132 (1.2883)	Acc 96.88 (97.34)	Lr 0.000302	eta 2:31:53
Epoch: [34/150][120/185]	Time 0.373 (0.421)	Data 0.000 (0.004)	Loss_t 0.3657 (0.3496)	Loss_x 1.1984 (1.2822)	Acc 100.00 (97.42)	Lr 0.000302	eta 2:30:55
Epoch: [34/150][140/185]	Time 0.359 (0.418)	Data 0.000 (0.003)	Loss_t 0.3722 (0.3493)	Loss_x 1.2216 (1.2785)	Acc 96.88 (97.50)	Lr 0.000302	eta 2:29:55
Epoch: [34/150][160/185]	Time 0.400 (0.417)	Data 0.000 (0.003)	Loss_t 0.3066 (0.3508)	Loss_x 1.2199 (1.2747)	Acc 98.44 (97.59)	Lr 0.000302	eta 2:29:25
Epoch: [34/150][180/185]	Time 0.422 (0.417)	Data 0.000 (0.002)	Loss_t 0.3546 (0.3522)	Loss_x 1.2048 (1.2685)	Acc 98.44 (97.69)	Lr 0.000302	eta 2:29:08
Epoch: [35/150][20/185]	Time 0.413 (0.435)	Data 0.000 (0.024)	Loss_t 0.3178 (0.3291)	Loss_x 1.2633 (1.3098)	Acc 98.44 (97.19)	Lr 0.000310	eta 2:35:23
Epoch: [35/150][40/185]	Time 0.362 (0.426)	Data 0.000 (0.012)	Loss_t 0.3439 (0.3282)	Loss_x 1.3134 (1.2960)	Acc 98.44 (97.34)	Lr 0.000310	eta 2:32:03
Epoch: [35/150][60/185]	Time 0.393 (0.417)	Data 0.000 (0.008)	Loss_t 0.3454 (0.3308)	Loss_x 1.2512 (1.2904)	Acc 100.00 (97.60)	Lr 0.000310	eta 2:28:36
Epoch: [35/150][80/185]	Time 0.369 (0.411)	Data 0.000 (0.006)	Loss_t 0.2578 (0.3274)	Loss_x 1.2808 (1.2893)	Acc 95.31 (97.68)	Lr 0.000310	eta 2:26:21
Epoch: [35/150][100/185]	Time 0.364 (0.410)	Data 0.000 (0.005)	Loss_t 0.3007 (0.3265)	Loss_x 1.2202 (1.2864)	Acc 98.44 (97.73)	Lr 0.000310	eta 2:26:01
Epoch: [35/150][120/185]	Time 0.399 (0.409)	Data 0.000 (0.004)	Loss_t 0.3505 (0.3290)	Loss_x 1.2423 (1.2833)	Acc 96.88 (97.72)	Lr 0.000310	eta 2:25:32
Epoch: [35/150][140/185]	Time 0.363 (0.407)	Data 0.000 (0.004)	Loss_t 0.3701 (0.3302)	Loss_x 1.2409 (1.2772)	Acc 100.00 (97.81)	Lr 0.000310	eta 2:24:39
Epoch: [35/150][160/185]	Time 0.415 (0.407)	Data 0.000 (0.003)	Loss_t 0.3670 (0.3326)	Loss_x 1.2059 (1.2710)	Acc 98.44 (97.89)	Lr 0.000310	eta 2:24:33
Epoch: [35/150][180/185]	Time 0.413 (0.407)	Data 0.000 (0.003)	Loss_t 0.3455 (0.3318)	Loss_x 1.1598 (1.2642)	Acc 100.00 (97.99)	Lr 0.000310	eta 2:24:27
Epoch: [36/150][20/185]	Time 0.491 (0.448)	Data 0.001 (0.023)	Loss_t 0.3531 (0.3291)	Loss_x 1.3334 (1.3134)	Acc 96.88 (97.81)	Lr 0.000318	eta 2:38:40
Epoch: [36/150][40/185]	Time 0.391 (0.436)	Data 0.000 (0.012)	Loss_t 0.3387 (0.3330)	Loss_x 1.2412 (1.2962)	Acc 100.00 (97.70)	Lr 0.000318	eta 2:34:23
Epoch: [36/150][60/185]	Time 0.416 (0.434)	Data 0.000 (0.008)	Loss_t 0.3971 (0.3383)	Loss_x 1.2851 (1.2982)	Acc 100.00 (97.53)	Lr 0.000318	eta 2:33:23
Epoch: [36/150][80/185]	Time 0.454 (0.432)	Data 0.000 (0.006)	Loss_t 0.4445 (0.3355)	Loss_x 1.2834 (1.2894)	Acc 95.31 (97.77)	Lr 0.000318	eta 2:32:30
Epoch: [36/150][100/185]	Time 0.476 (0.433)	Data 0.000 (0.005)	Loss_t 0.4100 (0.3350)	Loss_x 1.3401 (1.2834)	Acc 95.31 (98.00)	Lr 0.000318	eta 2:32:51
Epoch: [36/150][120/185]	Time 0.442 (0.432)	Data 0.000 (0.004)	Loss_t 0.3575 (0.3307)	Loss_x 1.2631 (1.2791)	Acc 98.44 (98.05)	Lr 0.000318	eta 2:32:21
Epoch: [36/150][140/185]	Time 0.405 (0.430)	Data 0.000 (0.003)	Loss_t 0.2539 (0.3273)	Loss_x 1.1920 (1.2734)	Acc 98.44 (98.06)	Lr 0.000318	eta 2:31:21
Epoch: [36/150][160/185]	Time 0.396 (0.429)	Data 0.000 (0.003)	Loss_t 0.4273 (0.3281)	Loss_x 1.3036 (1.2704)	Acc 95.31 (98.06)	Lr 0.000318	eta 2:30:53
Epoch: [36/150][180/185]	Time 0.415 (0.428)	Data 0.000 (0.003)	Loss_t 0.3471 (0.3278)	Loss_x 1.1854 (1.2635)	Acc 100.00 (98.13)	Lr 0.000318	eta 2:30:27
Epoch: [37/150][20/185]	Time 0.463 (0.474)	Data 0.000 (0.024)	Loss_t 0.3665 (0.3286)	Loss_x 1.2535 (1.3008)	Acc 98.44 (98.05)	Lr 0.000326	eta 2:46:37
Epoch: [37/150][40/185]	Time 0.421 (0.453)	Data 0.000 (0.012)	Loss_t 0.2710 (0.3141)	Loss_x 1.3001 (1.2883)	Acc 96.88 (97.97)	Lr 0.000326	eta 2:38:55
Epoch: [37/150][60/185]	Time 0.440 (0.446)	Data 0.000 (0.008)	Loss_t 0.3015 (0.3170)	Loss_x 1.3222 (1.2925)	Acc 95.31 (97.71)	Lr 0.000326	eta 2:36:28
Epoch: [37/150][80/185]	Time 0.431 (0.445)	Data 0.000 (0.006)	Loss_t 0.3838 (0.3201)	Loss_x 1.3481 (1.2877)	Acc 98.44 (97.89)	Lr 0.000326	eta 2:35:56
Epoch: [37/150][100/185]	Time 0.460 (0.439)	Data 0.000 (0.005)	Loss_t 0.2892 (0.3187)	Loss_x 1.1996 (1.2831)	Acc 100.00 (97.91)	Lr 0.000326	eta 2:33:37
Epoch: [37/150][120/185]	Time 0.400 (0.434)	Data 0.000 (0.004)	Loss_t 0.2910 (0.3184)	Loss_x 1.3270 (1.2813)	Acc 98.44 (97.90)	Lr 0.000326	eta 2:31:46
Epoch: [37/150][140/185]	Time 0.434 (0.432)	Data 0.000 (0.004)	Loss_t 0.3255 (0.3156)	Loss_x 1.2052 (1.2783)	Acc 96.88 (97.81)	Lr 0.000326	eta 2:30:51
Epoch: [37/150][160/185]	Time 0.387 (0.432)	Data 0.000 (0.003)	Loss_t 0.2367 (0.3120)	Loss_x 1.1634 (1.2700)	Acc 100.00 (97.92)	Lr 0.000326	eta 2:30:49
Epoch: [37/150][180/185]	Time 0.386 (0.430)	Data 0.000 (0.003)	Loss_t 0.2753 (0.3107)	Loss_x 1.1460 (1.2618)	Acc 100.00 (98.05)	Lr 0.000326	eta 2:29:53
Epoch: [38/150][20/185]	Time 0.415 (0.437)	Data 0.000 (0.022)	Loss_t 0.3625 (0.3075)	Loss_x 1.2817 (1.2928)	Acc 98.44 (97.50)	Lr 0.000334	eta 2:32:05
Epoch: [38/150][40/185]	Time 0.417 (0.444)	Data 0.000 (0.011)	Loss_t 0.3342 (0.3078)	Loss_x 1.2492 (1.2903)	Acc 96.88 (97.58)	Lr 0.000334	eta 2:34:17
Epoch: [38/150][60/185]	Time 0.400 (0.441)	Data 0.000 (0.007)	Loss_t 0.3186 (0.3017)	Loss_x 1.3359 (1.2866)	Acc 98.44 (97.73)	Lr 0.000334	eta 2:33:22
Epoch: [38/150][80/185]	Time 0.392 (0.436)	Data 0.000 (0.006)	Loss_t 0.3694 (0.3062)	Loss_x 1.3439 (1.2879)	Acc 96.88 (97.68)	Lr 0.000334	eta 2:31:19
Epoch: [38/150][100/185]	Time 0.415 (0.432)	Data 0.000 (0.004)	Loss_t 0.3713 (0.3056)	Loss_x 1.2799 (1.2858)	Acc 98.44 (97.80)	Lr 0.000334	eta 2:29:51
Epoch: [38/150][120/185]	Time 0.405 (0.430)	Data 0.000 (0.004)	Loss_t 0.2780 (0.3033)	Loss_x 1.2764 (1.2825)	Acc 98.44 (97.81)	Lr 0.000334	eta 2:28:48
Epoch: [38/150][140/185]	Time 0.400 (0.427)	Data 0.000 (0.003)	Loss_t 0.3201 (0.3015)	Loss_x 1.2527 (1.2759)	Acc 96.88 (97.89)	Lr 0.000334	eta 2:27:46
Epoch: [38/150][160/185]	Time 0.452 (0.427)	Data 0.000 (0.003)	Loss_t 0.2428 (0.3009)	Loss_x 1.1944 (1.2697)	Acc 98.44 (98.05)	Lr 0.000334	eta 2:27:41
Epoch: [38/150][180/185]	Time 0.432 (0.426)	Data 0.000 (0.003)	Loss_t 0.2199 (0.2991)	Loss_x 1.1496 (1.2621)	Acc 100.00 (98.09)	Lr 0.000334	eta 2:27:09
Epoch: [39/150][20/185]	Time 0.413 (0.441)	Data 0.000 (0.021)	Loss_t 0.3821 (0.3166)	Loss_x 1.3498 (1.3333)	Acc 95.31 (96.80)	Lr 0.000342	eta 2:32:05
Epoch: [39/150][40/185]	Time 0.468 (0.434)	Data 0.000 (0.010)	Loss_t 0.2694 (0.3063)	Loss_x 1.3429 (1.3192)	Acc 95.31 (96.88)	Lr 0.000342	eta 2:29:24
Epoch: [39/150][60/185]	Time 0.483 (0.424)	Data 0.000 (0.007)	Loss_t 0.2356 (0.2915)	Loss_x 1.2949 (1.3068)	Acc 100.00 (97.32)	Lr 0.000342	eta 2:26:03
Epoch: [39/150][80/185]	Time 0.411 (0.430)	Data 0.000 (0.005)	Loss_t 0.2420 (0.2913)	Loss_x 1.2287 (1.3019)	Acc 100.00 (97.46)	Lr 0.000342	eta 2:27:56
Epoch: [39/150][100/185]	Time 0.553 (0.432)	Data 0.000 (0.004)	Loss_t 0.2483 (0.2944)	Loss_x 1.1694 (1.2969)	Acc 100.00 (97.47)	Lr 0.000342	eta 2:28:37
Epoch: [39/150][120/185]	Time 0.420 (0.429)	Data 0.000 (0.004)	Loss_t 0.2723 (0.2945)	Loss_x 1.2362 (1.2916)	Acc 96.88 (97.58)	Lr 0.000342	eta 2:27:15
Epoch: [39/150][140/185]	Time 0.449 (0.428)	Data 0.000 (0.003)	Loss_t 0.2935 (0.2967)	Loss_x 1.2766 (1.2884)	Acc 98.44 (97.56)	Lr 0.000342	eta 2:26:47
Epoch: [39/150][160/185]	Time 0.437 (0.430)	Data 0.000 (0.003)	Loss_t 0.2935 (0.2951)	Loss_x 1.2351 (1.2835)	Acc 96.88 (97.59)	Lr 0.000342	eta 2:27:13
Epoch: [39/150][180/185]	Time 0.443 (0.431)	Data 0.000 (0.002)	Loss_t 0.2884 (0.2961)	Loss_x 1.1679 (1.2774)	Acc 100.00 (97.63)	Lr 0.000342	eta 2:27:26
Epoch: [40/150][20/185]	Time 0.383 (0.445)	Data 0.000 (0.020)	Loss_t 0.3021 (0.2992)	Loss_x 1.2473 (1.3066)	Acc 96.88 (96.72)	Lr 0.000350	eta 2:32:12
Epoch: [40/150][40/185]	Time 0.372 (0.421)	Data 0.000 (0.010)	Loss_t 0.3470 (0.2988)	Loss_x 1.3061 (1.3000)	Acc 98.44 (97.07)	Lr 0.000350	eta 2:23:50
Epoch: [40/150][60/185]	Time 0.418 (0.421)	Data 0.000 (0.007)	Loss_t 0.2598 (0.2944)	Loss_x 1.3297 (1.3049)	Acc 98.44 (97.29)	Lr 0.000350	eta 2:23:43
Epoch: [40/150][80/185]	Time 0.440 (0.422)	Data 0.000 (0.005)	Loss_t 0.3348 (0.2914)	Loss_x 1.2887 (1.2963)	Acc 98.44 (97.52)	Lr 0.000350	eta 2:23:55
Epoch: [40/150][100/185]	Time 0.454 (0.421)	Data 0.000 (0.004)	Loss_t 0.2250 (0.2894)	Loss_x 1.2389 (1.2946)	Acc 98.44 (97.45)	Lr 0.000350	eta 2:23:22
Epoch: [40/150][120/185]	Time 0.432 (0.421)	Data 0.000 (0.003)	Loss_t 0.1809 (0.2883)	Loss_x 1.2509 (1.2931)	Acc 96.88 (97.45)	Lr 0.000350	eta 2:23:12
Epoch: [40/150][140/185]	Time 0.350 (0.417)	Data 0.000 (0.003)	Loss_t 0.2933 (0.2872)	Loss_x 1.1996 (1.2888)	Acc 98.44 (97.58)	Lr 0.000350	eta 2:21:40
Epoch: [40/150][160/185]	Time 0.382 (0.412)	Data 0.000 (0.003)	Loss_t 0.3232 (0.2850)	Loss_x 1.2560 (1.2804)	Acc 98.44 (97.77)	Lr 0.000350	eta 2:19:52
Epoch: [40/150][180/185]	Time 0.371 (0.408)	Data 0.000 (0.002)	Loss_t 0.2590 (0.2833)	Loss_x 1.2958 (1.2730)	Acc 92.19 (97.82)	Lr 0.000350	eta 2:18:27
##### Evaluating market1501 (source) #####
Extracting features from query set ...
Done, obtained 3368-by-4608 matrix
Extracting features from gallery set ...
Done, obtained 15913-by-4608 matrix
Speed: 0.0375 sec/batch
Computing distance matrix with metric=euclidean ...
Computing CMC and mAP ...
** Results **
mAP: 78.2%
CMC curve
Rank-1  : 90.5%
Rank-5  : 96.9%
Rank-10 : 98.0%
Rank-20 : 98.9%
Checkpoint saved to "log/model.pth.tar-40"
Epoch: [41/150][20/185]	Time 0.425 (0.448)	Data 0.000 (0.026)	Loss_t 0.2952 (0.2796)	Loss_x 1.3094 (1.3281)	Acc 95.31 (96.88)	Lr 0.000350	eta 2:31:40
Epoch: [41/150][40/185]	Time 0.486 (0.435)	Data 0.000 (0.013)	Loss_t 0.2248 (0.2864)	Loss_x 1.3006 (1.3136)	Acc 98.44 (97.50)	Lr 0.000350	eta 2:27:19
Epoch: [41/150][60/185]	Time 0.421 (0.427)	Data 0.000 (0.009)	Loss_t 0.2491 (0.2793)	Loss_x 1.3554 (1.3026)	Acc 96.88 (97.63)	Lr 0.000350	eta 2:24:26
Epoch: [41/150][80/185]	Time 0.440 (0.425)	Data 0.000 (0.007)	Loss_t 0.2725 (0.2776)	Loss_x 1.2370 (1.2948)	Acc 98.44 (97.73)	Lr 0.000350	eta 2:23:43
Epoch: [41/150][100/185]	Time 0.463 (0.427)	Data 0.000 (0.005)	Loss_t 0.2222 (0.2795)	Loss_x 1.2434 (1.2917)	Acc 98.44 (97.80)	Lr 0.000350	eta 2:24:08
Epoch: [41/150][120/185]	Time 0.412 (0.426)	Data 0.000 (0.004)	Loss_t 0.4080 (0.2782)	Loss_x 1.2603 (1.2847)	Acc 96.88 (97.90)	Lr 0.000350	eta 2:23:47
Epoch: [41/150][140/185]	Time 0.416 (0.425)	Data 0.000 (0.004)	Loss_t 0.2882 (0.2775)	Loss_x 1.2411 (1.2800)	Acc 96.88 (97.88)	Lr 0.000350	eta 2:23:11
Epoch: [41/150][160/185]	Time 0.422 (0.425)	Data 0.000 (0.003)	Loss_t 0.2438 (0.2746)	Loss_x 1.1599 (1.2728)	Acc 100.00 (97.98)	Lr 0.000350	eta 2:23:00
Epoch: [41/150][180/185]	Time 0.436 (0.428)	Data 0.000 (0.003)	Loss_t 0.2192 (0.2715)	Loss_x 1.1878 (1.2645)	Acc 96.88 (98.07)	Lr 0.000350	eta 2:23:46
Epoch: [42/150][20/185]	Time 0.396 (0.463)	Data 0.000 (0.019)	Loss_t 0.2330 (0.2572)	Loss_x 1.3840 (1.3078)	Acc 95.31 (96.95)	Lr 0.000350	eta 2:35:23
Epoch: [42/150][40/185]	Time 0.421 (0.453)	Data 0.000 (0.010)	Loss_t 0.3621 (0.2591)	Loss_x 1.2641 (1.2932)	Acc 96.88 (97.42)	Lr 0.000350	eta 2:31:57
Epoch: [42/150][60/185]	Time 0.452 (0.449)	Data 0.000 (0.007)	Loss_t 0.2547 (0.2548)	Loss_x 1.2496 (1.2887)	Acc 96.88 (97.63)	Lr 0.000350	eta 2:30:31
Epoch: [42/150][80/185]	Time 0.463 (0.443)	Data 0.000 (0.005)	Loss_t 0.3161 (0.2563)	Loss_x 1.2767 (1.2861)	Acc 100.00 (97.70)	Lr 0.000350	eta 2:28:12
Epoch: [42/150][100/185]	Time 0.419 (0.443)	Data 0.000 (0.004)	Loss_t 0.2571 (0.2549)	Loss_x 1.2315 (1.2813)	Acc 98.44 (97.64)	Lr 0.000350	eta 2:28:13
Epoch: [42/150][120/185]	Time 0.417 (0.437)	Data 0.000 (0.003)	Loss_t 0.2249 (0.2578)	Loss_x 1.2624 (1.2795)	Acc 96.88 (97.72)	Lr 0.000350	eta 2:26:08
Epoch: [42/150][140/185]	Time 0.418 (0.437)	Data 0.000 (0.003)	Loss_t 0.2227 (0.2568)	Loss_x 1.1982 (1.2740)	Acc 98.44 (97.80)	Lr 0.000350	eta 2:25:51
Epoch: [42/150][160/185]	Time 0.345 (0.435)	Data 0.000 (0.003)	Loss_t 0.3280 (0.2584)	Loss_x 1.3052 (1.2687)	Acc 95.31 (97.92)	Lr 0.000350	eta 2:25:04
Epoch: [42/150][180/185]	Time 0.415 (0.429)	Data 0.000 (0.002)	Loss_t 0.2146 (0.2572)	Loss_x 1.1771 (1.2622)	Acc 98.44 (97.91)	Lr 0.000350	eta 2:22:55
Epoch: [43/150][20/185]	Time 0.510 (0.473)	Data 0.000 (0.022)	Loss_t 0.3209 (0.2633)	Loss_x 1.3789 (1.3247)	Acc 93.75 (96.72)	Lr 0.000350	eta 2:37:26
Epoch: [43/150][40/185]	Time 0.419 (0.461)	Data 0.000 (0.011)	Loss_t 0.2706 (0.2702)	Loss_x 1.2497 (1.3085)	Acc 98.44 (96.99)	Lr 0.000350	eta 2:33:18
Epoch: [43/150][60/185]	Time 0.431 (0.448)	Data 0.000 (0.007)	Loss_t 0.3220 (0.2704)	Loss_x 1.2457 (1.3042)	Acc 98.44 (97.29)	Lr 0.000350	eta 2:28:35
Epoch: [43/150][80/185]	Time 0.393 (0.439)	Data 0.000 (0.006)	Loss_t 0.1852 (0.2629)	Loss_x 1.2899 (1.2961)	Acc 100.00 (97.70)	Lr 0.000350	eta 2:25:35
Epoch: [43/150][100/185]	Time 0.407 (0.439)	Data 0.000 (0.004)	Loss_t 0.3859 (0.2661)	Loss_x 1.2648 (1.2910)	Acc 100.00 (97.59)	Lr 0.000350	eta 2:25:28
Epoch: [43/150][120/185]	Time 0.420 (0.438)	Data 0.000 (0.004)	Loss_t 0.1969 (0.2637)	Loss_x 1.2869 (1.2876)	Acc 100.00 (97.62)	Lr 0.000350	eta 2:24:51
Epoch: [43/150][140/185]	Time 0.403 (0.437)	Data 0.000 (0.003)	Loss_t 0.3106 (0.2606)	Loss_x 1.2471 (1.2829)	Acc 98.44 (97.69)	Lr 0.000350	eta 2:24:21
Epoch: [43/150][160/185]	Time 0.469 (0.435)	Data 0.000 (0.003)	Loss_t 0.1657 (0.2589)	Loss_x 1.1643 (1.2765)	Acc 100.00 (97.78)	Lr 0.000350	eta 2:23:47
Epoch: [43/150][180/185]	Time 0.439 (0.434)	Data 0.000 (0.003)	Loss_t 0.2873 (0.2563)	Loss_x 1.2320 (1.2669)	Acc 98.44 (97.95)	Lr 0.000350	eta 2:23:14
Epoch: [44/150][20/185]	Time 0.496 (0.475)	Data 0.000 (0.022)	Loss_t 0.1637 (0.2547)	Loss_x 1.1823 (1.2818)	Acc 100.00 (98.91)	Lr 0.000350	eta 2:36:41
Epoch: [44/150][40/185]	Time 0.432 (0.451)	Data 0.000 (0.011)	Loss_t 0.3183 (0.2476)	Loss_x 1.3577 (1.2799)	Acc 95.31 (98.01)	Lr 0.000350	eta 2:28:24
Epoch: [44/150][60/185]	Time 0.384 (0.443)	Data 0.000 (0.007)	Loss_t 0.1951 (0.2409)	Loss_x 1.2510 (1.2757)	Acc 98.44 (97.94)	Lr 0.000350	eta 2:25:39
Epoch: [44/150][80/185]	Time 0.416 (0.435)	Data 0.000 (0.005)	Loss_t 0.2861 (0.2394)	Loss_x 1.2712 (1.2697)	Acc 100.00 (98.16)	Lr 0.000350	eta 2:23:02
Epoch: [44/150][100/185]	Time 0.428 (0.432)	Data 0.000 (0.004)	Loss_t 0.1576 (0.2362)	Loss_x 1.2604 (1.2687)	Acc 98.44 (98.12)	Lr 0.000350	eta 2:21:56
Epoch: [44/150][120/185]	Time 0.413 (0.430)	Data 0.000 (0.004)	Loss_t 0.2164 (0.2378)	Loss_x 1.1622 (1.2625)	Acc 100.00 (98.27)	Lr 0.000350	eta 2:21:00
Epoch: [44/150][140/185]	Time 0.410 (0.427)	Data 0.000 (0.003)	Loss_t 0.2122 (0.2358)	Loss_x 1.2300 (1.2591)	Acc 98.44 (98.28)	Lr 0.000350	eta 2:19:58
Epoch: [44/150][160/185]	Time 0.415 (0.425)	Data 0.000 (0.003)	Loss_t 0.1877 (0.2334)	Loss_x 1.1792 (1.2546)	Acc 98.44 (98.34)	Lr 0.000350	eta 2:18:58
Epoch: [44/150][180/185]	Time 0.404 (0.422)	Data 0.000 (0.002)	Loss_t 0.2319 (0.2345)	Loss_x 1.2021 (1.2489)	Acc 96.88 (98.36)	Lr 0.000350	eta 2:18:04
Epoch: [45/150][20/185]	Time 0.410 (0.446)	Data 0.000 (0.023)	Loss_t 0.2415 (0.2295)	Loss_x 1.2434 (1.2980)	Acc 100.00 (97.42)	Lr 0.000350	eta 2:25:46
Epoch: [45/150][40/185]	Time 0.377 (0.420)	Data 0.000 (0.012)	Loss_t 0.2779 (0.2436)	Loss_x 1.2951 (1.2998)	Acc 98.44 (97.54)	Lr 0.000350	eta 2:16:59
Epoch: [45/150][60/185]	Time 0.432 (0.420)	Data 0.000 (0.008)	Loss_t 0.1818 (0.2377)	Loss_x 1.2054 (1.2935)	Acc 100.00 (97.63)	Lr 0.000350	eta 2:16:59
Epoch: [45/150][80/185]	Time 0.432 (0.421)	Data 0.000 (0.006)	Loss_t 0.2317 (0.2320)	Loss_x 1.2380 (1.2869)	Acc 100.00 (97.66)	Lr 0.000350	eta 2:17:05
Epoch: [45/150][100/185]	Time 0.434 (0.421)	Data 0.000 (0.005)	Loss_t 0.1940 (0.2312)	Loss_x 1.2010 (1.2808)	Acc 100.00 (97.89)	Lr 0.000350	eta 2:16:46
Epoch: [45/150][120/185]	Time 0.409 (0.421)	Data 0.000 (0.004)	Loss_t 0.2109 (0.2296)	Loss_x 1.2258 (1.2762)	Acc 100.00 (97.96)	Lr 0.000350	eta 2:16:50
Epoch: [45/150][140/185]	Time 0.439 (0.422)	Data 0.000 (0.003)	Loss_t 0.1343 (0.2288)	Loss_x 1.2496 (1.2728)	Acc 100.00 (98.05)	Lr 0.000350	eta 2:16:51
Epoch: [45/150][160/185]	Time 0.427 (0.421)	Data 0.001 (0.003)	Loss_t 0.3667 (0.2304)	Loss_x 1.3247 (1.2668)	Acc 95.31 (98.12)	Lr 0.000350	eta 2:16:29
Epoch: [45/150][180/185]	Time 0.531 (0.418)	Data 0.000 (0.003)	Loss_t 0.2101 (0.2317)	Loss_x 1.2423 (1.2610)	Acc 95.31 (98.18)	Lr 0.000350	eta 2:15:15
Epoch: [46/150][20/185]	Time 0.430 (0.461)	Data 0.000 (0.019)	Loss_t 0.2295 (0.2499)	Loss_x 1.2954 (1.3149)	Acc 98.44 (97.11)	Lr 0.000350	eta 2:29:12
Epoch: [46/150][40/185]	Time 0.380 (0.444)	Data 0.000 (0.010)	Loss_t 0.3246 (0.2400)	Loss_x 1.2820 (1.2924)	Acc 96.88 (97.54)	Lr 0.000350	eta 2:23:35
Epoch: [46/150][60/185]	Time 0.459 (0.437)	Data 0.000 (0.006)	Loss_t 0.3140 (0.2373)	Loss_x 1.2373 (1.2869)	Acc 100.00 (97.73)	Lr 0.000350	eta 2:21:04
Epoch: [46/150][80/185]	Time 0.350 (0.426)	Data 0.000 (0.005)	Loss_t 0.2189 (0.2325)	Loss_x 1.2679 (1.2845)	Acc 96.88 (97.81)	Lr 0.000350	eta 2:17:17
Epoch: [46/150][100/185]	Time 0.457 (0.425)	Data 0.000 (0.004)	Loss_t 0.2429 (0.2330)	Loss_x 1.2386 (1.2817)	Acc 100.00 (97.91)	Lr 0.000350	eta 2:16:43
Epoch: [46/150][120/185]	Time 0.450 (0.423)	Data 0.000 (0.003)	Loss_t 0.3072 (0.2325)	Loss_x 1.2772 (1.2775)	Acc 96.88 (97.93)	Lr 0.000350	eta 2:16:04
Epoch: [46/150][140/185]	Time 0.407 (0.423)	Data 0.000 (0.003)	Loss_t 0.1705 (0.2287)	Loss_x 1.2153 (1.2685)	Acc 98.44 (98.07)	Lr 0.000350	eta 2:15:59
Epoch: [46/150][160/185]	Time 0.402 (0.423)	Data 0.000 (0.002)	Loss_t 0.4071 (0.2302)	Loss_x 1.2547 (1.2633)	Acc 98.44 (98.19)	Lr 0.000350	eta 2:15:56
Epoch: [46/150][180/185]	Time 0.429 (0.424)	Data 0.000 (0.002)	Loss_t 0.3262 (0.2303)	Loss_x 1.1827 (1.2563)	Acc 100.00 (98.25)	Lr 0.000350	eta 2:16:05
Epoch: [47/150][20/185]	Time 0.420 (0.432)	Data 0.000 (0.020)	Loss_t 0.2636 (0.2221)	Loss_x 1.3295 (1.2947)	Acc 95.31 (98.05)	Lr 0.000350	eta 2:18:13
Epoch: [47/150][40/185]	Time 0.443 (0.430)	Data 0.000 (0.010)	Loss_t 0.2471 (0.2178)	Loss_x 1.2308 (1.2846)	Acc 100.00 (98.16)	Lr 0.000350	eta 2:17:35
Epoch: [47/150][60/185]	Time 0.410 (0.427)	Data 0.000 (0.007)	Loss_t 0.1946 (0.2239)	Loss_x 1.2589 (1.2760)	Acc 100.00 (98.39)	Lr 0.000350	eta 2:16:26
Epoch: [47/150][80/185]	Time 0.421 (0.425)	Data 0.000 (0.005)	Loss_t 0.1638 (0.2216)	Loss_x 1.2408 (1.2709)	Acc 100.00 (98.34)	Lr 0.000350	eta 2:15:45
Epoch: [47/150][100/185]	Time 0.425 (0.414)	Data 0.000 (0.004)	Loss_t 0.2001 (0.2217)	Loss_x 1.2671 (1.2665)	Acc 98.44 (98.39)	Lr 0.000350	eta 2:11:56
Epoch: [47/150][120/185]	Time 0.405 (0.411)	Data 0.000 (0.003)	Loss_t 0.2784 (0.2194)	Loss_x 1.2496 (1.2629)	Acc 98.44 (98.40)	Lr 0.000350	eta 2:10:50
Epoch: [47/150][140/185]	Time 0.444 (0.414)	Data 0.000 (0.003)	Loss_t 0.1655 (0.2166)	Loss_x 1.1828 (1.2603)	Acc 100.00 (98.35)	Lr 0.000350	eta 2:11:42
Epoch: [47/150][160/185]	Time 0.426 (0.413)	Data 0.000 (0.003)	Loss_t 0.3079 (0.2157)	Loss_x 1.2025 (1.2548)	Acc 98.44 (98.43)	Lr 0.000350	eta 2:11:16
Epoch: [47/150][180/185]	Time 0.433 (0.413)	Data 0.000 (0.002)	Loss_t 0.1985 (0.2135)	Loss_x 1.1357 (1.2479)	Acc 100.00 (98.46)	Lr 0.000350	eta 2:11:06
Epoch: [48/150][20/185]	Time 0.434 (0.453)	Data 0.000 (0.021)	Loss_t 0.2122 (0.2297)	Loss_x 1.3157 (1.3087)	Acc 98.44 (97.19)	Lr 0.000350	eta 2:23:51
Epoch: [48/150][40/185]	Time 0.411 (0.441)	Data 0.000 (0.010)	Loss_t 0.2337 (0.2220)	Loss_x 1.3658 (1.2951)	Acc 96.88 (97.62)	Lr 0.000350	eta 2:19:44
Epoch: [48/150][60/185]	Time 0.459 (0.434)	Data 0.000 (0.007)	Loss_t 0.1910 (0.2228)	Loss_x 1.2308 (1.2891)	Acc 96.88 (97.55)	Lr 0.000350	eta 2:17:14
Epoch: [48/150][80/185]	Time 0.427 (0.428)	Data 0.000 (0.005)	Loss_t 0.1910 (0.2198)	Loss_x 1.2530 (1.2811)	Acc 100.00 (97.87)	Lr 0.000350	eta 2:15:26
Epoch: [48/150][100/185]	Time 0.344 (0.416)	Data 0.000 (0.004)	Loss_t 0.2196 (0.2150)	Loss_x 1.3321 (1.2757)	Acc 95.31 (97.97)	Lr 0.000350	eta 2:11:24
Epoch: [48/150][120/185]	Time 0.409 (0.412)	Data 0.000 (0.004)	Loss_t 0.2485 (0.2143)	Loss_x 1.2090 (1.2703)	Acc 96.88 (98.07)	Lr 0.000350	eta 2:10:00
Epoch: [48/150][140/185]	Time 0.462 (0.415)	Data 0.000 (0.003)	Loss_t 0.1987 (0.2128)	Loss_x 1.1963 (1.2665)	Acc 100.00 (98.16)	Lr 0.000350	eta 2:10:53
Epoch: [48/150][160/185]	Time 0.389 (0.419)	Data 0.000 (0.003)	Loss_t 0.1945 (0.2123)	Loss_x 1.2398 (1.2618)	Acc 98.44 (98.24)	Lr 0.000350	eta 2:11:57
Epoch: [48/150][180/185]	Time 0.420 (0.420)	Data 0.000 (0.002)	Loss_t 0.3709 (0.2111)	Loss_x 1.2027 (1.2544)	Acc 98.44 (98.34)	Lr 0.000350	eta 2:12:14
Epoch: [49/150][20/185]	Time 0.449 (0.473)	Data 0.000 (0.026)	Loss_t 0.1769 (0.2205)	Loss_x 1.2983 (1.2902)	Acc 98.44 (98.12)	Lr 0.000350	eta 2:28:31
Epoch: [49/150][40/185]	Time 0.411 (0.456)	Data 0.000 (0.013)	Loss_t 0.1658 (0.1986)	Loss_x 1.2999 (1.2771)	Acc 98.44 (97.93)	Lr 0.000350	eta 2:23:14
Epoch: [49/150][60/185]	Time 0.389 (0.445)	Data 0.000 (0.009)	Loss_t 0.2009 (0.1978)	Loss_x 1.3199 (1.2694)	Acc 96.88 (98.07)	Lr 0.000350	eta 2:19:35
Epoch: [49/150][80/185]	Time 0.383 (0.440)	Data 0.000 (0.006)	Loss_t 0.1564 (0.1983)	Loss_x 1.3056 (1.2655)	Acc 96.88 (98.24)	Lr 0.000350	eta 2:17:42
Epoch: [49/150][100/185]	Time 0.355 (0.434)	Data 0.000 (0.005)	Loss_t 0.2208 (0.2006)	Loss_x 1.2411 (1.2642)	Acc 98.44 (98.25)	Lr 0.000350	eta 2:15:43
Epoch: [49/150][120/185]	Time 0.416 (0.434)	Data 0.000 (0.004)	Loss_t 0.3291 (0.2002)	Loss_x 1.2434 (1.2577)	Acc 98.44 (98.36)	Lr 0.000350	eta 2:15:30
Epoch: [49/150][140/185]	Time 0.420 (0.435)	Data 0.000 (0.004)	Loss_t 0.1872 (0.2037)	Loss_x 1.2018 (1.2528)	Acc 96.88 (98.38)	Lr 0.000350	eta 2:15:39
Epoch: [49/150][160/185]	Time 0.358 (0.430)	Data 0.000 (0.003)	Loss_t 0.2615 (0.2049)	Loss_x 1.2715 (1.2481)	Acc 100.00 (98.46)	Lr 0.000350	eta 2:13:56
Epoch: [49/150][180/185]	Time 0.406 (0.427)	Data 0.000 (0.003)	Loss_t 0.1344 (0.2031)	Loss_x 1.1770 (1.2438)	Acc 100.00 (98.43)	Lr 0.000350	eta 2:13:05
Epoch: [50/150][20/185]	Time 0.398 (0.459)	Data 0.000 (0.023)	Loss_t 0.2552 (0.2231)	Loss_x 1.2414 (1.2994)	Acc 98.44 (97.34)	Lr 0.000350	eta 2:22:43
Epoch: [50/150][40/185]	Time 0.424 (0.435)	Data 0.000 (0.012)	Loss_t 0.3547 (0.2121)	Loss_x 1.3619 (1.2866)	Acc 96.88 (97.70)	Lr 0.000350	eta 2:15:14
Epoch: [50/150][60/185]	Time 0.425 (0.429)	Data 0.000 (0.008)	Loss_t 0.2022 (0.2069)	Loss_x 1.3337 (1.2769)	Acc 95.31 (97.94)	Lr 0.000350	eta 2:13:07
Epoch: [50/150][80/185]	Time 0.400 (0.425)	Data 0.000 (0.006)	Loss_t 0.1813 (0.2106)	Loss_x 1.3025 (1.2707)	Acc 96.88 (98.12)	Lr 0.000350	eta 2:11:54
Epoch: [50/150][100/185]	Time 0.428 (0.421)	Data 0.000 (0.005)	Loss_t 0.1744 (0.2107)	Loss_x 1.2107 (1.2697)	Acc 100.00 (98.20)	Lr 0.000350	eta 2:10:31
Epoch: [50/150][120/185]	Time 0.425 (0.423)	Data 0.000 (0.004)	Loss_t 0.1495 (0.2085)	Loss_x 1.1930 (1.2650)	Acc 100.00 (98.29)	Lr 0.000350	eta 2:10:55
Epoch: [50/150][140/185]	Time 0.427 (0.423)	Data 0.000 (0.003)	Loss_t 0.1334 (0.2031)	Loss_x 1.2155 (1.2594)	Acc 100.00 (98.37)	Lr 0.000350	eta 2:10:35
Epoch: [50/150][160/185]	Time 0.439 (0.421)	Data 0.000 (0.003)	Loss_t 0.3104 (0.2026)	Loss_x 1.2761 (1.2568)	Acc 96.88 (98.31)	Lr 0.000350	eta 2:10:06
Epoch: [50/150][180/185]	Time 0.375 (0.420)	Data 0.000 (0.003)	Loss_t 0.1538 (0.1987)	Loss_x 1.2260 (1.2500)	Acc 100.00 (98.38)	Lr 0.000350	eta 2:09:36
##### Evaluating market1501 (source) #####
Extracting features from query set ...
Done, obtained 3368-by-4608 matrix
Extracting features from gallery set ...
Done, obtained 15913-by-4608 matrix
Speed: 0.0368 sec/batch
Computing distance matrix with metric=euclidean ...
Computing CMC and mAP ...
** Results **
mAP: 81.3%
CMC curve
Rank-1  : 92.2%
Rank-5  : 97.4%
Rank-10 : 98.4%
Rank-20 : 98.9%
Checkpoint saved to "log/model.pth.tar-50"
Epoch: [51/150][20/185]	Time 0.396 (0.450)	Data 0.000 (0.025)	Loss_t 0.3270 (0.2050)	Loss_x 1.2739 (1.2765)	Acc 98.44 (97.89)	Lr 0.000350	eta 2:18:45
Epoch: [51/150][40/185]	Time 0.439 (0.439)	Data 0.000 (0.013)	Loss_t 0.2728 (0.2028)	Loss_x 1.2150 (1.2699)	Acc 100.00 (98.01)	Lr 0.000350	eta 2:15:13
Epoch: [51/150][60/185]	Time 0.415 (0.432)	Data 0.000 (0.008)	Loss_t 0.1990 (0.2062)	Loss_x 1.2829 (1.2639)	Acc 98.44 (98.05)	Lr 0.000350	eta 2:12:48
Epoch: [51/150][80/185]	Time 0.453 (0.428)	Data 0.000 (0.006)	Loss_t 0.2418 (0.2000)	Loss_x 1.3550 (1.2616)	Acc 98.44 (98.24)	Lr 0.000350	eta 2:11:25
Epoch: [51/150][100/185]	Time 0.465 (0.427)	Data 0.000 (0.005)	Loss_t 0.2090 (0.1970)	Loss_x 1.3404 (1.2589)	Acc 93.75 (98.28)	Lr 0.000350	eta 2:10:53
Epoch: [51/150][120/185]	Time 0.399 (0.427)	Data 0.000 (0.004)	Loss_t 0.2211 (0.1970)	Loss_x 1.2314 (1.2589)	Acc 98.44 (98.23)	Lr 0.000350	eta 2:10:39
Epoch: [51/150][140/185]	Time 0.439 (0.425)	Data 0.000 (0.004)	Loss_t 0.1613 (0.1959)	Loss_x 1.1565 (1.2557)	Acc 100.00 (98.28)	Lr 0.000350	eta 2:10:11
Epoch: [51/150][160/185]	Time 0.450 (0.426)	Data 0.000 (0.003)	Loss_t 0.2091 (0.1928)	Loss_x 1.1795 (1.2484)	Acc 100.00 (98.33)	Lr 0.000350	eta 2:10:12
Epoch: [51/150][180/185]	Time 0.337 (0.423)	Data 0.000 (0.003)	Loss_t 0.2343 (0.1913)	Loss_x 1.1891 (1.2407)	Acc 100.00 (98.41)	Lr 0.000350	eta 2:09:09
Epoch: [52/150][20/185]	Time 0.417 (0.446)	Data 0.000 (0.022)	Loss_t 0.2889 (0.2104)	Loss_x 1.2724 (1.2915)	Acc 100.00 (98.36)	Lr 0.000350	eta 2:16:01
Epoch: [52/150][40/185]	Time 0.422 (0.441)	Data 0.000 (0.011)	Loss_t 0.2013 (0.1866)	Loss_x 1.2128 (1.2708)	Acc 100.00 (98.40)	Lr 0.000350	eta 2:14:13
Epoch: [52/150][60/185]	Time 0.417 (0.433)	Data 0.000 (0.007)	Loss_t 0.1029 (0.1821)	Loss_x 1.2215 (1.2631)	Acc 100.00 (98.44)	Lr 0.000350	eta 2:11:47
Epoch: [52/150][80/185]	Time 0.424 (0.431)	Data 0.000 (0.006)	Loss_t 0.2159 (0.1799)	Loss_x 1.3414 (1.2593)	Acc 96.88 (98.26)	Lr 0.000350	eta 2:10:52
Epoch: [52/150][100/185]	Time 0.472 (0.428)	Data 0.000 (0.005)	Loss_t 0.2326 (0.1840)	Loss_x 1.2394 (1.2575)	Acc 98.44 (98.34)	Lr 0.000350	eta 2:09:58
Epoch: [52/150][120/185]	Time 0.375 (0.427)	Data 0.000 (0.004)	Loss_t 0.1436 (0.1840)	Loss_x 1.2334 (1.2567)	Acc 96.88 (98.31)	Lr 0.000350	eta 2:09:29
Epoch: [52/150][140/185]	Time 0.458 (0.423)	Data 0.000 (0.003)	Loss_t 0.2222 (0.1868)	Loss_x 1.1832 (1.2525)	Acc 100.00 (98.45)	Lr 0.000350	eta 2:08:09
Epoch: [52/150][160/185]	Time 0.430 (0.422)	Data 0.000 (0.003)	Loss_t 0.1679 (0.1861)	Loss_x 1.2817 (1.2467)	Acc 98.44 (98.45)	Lr 0.000350	eta 2:07:33
Epoch: [52/150][180/185]	Time 0.374 (0.417)	Data 0.000 (0.003)	Loss_t 0.1565 (0.1835)	Loss_x 1.1923 (1.2384)	Acc 98.44 (98.52)	Lr 0.000350	eta 2:06:06
Epoch: [53/150][20/185]	Time 0.438 (0.475)	Data 0.000 (0.020)	Loss_t 0.2429 (0.1862)	Loss_x 1.3492 (1.2953)	Acc 96.88 (97.73)	Lr 0.000350	eta 2:23:18
Epoch: [53/150][40/185]	Time 0.372 (0.452)	Data 0.000 (0.010)	Loss_t 0.1513 (0.1897)	Loss_x 1.2687 (1.2881)	Acc 98.44 (98.24)	Lr 0.000350	eta 2:16:13
Epoch: [53/150][60/185]	Time 0.430 (0.435)	Data 0.000 (0.007)	Loss_t 0.1591 (0.1889)	Loss_x 1.2080 (1.2776)	Acc 98.44 (98.33)	Lr 0.000350	eta 2:11:05
Epoch: [53/150][80/185]	Time 0.437 (0.435)	Data 0.000 (0.005)	Loss_t 0.1567 (0.1859)	Loss_x 1.3175 (1.2756)	Acc 98.44 (98.20)	Lr 0.000350	eta 2:10:43
Epoch: [53/150][100/185]	Time 0.348 (0.432)	Data 0.000 (0.004)	Loss_t 0.1297 (0.1904)	Loss_x 1.2147 (1.2756)	Acc 100.00 (98.14)	Lr 0.000350	eta 2:09:46
Epoch: [53/150][120/185]	Time 0.344 (0.417)	Data 0.000 (0.003)	Loss_t 0.1150 (0.1879)	Loss_x 1.2120 (1.2705)	Acc 98.44 (98.23)	Lr 0.000350	eta 2:05:14
Epoch: [53/150][140/185]	Time 0.357 (0.414)	Data 0.000 (0.003)	Loss_t 0.2433 (0.1864)	Loss_x 1.3068 (1.2673)	Acc 98.44 (98.23)	Lr 0.000350	eta 2:03:59
Epoch: [53/150][160/185]	Time 0.434 (0.413)	Data 0.000 (0.003)	Loss_t 0.1421 (0.1847)	Loss_x 1.1956 (1.2597)	Acc 100.00 (98.30)	Lr 0.000350	eta 2:03:48
Epoch: [53/150][180/185]	Time 0.396 (0.412)	Data 0.000 (0.002)	Loss_t 0.1690 (0.1864)	Loss_x 1.2030 (1.2528)	Acc 98.44 (98.32)	Lr 0.000350	eta 2:03:11
Epoch: [54/150][20/185]	Time 0.354 (0.413)	Data 0.000 (0.022)	Loss_t 0.1008 (0.1749)	Loss_x 1.2446 (1.2823)	Acc 98.44 (97.97)	Lr 0.000350	eta 2:03:31
Epoch: [54/150][40/185]	Time 0.427 (0.401)	Data 0.000 (0.011)	Loss_t 0.1765 (0.1798)	Loss_x 1.2941 (1.2708)	Acc 98.44 (98.16)	Lr 0.000350	eta 1:59:41
Epoch: [54/150][60/185]	Time 0.445 (0.409)	Data 0.000 (0.008)	Loss_t 0.1698 (0.1836)	Loss_x 1.2721 (1.2676)	Acc 98.44 (98.26)	Lr 0.000350	eta 2:01:53
Epoch: [54/150][80/185]	Time 0.400 (0.411)	Data 0.000 (0.006)	Loss_t 0.1193 (0.1803)	Loss_x 1.2557 (1.2601)	Acc 100.00 (98.46)	Lr 0.000350	eta 2:02:29
Epoch: [54/150][100/185]	Time 0.379 (0.411)	Data 0.000 (0.005)	Loss_t 0.0778 (0.1726)	Loss_x 1.1805 (1.2552)	Acc 100.00 (98.48)	Lr 0.000350	eta 2:02:21
Epoch: [54/150][120/185]	Time 0.387 (0.410)	Data 0.000 (0.004)	Loss_t 0.2188 (0.1739)	Loss_x 1.2776 (1.2535)	Acc 96.88 (98.46)	Lr 0.000350	eta 2:01:40
Epoch: [54/150][140/185]	Time 0.401 (0.408)	Data 0.000 (0.003)	Loss_t 0.2366 (0.1735)	Loss_x 1.2542 (1.2505)	Acc 98.44 (98.48)	Lr 0.000350	eta 2:01:08
Epoch: [54/150][160/185]	Time 0.383 (0.407)	Data 0.000 (0.003)	Loss_t 0.1977 (0.1744)	Loss_x 1.2565 (1.2462)	Acc 100.00 (98.48)	Lr 0.000350	eta 2:00:36
Epoch: [54/150][180/185]	Time 0.398 (0.405)	Data 0.000 (0.003)	Loss_t 0.1401 (0.1740)	Loss_x 1.1403 (1.2403)	Acc 100.00 (98.54)	Lr 0.000350	eta 1:59:59
Epoch: [55/150][20/185]	Time 0.436 (0.458)	Data 0.000 (0.024)	Loss_t 0.1831 (0.1787)	Loss_x 1.2041 (1.2776)	Acc 100.00 (98.28)	Lr 0.000350	eta 2:15:26
Epoch: [55/150][40/185]	Time 0.433 (0.446)	Data 0.000 (0.012)	Loss_t 0.2699 (0.1760)	Loss_x 1.3364 (1.2608)	Acc 92.19 (98.09)	Lr 0.000350	eta 2:11:38
Epoch: [55/150][60/185]	Time 0.387 (0.427)	Data 0.000 (0.008)	Loss_t 0.0931 (0.1768)	Loss_x 1.1717 (1.2607)	Acc 100.00 (98.18)	Lr 0.000350	eta 2:05:49
Epoch: [55/150][80/185]	Time 0.410 (0.420)	Data 0.000 (0.006)	Loss_t 0.1274 (0.1747)	Loss_x 1.2385 (1.2585)	Acc 100.00 (98.34)	Lr 0.000350	eta 2:03:53
Epoch: [55/150][100/185]	Time 0.429 (0.419)	Data 0.000 (0.005)	Loss_t 0.2607 (0.1776)	Loss_x 1.2559 (1.2566)	Acc 98.44 (98.36)	Lr 0.000350	eta 2:03:23
Epoch: [55/150][120/185]	Time 0.410 (0.415)	Data 0.000 (0.004)	Loss_t 0.2367 (0.1783)	Loss_x 1.1790 (1.2560)	Acc 98.44 (98.24)	Lr 0.000350	eta 2:01:54
Epoch: [55/150][140/185]	Time 0.481 (0.417)	Data 0.000 (0.003)	Loss_t 0.1497 (0.1773)	Loss_x 1.2185 (1.2518)	Acc 100.00 (98.33)	Lr 0.000350	eta 2:02:35
Epoch: [55/150][160/185]	Time 0.382 (0.419)	Data 0.000 (0.003)	Loss_t 0.1749 (0.1790)	Loss_x 1.1666 (1.2511)	Acc 100.00 (98.24)	Lr 0.000350	eta 2:02:54
Epoch: [55/150][180/185]	Time 0.410 (0.420)	Data 0.000 (0.003)	Loss_t 0.1606 (0.1772)	Loss_x 1.1752 (1.2441)	Acc 98.44 (98.35)	Lr 0.000350	eta 2:02:56
Epoch: [56/150][20/185]	Time 0.402 (0.476)	Data 0.000 (0.021)	Loss_t 0.2005 (0.1937)	Loss_x 1.2695 (1.2926)	Acc 98.44 (97.42)	Lr 0.000350	eta 2:19:21
Epoch: [56/150][40/185]	Time 0.446 (0.451)	Data 0.000 (0.011)	Loss_t 0.1628 (0.1734)	Loss_x 1.2024 (1.2812)	Acc 100.00 (98.05)	Lr 0.000350	eta 2:11:44
Epoch: [56/150][60/185]	Time 0.430 (0.441)	Data 0.000 (0.007)	Loss_t 0.0952 (0.1676)	Loss_x 1.2559 (1.2643)	Acc 93.75 (98.31)	Lr 0.000350	eta 2:08:44
Epoch: [56/150][80/185]	Time 0.468 (0.441)	Data 0.000 (0.005)	Loss_t 0.1140 (0.1655)	Loss_x 1.1683 (1.2594)	Acc 100.00 (98.38)	Lr 0.000350	eta 2:08:28
Epoch: [56/150][100/185]	Time 0.437 (0.438)	Data 0.000 (0.004)	Loss_t 0.2144 (0.1661)	Loss_x 1.2568 (1.2541)	Acc 98.44 (98.41)	Lr 0.000350	eta 2:07:34
Epoch: [56/150][120/185]	Time 0.470 (0.438)	Data 0.000 (0.004)	Loss_t 0.1716 (0.1642)	Loss_x 1.2627 (1.2498)	Acc 96.88 (98.48)	Lr 0.000350	eta 2:07:30
Epoch: [56/150][140/185]	Time 0.415 (0.433)	Data 0.000 (0.003)	Loss_t 0.2927 (0.1623)	Loss_x 1.2785 (1.2478)	Acc 98.44 (98.39)	Lr 0.000350	eta 2:05:44
Epoch: [56/150][160/185]	Time 0.391 (0.431)	Data 0.000 (0.003)	Loss_t 0.1283 (0.1600)	Loss_x 1.1534 (1.2409)	Acc 100.00 (98.46)	Lr 0.000350	eta 2:05:12
Epoch: [56/150][180/185]	Time 0.343 (0.425)	Data 0.000 (0.003)	Loss_t 0.2267 (0.1602)	Loss_x 1.2212 (1.2351)	Acc 93.75 (98.45)	Lr 0.000350	eta 2:03:21
Epoch: [57/150][20/185]	Time 0.481 (0.447)	Data 0.000 (0.018)	Loss_t 0.1735 (0.1709)	Loss_x 1.3622 (1.2868)	Acc 98.44 (97.89)	Lr 0.000350	eta 2:09:22
Epoch: [57/150][40/185]	Time 0.400 (0.432)	Data 0.000 (0.009)	Loss_t 0.2209 (0.1752)	Loss_x 1.2308 (1.2686)	Acc 96.88 (98.01)	Lr 0.000350	eta 2:05:03
Epoch: [57/150][60/185]	Time 0.349 (0.425)	Data 0.000 (0.006)	Loss_t 0.1613 (0.1675)	Loss_x 1.2527 (1.2593)	Acc 98.44 (98.15)	Lr 0.000350	eta 2:02:50
Epoch: [57/150][80/185]	Time 0.420 (0.417)	Data 0.000 (0.005)	Loss_t 0.1248 (0.1661)	Loss_x 1.2059 (1.2544)	Acc 100.00 (98.26)	Lr 0.000350	eta 2:00:21
Epoch: [57/150][100/185]	Time 0.362 (0.415)	Data 0.000 (0.004)	Loss_t 0.1784 (0.1681)	Loss_x 1.2808 (1.2567)	Acc 98.44 (98.28)	Lr 0.000350	eta 1:59:33
Epoch: [57/150][120/185]	Time 0.366 (0.410)	Data 0.000 (0.003)	Loss_t 0.1190 (0.1626)	Loss_x 1.1896 (1.2504)	Acc 98.44 (98.36)	Lr 0.000350	eta 1:57:58
Epoch: [57/150][140/185]	Time 0.415 (0.411)	Data 0.000 (0.003)	Loss_t 0.2834 (0.1657)	Loss_x 1.2792 (1.2463)	Acc 100.00 (98.45)	Lr 0.000350	eta 1:58:10
Epoch: [57/150][160/185]	Time 0.433 (0.411)	Data 0.000 (0.002)	Loss_t 0.1751 (0.1656)	Loss_x 1.1717 (1.2414)	Acc 100.00 (98.50)	Lr 0.000350	eta 1:57:59
Epoch: [57/150][180/185]	Time 0.423 (0.412)	Data 0.000 (0.002)	Loss_t 0.3150 (0.1663)	Loss_x 1.1822 (1.2345)	Acc 100.00 (98.60)	Lr 0.000350	eta 1:58:15
Epoch: [58/150][20/185]	Time 0.378 (0.444)	Data 0.000 (0.024)	Loss_t 0.1325 (0.1571)	Loss_x 1.2459 (1.2553)	Acc 100.00 (98.75)	Lr 0.000350	eta 2:07:17
Epoch: [58/150][40/185]	Time 0.400 (0.427)	Data 0.000 (0.012)	Loss_t 0.1513 (0.1585)	Loss_x 1.1720 (1.2541)	Acc 100.00 (98.83)	Lr 0.000350	eta 2:02:16
Epoch: [58/150][60/185]	Time 0.362 (0.424)	Data 0.000 (0.008)	Loss_t 0.2872 (0.1573)	Loss_x 1.2413 (1.2541)	Acc 96.88 (98.36)	Lr 0.000350	eta 2:01:01
Epoch: [58/150][80/185]	Time 0.472 (0.424)	Data 0.000 (0.006)	Loss_t 0.1720 (0.1586)	Loss_x 1.2167 (1.2502)	Acc 98.44 (98.48)	Lr 0.000350	eta 2:01:00
Epoch: [58/150][100/185]	Time 0.462 (0.428)	Data 0.000 (0.005)	Loss_t 0.1693 (0.1608)	Loss_x 1.2278 (1.2457)	Acc 96.88 (98.48)	Lr 0.000350	eta 2:01:58
Epoch: [58/150][120/185]	Time 0.414 (0.431)	Data 0.000 (0.004)	Loss_t 0.1764 (0.1585)	Loss_x 1.2815 (1.2443)	Acc 98.44 (98.50)	Lr 0.000350	eta 2:02:41
Epoch: [58/150][140/185]	Time 0.435 (0.428)	Data 0.000 (0.004)	Loss_t 0.2470 (0.1598)	Loss_x 1.2860 (1.2406)	Acc 96.88 (98.59)	Lr 0.000350	eta 2:01:51
Epoch: [58/150][160/185]	Time 0.347 (0.426)	Data 0.000 (0.003)	Loss_t 0.1214 (0.1601)	Loss_x 1.1579 (1.2343)	Acc 100.00 (98.66)	Lr 0.000350	eta 2:01:05
Epoch: [58/150][180/185]	Time 0.425 (0.425)	Data 0.000 (0.003)	Loss_t 0.1930 (0.1593)	Loss_x 1.1818 (1.2278)	Acc 100.00 (98.72)	Lr 0.000350	eta 2:00:41
Epoch: [59/150][20/185]	Time 0.400 (0.445)	Data 0.000 (0.024)	Loss_t 0.1187 (0.1713)	Loss_x 1.2257 (1.2692)	Acc 98.44 (97.97)	Lr 0.000350	eta 2:05:59
Epoch: [59/150][40/185]	Time 0.440 (0.442)	Data 0.000 (0.012)	Loss_t 0.0906 (0.1701)	Loss_x 1.2064 (1.2664)	Acc 100.00 (98.09)	Lr 0.000350	eta 2:05:04
Epoch: [59/150][60/185]	Time 0.430 (0.441)	Data 0.000 (0.008)	Loss_t 0.1981 (0.1642)	Loss_x 1.2490 (1.2578)	Acc 96.88 (98.39)	Lr 0.000350	eta 2:04:31
Epoch: [59/150][80/185]	Time 0.426 (0.435)	Data 0.000 (0.006)	Loss_t 0.1281 (0.1604)	Loss_x 1.1912 (1.2539)	Acc 100.00 (98.36)	Lr 0.000350	eta 2:02:49
Epoch: [59/150][100/185]	Time 0.442 (0.435)	Data 0.000 (0.005)	Loss_t 0.0998 (0.1590)	Loss_x 1.1626 (1.2526)	Acc 100.00 (98.47)	Lr 0.000350	eta 2:02:46
Epoch: [59/150][120/185]	Time 0.442 (0.431)	Data 0.000 (0.004)	Loss_t 0.1511 (0.1581)	Loss_x 1.1964 (1.2487)	Acc 100.00 (98.44)	Lr 0.000350	eta 2:01:30
Epoch: [59/150][140/185]	Time 0.439 (0.430)	Data 0.000 (0.004)	Loss_t 0.1806 (0.1608)	Loss_x 1.2737 (1.2484)	Acc 96.88 (98.38)	Lr 0.000350	eta 2:01:01
Epoch: [59/150][160/185]	Time 0.469 (0.431)	Data 0.000 (0.003)	Loss_t 0.2649 (0.1615)	Loss_x 1.3095 (1.2456)	Acc 93.75 (98.31)	Lr 0.000350	eta 2:00:59
Epoch: [59/150][180/185]	Time 0.413 (0.430)	Data 0.000 (0.003)	Loss_t 0.1623 (0.1609)	Loss_x 1.1294 (1.2389)	Acc 100.00 (98.44)	Lr 0.000350	eta 2:00:38
Epoch: [60/150][20/185]	Time 0.373 (0.446)	Data 0.000 (0.022)	Loss_t 0.1484 (0.1715)	Loss_x 1.2577 (1.2957)	Acc 98.44 (97.73)	Lr 0.000350	eta 2:04:59
Epoch: [60/150][40/185]	Time 0.410 (0.427)	Data 0.000 (0.011)	Loss_t 0.1840 (0.1625)	Loss_x 1.2963 (1.2711)	Acc 98.44 (98.20)	Lr 0.000350	eta 1:59:29
Epoch: [60/150][60/185]	Time 0.438 (0.424)	Data 0.000 (0.007)	Loss_t 0.1637 (0.1606)	Loss_x 1.2186 (1.2594)	Acc 98.44 (98.44)	Lr 0.000350	eta 1:58:33
Epoch: [60/150][80/185]	Time 0.507 (0.424)	Data 0.000 (0.006)	Loss_t 0.1084 (0.1655)	Loss_x 1.2345 (1.2565)	Acc 100.00 (98.55)	Lr 0.000350	eta 1:58:21
Epoch: [60/150][100/185]	Time 0.450 (0.423)	Data 0.000 (0.004)	Loss_t 0.1411 (0.1650)	Loss_x 1.2570 (1.2556)	Acc 98.44 (98.47)	Lr 0.000350	eta 1:57:53
Epoch: [60/150][120/185]	Time 0.364 (0.422)	Data 0.000 (0.004)	Loss_t 0.0841 (0.1571)	Loss_x 1.2981 (1.2475)	Acc 98.44 (98.61)	Lr 0.000350	eta 1:57:38
Epoch: [60/150][140/185]	Time 0.430 (0.423)	Data 0.000 (0.003)	Loss_t 0.1315 (0.1553)	Loss_x 1.1752 (1.2408)	Acc 100.00 (98.64)	Lr 0.000350	eta 1:57:36
Epoch: [60/150][160/185]	Time 0.423 (0.424)	Data 0.000 (0.003)	Loss_t 0.1095 (0.1522)	Loss_x 1.1885 (1.2360)	Acc 100.00 (98.62)	Lr 0.000350	eta 1:57:48
Epoch: [60/150][180/185]	Time 0.443 (0.424)	Data 0.000 (0.003)	Loss_t 0.1634 (0.1557)	Loss_x 1.2188 (1.2299)	Acc 96.88 (98.65)	Lr 0.000350	eta 1:57:44
##### Evaluating market1501 (source) #####
Extracting features from query set ...
Done, obtained 3368-by-4608 matrix
Extracting features from gallery set ...
Done, obtained 15913-by-4608 matrix
Speed: 0.0375 sec/batch
Computing distance matrix with metric=euclidean ...
Computing CMC and mAP ...
** Results **
mAP: 80.9%
CMC curve
Rank-1  : 92.0%
Rank-5  : 97.3%
Rank-10 : 98.3%
Rank-20 : 98.9%
Checkpoint saved to "log/model.pth.tar-60"
Epoch: [61/150][20/185]	Time 0.416 (0.455)	Data 0.000 (0.025)	Loss_t 0.1311 (0.1722)	Loss_x 1.2266 (1.3080)	Acc 100.00 (97.66)	Lr 0.000350	eta 2:06:02
Epoch: [61/150][40/185]	Time 0.414 (0.446)	Data 0.000 (0.012)	Loss_t 0.1754 (0.1653)	Loss_x 1.2831 (1.2849)	Acc 98.44 (98.09)	Lr 0.000350	eta 2:03:20
Epoch: [61/150][60/185]	Time 0.450 (0.446)	Data 0.000 (0.008)	Loss_t 0.1239 (0.1556)	Loss_x 1.2270 (1.2677)	Acc 98.44 (98.36)	Lr 0.000350	eta 2:03:12
Epoch: [61/150][80/185]	Time 0.371 (0.442)	Data 0.000 (0.006)	Loss_t 0.2562 (0.1575)	Loss_x 1.3444 (1.2649)	Acc 98.44 (98.24)	Lr 0.000350	eta 2:02:06
Epoch: [61/150][100/185]	Time 0.414 (0.441)	Data 0.000 (0.005)	Loss_t 0.1395 (0.1602)	Loss_x 1.2395 (1.2594)	Acc 96.88 (98.39)	Lr 0.000350	eta 2:01:35
Epoch: [61/150][120/185]	Time 0.453 (0.440)	Data 0.000 (0.004)	Loss_t 0.1669 (0.1571)	Loss_x 1.1733 (1.2516)	Acc 98.44 (98.45)	Lr 0.000350	eta 2:01:14
Epoch: [61/150][140/185]	Time 0.449 (0.441)	Data 0.000 (0.004)	Loss_t 0.0795 (0.1573)	Loss_x 1.2062 (1.2482)	Acc 98.44 (98.49)	Lr 0.000350	eta 2:01:26
Epoch: [61/150][160/185]	Time 0.503 (0.443)	Data 0.000 (0.003)	Loss_t 0.1128 (0.1537)	Loss_x 1.1349 (1.2402)	Acc 100.00 (98.58)	Lr 0.000350	eta 2:01:46
Epoch: [61/150][180/185]	Time 0.480 (0.442)	Data 0.000 (0.003)	Loss_t 0.1573 (0.1536)	Loss_x 1.1958 (1.2331)	Acc 96.88 (98.62)	Lr 0.000350	eta 2:01:27
Epoch: [62/150][20/185]	Time 0.414 (0.438)	Data 0.000 (0.022)	Loss_t 0.1003 (0.1494)	Loss_x 1.2826 (1.2749)	Acc 96.88 (98.12)	Lr 0.000350	eta 2:00:09
Epoch: [62/150][40/185]	Time 0.436 (0.445)	Data 0.000 (0.011)	Loss_t 0.1041 (0.1414)	Loss_x 1.1882 (1.2595)	Acc 96.88 (98.59)	Lr 0.000350	eta 2:01:51
Epoch: [62/150][60/185]	Time 0.370 (0.440)	Data 0.000 (0.008)	Loss_t 0.2084 (0.1491)	Loss_x 1.2745 (1.2560)	Acc 96.88 (98.62)	Lr 0.000350	eta 2:00:25
Epoch: [62/150][80/185]	Time 0.467 (0.438)	Data 0.000 (0.006)	Loss_t 0.1702 (0.1555)	Loss_x 1.2883 (1.2563)	Acc 96.88 (98.57)	Lr 0.000350	eta 1:59:38
Epoch: [62/150][100/185]	Time 0.421 (0.436)	Data 0.000 (0.005)	Loss_t 0.1203 (0.1533)	Loss_x 1.2332 (1.2537)	Acc 100.00 (98.61)	Lr 0.000350	eta 1:58:56
Epoch: [62/150][120/185]	Time 0.385 (0.432)	Data 0.000 (0.004)	Loss_t 0.1497 (0.1560)	Loss_x 1.2612 (1.2528)	Acc 98.44 (98.57)	Lr 0.000350	eta 1:57:46
Epoch: [62/150][140/185]	Time 0.443 (0.429)	Data 0.000 (0.003)	Loss_t 0.1207 (0.1521)	Loss_x 1.1981 (1.2466)	Acc 100.00 (98.66)	Lr 0.000350	eta 1:56:50
Epoch: [62/150][160/185]	Time 0.383 (0.427)	Data 0.000 (0.003)	Loss_t 0.1488 (0.1508)	Loss_x 1.1739 (1.2395)	Acc 100.00 (98.76)	Lr 0.000350	eta 1:56:02
Epoch: [62/150][180/185]	Time 0.424 (0.425)	Data 0.000 (0.003)	Loss_t 0.1117 (0.1485)	Loss_x 1.1324 (1.2312)	Acc 100.00 (98.85)	Lr 0.000350	eta 1:55:19
Epoch: [63/150][20/185]	Time 0.460 (0.451)	Data 0.000 (0.021)	Loss_t 0.1471 (0.1620)	Loss_x 1.3097 (1.2818)	Acc 93.75 (98.12)	Lr 0.000350	eta 2:02:15
Epoch: [63/150][40/185]	Time 0.376 (0.444)	Data 0.000 (0.010)	Loss_t 0.1011 (0.1529)	Loss_x 1.2756 (1.2679)	Acc 96.88 (98.32)	Lr 0.000350	eta 2:00:15
Epoch: [63/150][60/185]	Time 0.405 (0.438)	Data 0.000 (0.007)	Loss_t 0.2407 (0.1547)	Loss_x 1.2473 (1.2610)	Acc 96.88 (98.23)	Lr 0.000350	eta 1:58:32
Epoch: [63/150][80/185]	Time 0.341 (0.429)	Data 0.000 (0.005)	Loss_t 0.2138 (0.1619)	Loss_x 1.2853 (1.2655)	Acc 100.00 (98.18)	Lr 0.000350	eta 1:55:48
Epoch: [63/150][100/185]	Time 0.404 (0.417)	Data 0.000 (0.004)	Loss_t 0.1471 (0.1565)	Loss_x 1.1957 (1.2592)	Acc 100.00 (98.27)	Lr 0.000350	eta 1:52:28
Epoch: [63/150][120/185]	Time 0.384 (0.416)	Data 0.000 (0.004)	Loss_t 0.1162 (0.1586)	Loss_x 1.1821 (1.2550)	Acc 100.00 (98.40)	Lr 0.000350	eta 1:52:07
Epoch: [63/150][140/185]	Time 0.408 (0.415)	Data 0.000 (0.003)	Loss_t 0.1508 (0.1579)	Loss_x 1.2678 (1.2492)	Acc 98.44 (98.50)	Lr 0.000350	eta 1:51:35
Epoch: [63/150][160/185]	Time 0.400 (0.415)	Data 0.000 (0.003)	Loss_t 0.1103 (0.1567)	Loss_x 1.1754 (1.2431)	Acc 98.44 (98.50)	Lr 0.000350	eta 1:51:27
Epoch: [63/150][180/185]	Time 0.397 (0.414)	Data 0.000 (0.002)	Loss_t 0.1087 (0.1576)	Loss_x 1.1373 (1.2360)	Acc 100.00 (98.59)	Lr 0.000350	eta 1:51:07
Epoch: [64/150][20/185]	Time 0.371 (0.429)	Data 0.000 (0.019)	Loss_t 0.2191 (0.1600)	Loss_x 1.3309 (1.2792)	Acc 98.44 (98.28)	Lr 0.000350	eta 1:55:03
Epoch: [64/150][40/185]	Time 0.434 (0.433)	Data 0.000 (0.010)	Loss_t 0.0894 (0.1514)	Loss_x 1.2047 (1.2589)	Acc 100.00 (98.32)	Lr 0.000350	eta 1:55:52
Epoch: [64/150][60/185]	Time 0.434 (0.434)	Data 0.000 (0.007)	Loss_t 0.1100 (0.1442)	Loss_x 1.2219 (1.2491)	Acc 98.44 (98.54)	Lr 0.000350	eta 1:55:59
Epoch: [64/150][80/185]	Time 0.383 (0.428)	Data 0.000 (0.005)	Loss_t 0.0840 (0.1438)	Loss_x 1.1876 (1.2425)	Acc 98.44 (98.52)	Lr 0.000350	eta 1:54:07
Epoch: [64/150][100/185]	Time 0.423 (0.421)	Data 0.000 (0.004)	Loss_t 0.1317 (0.1475)	Loss_x 1.1665 (1.2434)	Acc 100.00 (98.52)	Lr 0.000350	eta 1:52:12
Epoch: [64/150][120/185]	Time 0.366 (0.417)	Data 0.000 (0.003)	Loss_t 0.1526 (0.1467)	Loss_x 1.2709 (1.2392)	Acc 95.31 (98.50)	Lr 0.000350	eta 1:51:05
Epoch: [64/150][140/185]	Time 0.419 (0.415)	Data 0.000 (0.003)	Loss_t 0.0980 (0.1436)	Loss_x 1.1767 (1.2341)	Acc 100.00 (98.58)	Lr 0.000350	eta 1:50:18
Epoch: [64/150][160/185]	Time 0.415 (0.414)	Data 0.000 (0.003)	Loss_t 0.0996 (0.1397)	Loss_x 1.1575 (1.2277)	Acc 100.00 (98.67)	Lr 0.000350	eta 1:49:57
Epoch: [64/150][180/185]	Time 0.360 (0.412)	Data 0.000 (0.002)	Loss_t 0.1065 (0.1406)	Loss_x 1.1864 (1.2226)	Acc 100.00 (98.70)	Lr 0.000350	eta 1:49:17
Epoch: [65/150][20/185]	Time 0.397 (0.456)	Data 0.000 (0.021)	Loss_t 0.1659 (0.1468)	Loss_x 1.2059 (1.2863)	Acc 100.00 (97.81)	Lr 0.000350	eta 2:00:50
Epoch: [65/150][40/185]	Time 0.391 (0.444)	Data 0.000 (0.011)	Loss_t 0.1507 (0.1436)	Loss_x 1.2285 (1.2681)	Acc 100.00 (98.32)	Lr 0.000350	eta 1:57:29
Epoch: [65/150][60/185]	Time 0.402 (0.436)	Data 0.000 (0.007)	Loss_t 0.3068 (0.1399)	Loss_x 1.2585 (1.2576)	Acc 98.44 (98.59)	Lr 0.000350	eta 1:55:10
Epoch: [65/150][80/185]	Time 0.430 (0.438)	Data 0.000 (0.005)	Loss_t 0.0890 (0.1399)	Loss_x 1.1843 (1.2529)	Acc 100.00 (98.57)	Lr 0.000350	eta 1:55:25
Epoch: [65/150][100/185]	Time 0.396 (0.432)	Data 0.000 (0.004)	Loss_t 0.1068 (0.1405)	Loss_x 1.2072 (1.2487)	Acc 98.44 (98.59)	Lr 0.000350	eta 1:53:47
Epoch: [65/150][120/185]	Time 0.413 (0.430)	Data 0.000 (0.004)	Loss_t 0.1076 (0.1435)	Loss_x 1.1923 (1.2428)	Acc 100.00 (98.63)	Lr 0.000350	eta 1:53:07
Epoch: [65/150][140/185]	Time 0.527 (0.430)	Data 0.000 (0.003)	Loss_t 0.1559 (0.1444)	Loss_x 1.2090 (1.2370)	Acc 98.44 (98.69)	Lr 0.000350	eta 1:52:59
Epoch: [65/150][160/185]	Time 0.439 (0.430)	Data 0.000 (0.003)	Loss_t 0.2477 (0.1427)	Loss_x 1.2210 (1.2313)	Acc 98.44 (98.79)	Lr 0.000350	eta 1:52:49
Epoch: [65/150][180/185]	Time 0.435 (0.430)	Data 0.000 (0.003)	Loss_t 0.0727 (0.1426)	Loss_x 1.1287 (1.2256)	Acc 100.00 (98.85)	Lr 0.000350	eta 1:52:45
Epoch: [66/150][20/185]	Time 0.437 (0.438)	Data 0.000 (0.021)	Loss_t 0.1448 (0.1747)	Loss_x 1.3331 (1.2952)	Acc 98.44 (97.89)	Lr 0.000350	eta 1:54:34
Epoch: [66/150][40/185]	Time 0.482 (0.436)	Data 0.000 (0.011)	Loss_t 0.1912 (0.1538)	Loss_x 1.2579 (1.2691)	Acc 98.44 (98.32)	Lr 0.000350	eta 1:54:02
Epoch: [66/150][60/185]	Time 0.397 (0.430)	Data 0.000 (0.007)	Loss_t 0.1009 (0.1491)	Loss_x 1.1730 (1.2624)	Acc 100.00 (98.52)	Lr 0.000350	eta 1:52:15
Epoch: [66/150][80/185]	Time 0.402 (0.426)	Data 0.000 (0.005)	Loss_t 0.1449 (0.1484)	Loss_x 1.2378 (1.2580)	Acc 96.88 (98.44)	Lr 0.000350	eta 1:51:04
Epoch: [66/150][100/185]	Time 0.427 (0.423)	Data 0.000 (0.004)	Loss_t 0.1306 (0.1445)	Loss_x 1.1956 (1.2486)	Acc 98.44 (98.58)	Lr 0.000350	eta 1:50:13
Epoch: [66/150][120/185]	Time 0.368 (0.421)	Data 0.000 (0.004)	Loss_t 0.1984 (0.1460)	Loss_x 1.2751 (1.2476)	Acc 96.88 (98.55)	Lr 0.000350	eta 1:49:22
Epoch: [66/150][140/185]	Time 0.432 (0.419)	Data 0.000 (0.003)	Loss_t 0.1870 (0.1469)	Loss_x 1.2184 (1.2448)	Acc 100.00 (98.62)	Lr 0.000350	eta 1:48:50
